File: ./Cargo.toml
-------------------------------------------------
[package]
name = "sd-core"
version = "0.1.4"
description = "Virtual distributed filesystem engine that powers Spacedrive."
authors = ["Spacedrive Technology Inc."]
rust-version = "1.73.0"
license = { workspace = true }
repository = { workspace = true }
edition = { workspace = true }

[features]
default = []
# This feature allows features to be disabled when the Core is running on mobile.
mobile = []
# This feature controls whether the Spacedrive Core contains functionality which requires FFmpeg.
ffmpeg = ["dep:sd-ffmpeg"]
location-watcher = ["dep:notify"]
heif = ["sd-images/heif"]

[dependencies]
sd-media-metadata = { path = "../crates/media-metadata" }
sd-prisma = { path = "../crates/prisma" }
sd-ffmpeg = { path = "../crates/ffmpeg", optional = true }
sd-crypto = { path = "../crates/crypto", features = [
  "rspc",
  "specta",
  "serde",
  "keymanager",
] }

sd-images = { path = "../crates/images", features = [
  "rspc",
  "serde",
  "specta",
] }
sd-file-ext = { path = "../crates/file-ext" }
sd-sync = { path = "../crates/sync" }
sd-p2p = { path = "../crates/p2p", features = ["specta", "serde"] }
sd-utils = { path = "../crates/utils" }
sd-core-sync = { path = "./crates/sync" }

rspc = { workspace = true, features = [
  "uuid",
  "chrono",
  "tracing",
  "alpha",
  "unstable",
] }
prisma-client-rust = { workspace = true }
specta = { workspace = true }
tokio = { workspace = true, features = [
  "sync",
  "rt-multi-thread",
  "io-util",
  "macros",
  "time",
  "process",
] }
serde = { version = "1.0", features = ["derive"] }
chrono = { version = "0.4.31", features = ["serde"] }
serde_json = { workspace = true }
serde_repr = "0.1"
futures = "0.3"
rmp-serde = "^1.1.2"
rmpv = "^1.0.1"
blake3 = "1.5.0"
hostname = "0.3.1"
uuid = { workspace = true }
sysinfo = "0.29.10"
thiserror = "1.0.50"
async-trait = "^0.1.74"
image = "0.24.7"
webp = "0.2.6"
tracing = { workspace = true }
tracing-subscriber = { workspace = true, features = ["env-filter"] }
async-stream = "0.3.5"
once_cell = "1.18.0"
ctor = "0.2.5"
globset = { version = "^0.4.13", features = ["serde1"] }
itertools = "^0.11.0"
http-range = "0.1.5"
mini-moka = "0.10.2"
serde_with = "3.4.0"
notify = { version = "=5.2.0", default-features = false, features = [
  "macos_fsevent",
], optional = true }
static_assertions = "1.1.0"
serde-hashkey = "0.4.5"
normpath = { version = "1.1.1", features = ["localization"] }
tracing-appender = { workspace = true }
strum = { version = "0.25", features = ["derive"] }
strum_macros = "0.25"
regex = "1.10.2"
int-enum = "0.5.0"
tokio-stream = { version = "0.1.14", features = ["fs"] }
futures-concurrency = "7.4.3"
async-channel = "2.0.0"
tokio-util = { version = "0.7.10", features = ["io"] }
slotmap = "1.0.6"
flate2 = "1.0.28"
tar = "0.4.40"
tempfile = "^3.8.1"
axum = "0.6.20"
http-body = "0.4.5"
pin-project-lite = "0.2.13"
bytes = "1.5.0"
reqwest = { version = "0.11.22", features = ["json", "native-tls-vendored"] }
directories = "5.0.1"
async-recursion = "1.0.5"

# Override features of transitive dependencies
[dependencies.openssl]
version = "=0.10.57"
features = ["vendored"]
[dependencies.openssl-sys]
version = "=0.9.93"
features = ["vendored"]

[target.'cfg(target_os = "macos")'.dependencies]
plist = "1"

[target.'cfg(windows)'.dependencies.winapi-util]
version = "0.1.6"

[dev-dependencies]
tracing-test = "^0.2.4"
aovec = "1.1.0"



File: ./crates/sync/Cargo.toml
-------------------------------------------------
[package]
name = "sd-core-sync"
version = "0.0.0"
edition = "2021"

[features]
default = []

[dependencies]
sd-prisma = { path = "../../../crates/prisma" }
sd-sync = { path = "../../../crates/sync" }
sd-utils = { path = "../../../crates/utils" }

prisma-client-rust = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tokio = { workspace = true }
uuid = { workspace = true }
tracing = { workspace = true }
uhlc = "=0.5.2"



File: ./crates/sync/tests/lib.rs
-------------------------------------------------
use sd_core_sync::*;
use sd_prisma::{prisma, prisma_sync};
use sd_sync::*;
use sd_utils::uuid_to_bytes;

use prisma_client_rust::chrono::Utc;
use serde_json::json;
use std::sync::Arc;
use tokio::sync::broadcast;
use uuid::Uuid;

fn db_path(id: Uuid) -> String {
	format!("/tmp/test-{id}.db")
}

#[derive(Clone)]
struct Instance {
	id: Uuid,
	db: Arc<prisma::PrismaClient>,
	sync: Arc<sd_core_sync::Manager>,
}

impl Instance {
	async fn new(id: Uuid) -> (Arc<Self>, broadcast::Receiver<SyncMessage>) {
		let db = Arc::new(
			prisma::PrismaClient::_builder()
				.with_url(format!("file:{}", db_path(id)))
				.build()
				.await
				.unwrap(),
		);

		db._db_push().await.unwrap();

		db.instance()
			.create(
				uuid_to_bytes(id),
				vec![],
				vec![],
				format!("Instace {id}"),
				0,
				Utc::now().into(),
				Utc::now().into(),
				vec![],
			)
			.exec()
			.await
			.unwrap();

		let sync = sd_core_sync::Manager::new(&db, id, &Default::default());

		(
			Arc::new(Self {
				id,
				db,
				sync: Arc::new(sync.manager),
			}),
			sync.rx,
		)
	}

	async fn teardown(&self) {
		tokio::fs::remove_file(db_path(self.id)).await.unwrap();
	}

	async fn pair(left: &Self, right: &Self) {
		left.db
			.instance()
			.create(
				uuid_to_bytes(right.id),
				vec![],
				vec![],
				"".to_string(),
				0,
				Utc::now().into(),
				Utc::now().into(),
				vec![],
			)
			.exec()
			.await
			.unwrap();

		right
			.db
			.instance()
			.create(
				uuid_to_bytes(left.id),
				vec![],
				vec![],
				"".to_string(),
				0,
				Utc::now().into(),
				Utc::now().into(),
				vec![],
			)
			.exec()
			.await
			.unwrap();
	}
}

#[tokio::test]
async fn bruh() -> Result<(), Box<dyn std::error::Error>> {
	let (instance1, mut sync_rx1) = Instance::new(Uuid::new_v4()).await;
	let (instance2, mut sync_rx2) = Instance::new(Uuid::new_v4()).await;

	Instance::pair(&instance1, &instance2).await;

	tokio::spawn({
		let _instance1 = instance1.clone();
		let instance2 = instance2.clone();

		async move {
			while let Ok(msg) = sync_rx1.recv().await {
				if let SyncMessage::Created = msg {
					instance2
						.sync
						.ingest
						.event_tx
						.send(ingest::Event::Notification)
						.await
						.unwrap()
				}
			}
		}
	});

	tokio::spawn({
		let instance1 = instance1.clone();
		let instance2 = instance2.clone();

		async move {
			while let Some(msg) = instance2.sync.ingest.req_rx.lock().await.recv().await {
				match msg {
					ingest::Request::Messages { timestamps, .. } => {
						let messages = instance1
							.sync
							.get_ops(GetOpsArgs {
								clocks: timestamps,
								count: 100,
							})
							.await
							.unwrap();

						let ingest = &instance2.sync.ingest;
						ingest
							.event_tx
							.send(ingest::Event::Messages(ingest::MessagesEvent {
								messages,
								has_more: false,
								instance_id: instance1.id,
							}))
							.await
							.unwrap();
					}
					ingest::Request::Ingested => {
						instance2.sync.tx.send(SyncMessage::Ingested).ok();
					}
					_ => todo!(),
				}
			}
		}
	});

	instance1
		.sync
		.write_ops(&instance1.db, {
			let id = Uuid::new_v4();

			use prisma::location;

			macro_rules! item {
				($name:ident, $value:expr) => {
					(
						(location::$name::NAME, json!($value)),
						location::$name::set(Some($value.to_string())),
					)
				};
			}

			let (sync_ops, db_ops): (Vec<_>, Vec<_>) = [
				item!(name, "Location 0"),
				item!(path, "/User/Brendan/Documents"),
			]
			.into_iter()
			.unzip();

			(
				instance1.sync.shared_create(
					prisma_sync::location::SyncId {
						pub_id: uuid_to_bytes(id),
					},
					sync_ops,
				),
				instance1.db.location().create(uuid_to_bytes(id), db_ops),
			)
		})
		.await?;

	assert!(matches!(sync_rx2.recv().await?, SyncMessage::Ingested));

	let out = instance2
		.sync
		.get_ops(GetOpsArgs {
			clocks: vec![],
			count: 100,
		})
		.await?;

	assert_eq!(out.len(), 3);
	assert!(matches!(out[0].typ, CRDTOperationType::Shared(_)));

	instance1.teardown().await;
	instance2.teardown().await;

	Ok(())
}



File: ./crates/sync/src/lib.rs
-------------------------------------------------
#![allow(clippy::unwrap_used, clippy::panic)] // TODO: Brendan remove this once you've got error handling here

mod actor;
mod db_operation;
pub mod ingest;
mod manager;

use sd_prisma::prisma::*;
use sd_sync::*;

use std::{
	collections::HashMap,
	sync::{atomic::AtomicBool, Arc},
};

pub use ingest::*;
pub use manager::*;
pub use uhlc::NTP64;

#[derive(Clone)]
pub enum SyncMessage {
	Ingested,
	Created,
}

pub type Timestamps = Arc<tokio::sync::RwLock<HashMap<uuid::Uuid, NTP64>>>;

pub struct SharedState {
	pub db: Arc<PrismaClient>,
	pub emit_messages_flag: Arc<AtomicBool>,
	pub instance: uuid::Uuid,
	pub timestamps: Timestamps,
	pub clock: uhlc::HLC,
}

pub fn shared_op_db(op: &CRDTOperation, shared_op: &SharedOperation) -> shared_operation::Create {
	shared_operation::Create {
		id: op.id.as_bytes().to_vec(),
		timestamp: op.timestamp.0 as i64,
		instance: instance::pub_id::equals(op.instance.as_bytes().to_vec()),
		kind: shared_op.kind().to_string(),
		data: serde_json::to_vec(&shared_op.data).unwrap(),
		model: shared_op.model.to_string(),
		record_id: serde_json::to_vec(&shared_op.record_id).unwrap(),
		_params: vec![],
	}
}

pub fn relation_op_db(
	op: &CRDTOperation,
	relation_op: &RelationOperation,
) -> relation_operation::Create {
	relation_operation::Create {
		id: op.id.as_bytes().to_vec(),
		timestamp: op.timestamp.0 as i64,
		instance: instance::pub_id::equals(op.instance.as_bytes().to_vec()),
		kind: relation_op.kind().to_string(),
		data: serde_json::to_vec(&relation_op.data).unwrap(),
		relation: relation_op.relation.to_string(),
		item_id: serde_json::to_vec(&relation_op.relation_item).unwrap(),
		group_id: serde_json::to_vec(&relation_op.relation_group).unwrap(),
		_params: vec![],
	}
}



File: ./crates/sync/src/ingest.rs
-------------------------------------------------
use std::{ops::Deref, sync::Arc};

use sd_prisma::{prisma::*, prisma_sync::ModelSyncData};
use sd_sync::*;
use sd_utils::uuid_to_bytes;
use serde_json::to_vec;
use tokio::sync::{mpsc, Mutex};
use uhlc::{Timestamp, NTP64};
use uuid::Uuid;

use crate::{actor::*, wait, SharedState};

#[derive(Debug)]
#[must_use]
/// Stuff that can be handled outside the actor
pub enum Request {
	Messages { timestamps: Vec<(Uuid, NTP64)> },
	Ingested,
	FinishedIngesting,
}

/// Stuff that the actor consumes
#[derive(Debug)]
pub enum Event {
	Notification,
	Messages(MessagesEvent),
}

#[derive(Debug, Default)]
pub enum State {
	#[default]
	WaitingForNotification,
	RetrievingMessages,
	Ingesting(MessagesEvent),
}

pub struct Actor {
	state: Option<State>,
	shared: Arc<SharedState>,
	io: ActorIO<Self>,
}

impl Actor {
	async fn tick(mut self) -> Option<Self> {
		let state = match self.state.take()? {
			State::WaitingForNotification => {
				wait!(self.io.event_rx, Event::Notification);

				State::RetrievingMessages
			}
			State::RetrievingMessages => {
				self.io
					.send(Request::Messages {
						timestamps: self
							.timestamps
							.read()
							.await
							.iter()
							.map(|(&k, &v)| (k, v))
							.collect(),
					})
					.await
					.ok();

				State::Ingesting(wait!(self.io.event_rx, Event::Messages(event) => event))
			}
			State::Ingesting(event) => {
				for op in event.messages {
					let fut = self.receive_crdt_operation(op);
					fut.await;
				}

				match event.has_more {
					true => State::RetrievingMessages,
					false => {
						self.io.send(Request::FinishedIngesting).await.ok();

						State::WaitingForNotification
					}
				}
			}
		};

		Some(Self {
			state: Some(state),
			..self
		})
	}

	pub fn spawn(shared: Arc<SharedState>) -> Handler {
		let (actor_io, handler_io) = create_actor_io::<Self>();

		tokio::spawn(async move {
			let mut this = Self {
				state: Some(Default::default()),
				io: actor_io,
				shared,
			};

			loop {
				this = match this.tick().await {
					Some(this) => this,
					None => break,
				};
			}
		});

		Handler {
			event_tx: handler_io.event_tx,
			req_rx: Arc::new(Mutex::new(handler_io.req_rx)),
		}
	}

	async fn receive_crdt_operation(&mut self, op: CRDTOperation) {
		self.clock
			.update_with_timestamp(&Timestamp::new(op.timestamp, op.instance.into()))
			.ok();

		let mut timestamp = {
			let mut clocks = self.timestamps.write().await;
			*clocks.entry(op.instance).or_insert_with(|| op.timestamp)
		};

		if timestamp < op.timestamp {
			timestamp = op.timestamp;
		}

		let op_instance = op.instance;

		let is_old = self.compare_message(&op).await;

		if !is_old {
			self.apply_op(op).await.ok();
		}

		self.db
			._transaction()
			.run({
				let timestamps = self.timestamps.clone();
				|db| async move {
					match db
						.instance()
						.update(
							instance::pub_id::equals(uuid_to_bytes(op_instance)),
							vec![instance::timestamp::set(Some(timestamp.as_u64() as i64))],
						)
						.exec()
						.await
					{
						Ok(_) => {
							timestamps.write().await.insert(op_instance, timestamp);
							Ok(())
						}
						Err(e) => Err(e),
					}
				}
			})
			.await
			.unwrap();
	}

	async fn apply_op(&mut self, op: CRDTOperation) -> prisma_client_rust::Result<()> {
		ModelSyncData::from_op(op.typ.clone())
			.unwrap()
			.exec(&self.db)
			.await?;

		match &op.typ {
			CRDTOperationType::Shared(shared_op) => {
				shared_op_db(&op, shared_op)
					.to_query(&self.db)
					.exec()
					.await?;
			}
			CRDTOperationType::Relation(relation_op) => {
				relation_op_db(&op, relation_op)
					.to_query(&self.db)
					.exec()
					.await?;
			}
		}

		self.io.req_tx.send(Request::Ingested).await.ok();

		Ok(())
	}

	async fn compare_message(&mut self, op: &CRDTOperation) -> bool {
		let old_timestamp = match &op.typ {
			CRDTOperationType::Shared(shared_op) => {
				let newer_op = self
					.db
					.shared_operation()
					.find_first(vec![
						shared_operation::timestamp::gte(op.timestamp.as_u64() as i64),
						shared_operation::model::equals(shared_op.model.to_string()),
						shared_operation::record_id::equals(
							serde_json::to_vec(&shared_op.record_id).unwrap(),
						),
						shared_operation::kind::equals(shared_op.kind().to_string()),
					])
					.order_by(shared_operation::timestamp::order(SortOrder::Desc))
					.exec()
					.await
					.unwrap();

				newer_op.map(|newer_op| newer_op.timestamp)
			}
			CRDTOperationType::Relation(relation_op) => {
				let newer_op = self
					.db
					.relation_operation()
					.find_first(vec![
						relation_operation::timestamp::gte(op.timestamp.as_u64() as i64),
						relation_operation::relation::equals(relation_op.relation.to_string()),
						relation_operation::item_id::equals(
							serde_json::to_vec(&relation_op.relation_item).unwrap(),
						),
						relation_operation::kind::equals(relation_op.kind().to_string()),
					])
					.order_by(relation_operation::timestamp::order(SortOrder::Desc))
					.exec()
					.await
					.unwrap();

				newer_op.map(|newer_op| newer_op.timestamp)
			}
		};

		old_timestamp
			.map(|old| old != op.timestamp.as_u64() as i64)
			.unwrap_or_default()
	}
}

impl Deref for Actor {
	type Target = SharedState;

	fn deref(&self) -> &Self::Target {
		&self.shared
	}
}

pub struct Handler {
	pub event_tx: mpsc::Sender<Event>,
	pub req_rx: Arc<Mutex<mpsc::Receiver<Request>>>,
}

#[derive(Debug)]
pub struct MessagesEvent {
	pub instance_id: Uuid,
	pub messages: Vec<CRDTOperation>,
	pub has_more: bool,
}

impl ActorTypes for Actor {
	type Event = Event;
	type Request = Request;
	type Handler = Handler;
}

fn shared_op_db(op: &CRDTOperation, shared_op: &SharedOperation) -> shared_operation::Create {
	shared_operation::Create {
		id: op.id.as_bytes().to_vec(),
		timestamp: op.timestamp.0 as i64,
		instance: instance::pub_id::equals(op.instance.as_bytes().to_vec()),
		kind: shared_op.kind().to_string(),
		data: to_vec(&shared_op.data).unwrap(),
		model: shared_op.model.to_string(),
		record_id: to_vec(&shared_op.record_id).unwrap(),
		_params: vec![],
	}
}

fn relation_op_db(
	op: &CRDTOperation,
	relation_op: &RelationOperation,
) -> relation_operation::Create {
	relation_operation::Create {
		id: op.id.as_bytes().to_vec(),
		timestamp: op.timestamp.0 as i64,
		instance: instance::pub_id::equals(op.instance.as_bytes().to_vec()),
		kind: relation_op.kind().to_string(),
		data: to_vec(&relation_op.data).unwrap(),
		relation: relation_op.relation.to_string(),
		item_id: to_vec(&relation_op.relation_item).unwrap(),
		group_id: to_vec(&relation_op.relation_group).unwrap(),
		_params: vec![],
	}
}

// #[must_use]
// pub struct ReqRes<TReq, TResp> {
// 	request: TReq,
// 	response_sender: oneshot::Sender<TResp>,
// }

// impl<TReq, TResp> ReqRes<TReq, TResp> {
// 	pub async fn send<TContainer>(
// 		request: TReq,
// 		container_fn: impl Fn(Self) -> TContainer,
// 		sender: &mpsc::Sender<TContainer>,
// 	) -> TResp {
// 		let (tx, rx) = oneshot::channel();

// 		let payload = container_fn(Self {
// 			request,
// 			response_sender: tx,
// 		});

// 		sender.send(payload).await.ok();

// 		rx.await.unwrap()
// 	}

// 	#[must_use]
// 	pub fn split(self) -> (TReq, impl FnOnce(TResp)) {
// 		(self.request, |response| {
// 			self.response_sender.send(response).ok();
// 		})
// 	}

// 	pub async fn map<
// 		TFn: FnOnce(TReq) -> TFut,
// 		TFut: Future<Output = Result<TResp, TErr>>,
// 		TErr,
// 	>(
// 		self,
// 		func: TFn,
// 	) -> Result<(), TErr> {
// 		self.response_sender.send(func(self.request).await?).ok();
// 		Ok(())
// 	}
// }



File: ./crates/sync/src/manager.rs
-------------------------------------------------
use sd_prisma::prisma::*;
use sd_sync::*;
use sd_utils::uuid_to_bytes;

use crate::{db_operation::*, *};
use std::{
	cmp::Ordering,
	ops::Deref,
	sync::{
		atomic::{self, AtomicBool},
		Arc,
	},
};
use tokio::sync::broadcast;
use uhlc::{HLCBuilder, HLC};
use uuid::Uuid;

pub struct Manager {
	pub tx: broadcast::Sender<SyncMessage>,
	pub ingest: ingest::Handler,
	shared: Arc<SharedState>,
}

#[derive(serde::Serialize, serde::Deserialize, Debug, PartialEq, Eq)]
pub struct GetOpsArgs {
	pub clocks: Vec<(Uuid, NTP64)>,
	pub count: u32,
}

pub struct New<T> {
	pub manager: T,
	pub rx: broadcast::Receiver<SyncMessage>,
}

impl Manager {
	pub fn new(
		db: &Arc<PrismaClient>,
		instance: Uuid,
		emit_messages_flag: &Arc<AtomicBool>,
	) -> New<Self> {
		let (tx, rx) = broadcast::channel(64);

		let timestamps: Timestamps = Default::default();
		let clock = HLCBuilder::new().with_id(instance.into()).build();

		let shared = Arc::new(SharedState {
			db: db.clone(),
			instance,
			timestamps,
			clock,
			emit_messages_flag: emit_messages_flag.clone(),
		});

		let ingest = ingest::Actor::spawn(shared.clone());

		New {
			manager: Self { shared, tx, ingest },
			rx,
		}
	}

	pub async fn write_ops<'item, I: prisma_client_rust::BatchItem<'item>>(
		&self,
		tx: &PrismaClient,
		(_ops, queries): (Vec<CRDTOperation>, I),
	) -> prisma_client_rust::Result<<I as prisma_client_rust::BatchItemParent>::ReturnValue> {
		// let start = Instant::now();

		let ret = if self.emit_messages_flag.load(atomic::Ordering::Relaxed) {
			macro_rules! variant {
				($var:ident, $variant:ident, $fn:ident) => {
					let $var = _ops
						.iter()
						.filter_map(|op| match &op.typ {
							CRDTOperationType::$variant(inner) => {
								Some($fn(&op, &inner).to_query(tx))
							}
							_ => None,
						})
						.collect::<Vec<_>>();
				};
			}

			variant!(shared, Shared, shared_op_db);
			variant!(relation, Relation, relation_op_db);

			let (res, _) = tx._batch((queries, (shared, relation))).await?;

			self.tx.send(SyncMessage::Created).ok();

			res
		} else {
			tx._batch([queries]).await?.remove(0)
		};

		// debug!("time: {}", start.elapsed().as_millis());

		Ok(ret)
	}

	#[allow(unused_variables)]
	pub async fn write_op<'item, Q: prisma_client_rust::BatchItem<'item>>(
		&self,
		tx: &PrismaClient,
		op: CRDTOperation,
		query: Q,
	) -> prisma_client_rust::Result<<Q as prisma_client_rust::BatchItemParent>::ReturnValue> {
		let ret = if self.emit_messages_flag.load(atomic::Ordering::Relaxed) {
			macro_rules! exec {
				($fn:ident, $inner:ident) => {
					tx._batch(($fn(&op, $inner).to_query(tx), query)).await?.1
				};
			}

			let ret = match &op.typ {
				CRDTOperationType::Shared(inner) => exec!(shared_op_db, inner),
				CRDTOperationType::Relation(inner) => exec!(relation_op_db, inner),
			};

			self.tx.send(SyncMessage::Created).ok();

			ret
		} else {
			tx._batch(vec![query]).await?.remove(0)
		};

		Ok(ret)
	}

	pub async fn get_ops(
		&self,
		args: GetOpsArgs,
	) -> prisma_client_rust::Result<Vec<CRDTOperation>> {
		let db = &self.db;

		macro_rules! db_args {
			($args:ident, $op:ident) => {
				vec![prisma_client_rust::operator::or(
					$args
						.clocks
						.iter()
						.map(|(instance_id, timestamp)| {
							prisma_client_rust::and![
								$op::instance::is(vec![instance::pub_id::equals(uuid_to_bytes(
									*instance_id
								))]),
								$op::timestamp::gt(timestamp.as_u64() as i64)
							]
						})
						.chain([
							$op::instance::is_not(vec![
								instance::pub_id::in_vec(
									$args
										.clocks
										.iter()
										.map(|(instance_id, _)| {
											uuid_to_bytes(*instance_id)
										})
										.collect()
								)
							])
						])
						.collect(),
				)]
			};
		}

		let (shared, relation) = db
			._batch((
				db.shared_operation()
					.find_many(db_args!(args, shared_operation))
					.take(args.count as i64)
					.order_by(shared_operation::timestamp::order(SortOrder::Asc))
					.include(shared_include::include()),
				db.relation_operation()
					.find_many(db_args!(args, relation_operation))
					.take(args.count as i64)
					.order_by(relation_operation::timestamp::order(SortOrder::Asc))
					.include(relation_include::include()),
			))
			.await?;

		let mut ops: Vec<_> = []
			.into_iter()
			.chain(shared.into_iter().map(DbOperation::Shared))
			.chain(relation.into_iter().map(DbOperation::Relation))
			.collect();

		ops.sort_by(|a, b| match a.timestamp().cmp(&b.timestamp()) {
			Ordering::Equal => a.instance().cmp(&b.instance()),
			o => o,
		});

		Ok(ops
			.into_iter()
			.take(args.count as usize)
			.map(DbOperation::into_operation)
			.collect())
	}
}

impl OperationFactory for Manager {
	fn get_clock(&self) -> &HLC {
		&self.clock
	}

	fn get_instance(&self) -> Uuid {
		self.instance
	}
}

impl Deref for Manager {
	type Target = SharedState;

	fn deref(&self) -> &Self::Target {
		&self.shared
	}
}



File: ./crates/sync/src/actor.rs
-------------------------------------------------
use tokio::sync::mpsc;

pub trait ActorTypes {
	type Event;
	type Request;
	type Handler;
}

pub struct ActorIO<T: ActorTypes> {
	pub event_rx: mpsc::Receiver<T::Event>,
	pub req_tx: mpsc::Sender<T::Request>,
}

impl<T: ActorTypes> ActorIO<T> {
	pub async fn send(&self, value: T::Request) -> Result<(), mpsc::error::SendError<T::Request>> {
		self.req_tx.send(value).await
	}
}

pub struct HandlerIO<T: ActorTypes> {
	pub event_tx: mpsc::Sender<T::Event>,
	pub req_rx: mpsc::Receiver<T::Request>,
}

pub fn create_actor_io<T: ActorTypes>() -> (ActorIO<T>, HandlerIO<T>) {
	let (req_tx, req_rx) = mpsc::channel(20);
	let (event_tx, event_rx) = mpsc::channel(20);

	(ActorIO { event_rx, req_tx }, HandlerIO { event_tx, req_rx })
}

#[macro_export]
macro_rules! wait {
	($rx:expr, $pattern:pat $(=> $expr:expr)?) => {
		loop {
			match $rx.recv().await {
				Some($pattern) => break $($expr)?,
				_ => continue
			}
		}
	};
}



File: ./crates/sync/src/db_operation.rs
-------------------------------------------------
use sd_prisma::prisma::*;
use sd_sync::*;
use uhlc::NTP64;
use uuid::Uuid;

shared_operation::include!(shared_include {
	instance: select { pub_id }
});
relation_operation::include!(relation_include {
	instance: select { pub_id }
});

pub enum DbOperation {
	Shared(shared_include::Data),
	Relation(relation_include::Data),
}

impl DbOperation {
	pub fn timestamp(&self) -> NTP64 {
		NTP64(match self {
			Self::Shared(op) => op.timestamp,
			Self::Relation(op) => op.timestamp,
		} as u64)
	}

	pub fn id(&self) -> Uuid {
		Uuid::from_slice(match self {
			Self::Shared(op) => &op.id,
			Self::Relation(op) => &op.id,
		})
		.unwrap()
	}

	pub fn instance(&self) -> Uuid {
		Uuid::from_slice(match self {
			Self::Shared(op) => &op.instance.pub_id,
			Self::Relation(op) => &op.instance.pub_id,
		})
		.unwrap()
	}

	pub fn into_operation(self) -> CRDTOperation {
		CRDTOperation {
			id: self.id(),
			instance: self.instance(),
			timestamp: self.timestamp(),
			typ: match self {
				Self::Shared(op) => CRDTOperationType::Shared(SharedOperation {
					record_id: serde_json::from_slice(&op.record_id).unwrap(),
					model: op.model,
					data: serde_json::from_slice(&op.data).unwrap(),
				}),
				Self::Relation(op) => CRDTOperationType::Relation(RelationOperation {
					relation: op.relation,
					data: serde_json::from_slice(&op.data).unwrap(),
					relation_item: serde_json::from_slice(&op.item_id).unwrap(),
					relation_group: serde_json::from_slice(&op.group_id).unwrap(),
				}),
			},
		}
	}
}



File: ./prisma/migrations/migration_lock.toml
-------------------------------------------------
# Please do not edit this file manually
# It should be added in your version-control system (i.e. Git)
provider = "sqlite"


File: ./prisma/schema.prisma
-------------------------------------------------
datasource db {
    provider = "sqlite"
    url      = "file:dev.db"
}

generator client {
    provider      = "cargo prisma"
    output        = "../../crates/prisma/src/prisma"
    module_path   = "prisma"
    client_format = "folder"
}

generator sync {
    provider      = "cargo prisma-sync"
    output        = "../../crates/prisma/src/prisma_sync"
    client_format = "folder"
}

//// Sync ////

model SharedOperation {
    id        Bytes  @id
    timestamp BigInt
    model     String

    record_id Bytes
    // Enum: ??
    kind      String
    data      Bytes

    instance_id Int
    instance    Instance @relation(fields: [instance_id], references: [id])

    // attestation Bytes

    @@map("shared_operation")
}

model RelationOperation {
    id        Bytes  @id
    timestamp BigInt
    relation  String

    item_id  Bytes
    group_id Bytes

    kind String
    data Bytes

    instance_id Int
    instance    Instance @relation(fields: [instance_id], references: [id])

    @@map("relation_operation")
}

/// @deprecated: This model has to exist solely for backwards compatibility.
model Node {
    id           Int      @id @default(autoincrement())
    pub_id       Bytes    @unique
    name         String
    // Enum: sd_core::node::Platform
    platform     Int
    date_created DateTime
    identity     Bytes? // TODO: Change to required field in future
    node_peer_id String? // TODO: Remove as part of - https://linear.app/spacedriveapp/issue/ENG-757/p2p-library-portability

    @@map("node")
}

/// @local(id: pub_id)
// represents a single `.db` file (SQLite DB) that is paired to the current library.
// A `LibraryInstance` is always owned by a single `Node` but it's possible for that node to change (or two to be owned by a single node).
model Instance {
    id       Int   @id @default(autoincrement()) // This is is NOT globally unique
    pub_id   Bytes @unique // This UUID is meaningless and exists soley cause the `uhlc::ID` must be 16-bit. Really this should be derived from the `identity` field.
    // Enum: sd_core::p2p::IdentityOrRemoteIdentity
    identity Bytes

    node_id       Bytes
    node_name     String
    // Enum: sd_core::node::Platform
    node_platform Int

    last_seen    DateTime // Time core started for owner, last P2P message for P2P node
    date_created DateTime

    // clock timestamp for sync
    timestamp BigInt?

    // attestation Bytes

    SharedOperation   SharedOperation[]
    RelationOperation RelationOperation[]
    locations         Location[]

    @@map("instance")
}

model Statistics {
    id                   Int      @id @default(autoincrement())
    date_captured        DateTime @default(now())
    total_object_count   Int      @default(0)
    library_db_size      String   @default("0")
    total_bytes_used     String   @default("0")
    total_bytes_capacity String   @default("0")
    total_unique_bytes   String   @default("0")
    total_bytes_free     String   @default("0")
    preview_media_bytes  String   @default("0")

    @@map("statistics")
}

/// @local
model Volume {
    id                    Int      @id @default(autoincrement())
    name                  String
    mount_point           String
    total_bytes_capacity  String   @default("0")
    total_bytes_available String   @default("0")
    disk_type             String?
    filesystem            String?
    is_system             Boolean  @default(false)
    date_modified         DateTime @default(now())

    @@unique([mount_point, name])
    @@map("volume")
}

/// @shared(id: pub_id)
model Location {
    id     Int   @id @default(autoincrement())
    pub_id Bytes @unique

    name                   String?
    path                   String?
    total_capacity         Int?
    available_capacity     Int?
    size_in_bytes          Bytes?
    is_archived            Boolean?
    generate_preview_media Boolean?
    sync_preview_media     Boolean?
    hidden                 Boolean?
    date_created           DateTime?

    instance_id Int?
    instance    Instance? @relation(fields: [instance_id], references: [id], onDelete: SetNull)

    file_paths    FilePath[]
    indexer_rules IndexerRulesInLocation[]

    @@map("location")
}

/// @shared(id: pub_id)
model FilePath {
    id     Int   @id @default(autoincrement())
    pub_id Bytes @unique

    is_dir Boolean?

    // content addressable storage id - blake3 sampled checksum
    cas_id             String?
    // full byte contents digested into blake3 checksum
    integrity_checksum String?

    // location that owns this path
    location_id Int?
    location    Location? @relation(fields: [location_id], references: [id], onDelete: SetNull)

    // the path of the file relative to its location
    materialized_path String?

    // the name and extension, MUST have 'COLLATE NOCASE' in migration
    name      String?
    extension String?
    hidden    Boolean?

    size_in_bytes       String? // deprecated
    size_in_bytes_bytes Bytes?

    inode Bytes? // This is actually an unsigned 64 bit integer, but we don't have this type in SQLite

    // the unique Object for this file path
    object_id Int?
    object    Object? @relation(fields: [object_id], references: [id], onDelete: SetNull)

    key_id Int? // replacement for encryption
    // permissions       String?

    date_created  DateTime?
    date_modified DateTime?
    date_indexed  DateTime?

    // key Key? @relation(fields: [key_id], references: [id])

    @@unique([location_id, materialized_path, name, extension])
    @@unique([location_id, inode])
    @@index([location_id])
    @@index([location_id, materialized_path])
    @@map("file_path")
}

/// @shared(id: pub_id)
model Object {
    id     Int   @id @default(autoincrement())
    pub_id Bytes @unique
    // Enum: sd_file_ext::kind::ObjectKind
    kind   Int?

    key_id        Int?
    // handy ways to mark an object
    hidden        Boolean?
    favorite      Boolean?
    important     Boolean?
    // if we have generated preview media for this object on at least one Node
    // commented out for now by @brendonovich since they they're irrelevant to the sync system
    // has_thumbnail     Boolean?
    // has_thumbstrip    Boolean?
    // has_video_preview Boolean?
    // TODO: change above to:
    // has_generated_thumbnail     Boolean  @default(false)
    // has_generated_thumbstrip    Boolean  @default(false)
    // has_generated_video_preview Boolean  @default(false)
    // integration with ipfs
    // ipfs_id           String?
    // plain text note
    note          String?
    // the original known creation date of this object
    date_created  DateTime?
    date_accessed DateTime?

    tags       TagOnObject[]
    labels     LabelOnObject[]
    albums     ObjectInAlbum[]
    spaces     ObjectInSpace[]
    file_paths FilePath[]
    // comments   Comment[]
    media_data MediaData?

    // key Key? @relation(fields: [key_id], references: [id])

    @@map("object")
}

// if there is a conflicting cas_id, the conficting file should be updated to have a larger cas_id as
//the field is unique, however this record is kept to tell the indexer (upon discovering this CAS) that
//there is alternate versions of the file and to check by a full integrity hash to define for which to associate with.
// @brendan: nah this probably won't fly
// model FileConflict {
//     original_object_id   Int @unique
//     detactched_object_id Int @unique

//     @@map("file_conflict")
// }

// keys allow us to know exactly which files can be decrypted with a given key
// they can be "mounted" to a client, and then used to decrypt files automatically
/// @shared(id: uuid)
// model Key {
//     id                Int       @id @default(autoincrement())
//     // uuid to identify the key
//     uuid              String    @unique
//     version           String
//     key_type          String
//     // the name that the user sets
//     name              String?
//     // is this key the default for encryption?
//     // was not tagged as unique as i'm not too sure if PCR will handle it
//     // can always be tagged as unique, the keys API will need updating to use `find_unique()`
//     default           Boolean   @default(false)
//     // nullable if concealed for security
//     date_created      DateTime? @default(now())
//     // encryption algorithm used to encrypt the key
//     algorithm         String
//     // hashing algorithm used for hashing the key with the content salt
//     hashing_algorithm String
//     // salt used for encrypting data with this key
//     content_salt      Bytes
//     // the *encrypted* master key (48 bytes)
//     master_key        Bytes
//     // the nonce used for encrypting the master key
//     master_key_nonce  Bytes
//     // the nonce used for encrypting the key
//     key_nonce         Bytes
//     // the *encrypted* key
//     key               Bytes
//     // the salt used for deriving the KEK (used for encrypting the master key) from the root key
//     salt              Bytes

//     automount Boolean @default(false)

//     objects    Object[]
//     file_paths FilePath[]

//     @@map("key")
// }

model MediaData {
    id Int @id @default(autoincrement())

    resolution     Bytes?
    media_date     Bytes?
    media_location Bytes?
    camera_data    Bytes?
    artist         String?
    description    String?
    copyright      String?
    exif_version   String?

    // purely for sorting/ordering, never sent to the frontend as they'd be useless
    // these are also usually one-way, and not reversible
    // (e.g. we can't get `MediaDate::Utc(2023-09-26T22:04:37+01:00)` from `1695758677` as we don't store the TZ)
    epoch_time BigInt? // time since unix epoch

    // video-specific
    // duration Int?
    // fps      Int?
    // streams  Int?
    // video_codec   String? // eg: "h264, h265, av1"
    // audio_codec String? // eg: "opus"

    object_id Int    @unique
    object    Object @relation(fields: [object_id], references: [id], onDelete: Cascade)

    @@map("media_data")
}

//// Tag ////

/// @shared(id: pub_id)
model Tag {
    id     Int     @id @default(autoincrement())
    pub_id Bytes   @unique
    name   String?
    color  String?

    // Enum: ??
    redundancy_goal Int?

    date_created  DateTime?
    date_modified DateTime?

    tag_objects TagOnObject[]

    @@map("tag")
}

/// @relation(item: tag, group: object)
model TagOnObject {
    tag_id Int
    tag    Tag @relation(fields: [tag_id], references: [id], onDelete: Restrict)

    object_id Int
    object    Object @relation(fields: [object_id], references: [id], onDelete: Restrict)

    date_created  DateTime?

    @@id([tag_id, object_id])
    @@map("tag_on_object")
}

//// Label ////

model Label {
    id            Int      @id @default(autoincrement())
    pub_id        Bytes    @unique
    name          String?
    date_created  DateTime @default(now())
    date_modified DateTime @default(now())

    label_objects LabelOnObject[]

    @@map("label")
}

model LabelOnObject {
    date_created DateTime @default(now())

    label_id Int
    label    Label @relation(fields: [label_id], references: [id], onDelete: Restrict)

    object_id Int
    object    Object @relation(fields: [object_id], references: [id], onDelete: Restrict)

    @@id([label_id, object_id])
    @@map("label_on_object")
}

//// Space ////

model Space {
    id            Int       @id @default(autoincrement())
    pub_id        Bytes     @unique
    name          String?
    description   String?
    date_created  DateTime?
    date_modified DateTime?

    objects ObjectInSpace[]

    @@map("space")
}

model ObjectInSpace {
    space_id Int
    space    Space @relation(fields: [space_id], references: [id], onDelete: Restrict)

    object_id Int
    object    Object @relation(fields: [object_id], references: [id], onDelete: Restrict)

    @@id([space_id, object_id])
    @@map("object_in_space")
}

//// Job ////

model Job {
    id Bytes @id

    name   String?
    action String? // Will be composed of "{action_description}(-{children_order})*"

    // Enum: sd_core::job::job_manager:JobStatus
    status Int? // 0 = Queued

    // List of errors, separated by "\n\n" in case of failed jobs or completed with errors
    errors_text String?

    data     Bytes? // Serialized data to be used on pause/resume
    metadata Bytes? // Serialized metadata field with info about the job after completion

    parent_id Bytes?

    task_count                Int?
    completed_task_count      Int?
    date_estimated_completion DateTime? // Estimated timestamp that the job will be complete at

    date_created   DateTime?
    date_started   DateTime? // Started execution
    date_completed DateTime? // Finished execution

    parent   Job?  @relation("jobs_dependency", fields: [parent_id], references: [id], onDelete: SetNull)
    children Job[] @relation("jobs_dependency")

    @@map("job")
}

//// Album ////

model Album {
    id        Int      @id
    pub_id    Bytes    @unique
    name      String?
    is_hidden Boolean?

    date_created  DateTime?
    date_modified DateTime?

    objects ObjectInAlbum[]

    @@map("album")
}

model ObjectInAlbum {
    date_created DateTime?
    album_id     Int
    album        Album     @relation(fields: [album_id], references: [id], onDelete: NoAction)

    object_id Int
    object    Object @relation(fields: [object_id], references: [id], onDelete: NoAction)

    @@id([album_id, object_id])
    @@map("object_in_album")
}

//// Comment ////

// model Comment {
//     id            Int      @id @default(autoincrement())
//     pub_id        Bytes    @unique
//     content       String
//     date_created  DateTime @default(now())
//     date_modified DateTime @default(now())
//     object_id     Int?
//     object        Object?  @relation(fields: [object_id], references: [id])

//     @@map("comment")
// }

//// Indexer Rules ////

model IndexerRule {
    id     Int   @id @default(autoincrement())
    pub_id Bytes @unique

    name           String?
    default        Boolean?
    rules_per_kind Bytes?
    date_created   DateTime?
    date_modified  DateTime?

    locations IndexerRulesInLocation[]

    @@map("indexer_rule")
}

model IndexerRulesInLocation {
    location_id Int
    location    Location @relation(fields: [location_id], references: [id], onDelete: Restrict)

    indexer_rule_id Int
    indexer_rule    IndexerRule @relation(fields: [indexer_rule_id], references: [id], onDelete: Restrict)

    @@id([location_id, indexer_rule_id])
    @@map("indexer_rule_in_location")
}

/// @shared(id: key)
model Preference {
    key   String @id
    value Bytes?

    @@map("preference")
}

model Notification {
    id         Int       @id @default(autoincrement())
    read       Boolean   @default(false)
    // Enum: crate::api::notifications::NotificationData
    data       Bytes
    expires_at DateTime?

    @@map("notification")
}

model SavedSearch {
    id            Int       @id @default(autoincrement())
    pub_id        Bytes     @unique

    search        String?
    filters       String?

    name          String?
    icon          String?
    description   String?
    // order         Int? // Add this line to include ordering

    date_created  DateTime?
    date_modified DateTime?

    @@map("saved_search")
}



File: ./build.rs
-------------------------------------------------
use std::process::Command;

fn main() {
	let output = Command::new("git")
		.args(["rev-parse", "--short", "HEAD"])
		.output()
		.expect("error getting git hash. Does `git rev-parse --short HEAD` work for you?");
	let git_hash = String::from_utf8(output.stdout)
		.expect("Error passing output of `git rev-parse --short HEAD`");
	println!("cargo:rustc-env=GIT_HASH={git_hash}");
}



File: ./src/preferences/kv.rs
-------------------------------------------------
use std::collections::BTreeMap;

use crate::prisma::{preference, PrismaClient};
use itertools::Itertools;
use rmpv::Value;
use serde::{de::DeserializeOwned, Serialize};

use super::Preferences;

#[derive(Debug)]
pub struct PreferenceKey(Vec<String>);

impl PreferenceKey {
	pub fn new(value: impl Into<String>) -> Self {
		Self(
			value
				.into()
				.split('.')
				.map(ToString::to_string)
				.collect_vec(),
		)
	}

	pub fn prepend_path(&mut self, prefix: &str) {
		self.0 = [prefix.to_string()]
			.into_iter()
			.chain(self.0.drain(..))
			.collect_vec();
	}
}

impl std::fmt::Display for PreferenceKey {
	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
		write!(f, "{}", self.0.join("."))
	}
}

#[derive(Debug)]
pub struct PreferenceValue(Vec<u8>);

impl PreferenceValue {
	pub fn new(value: impl Serialize) -> Self {
		let mut bytes = vec![];

		rmp_serde::encode::write_named(&mut bytes, &value)
			.expect("Failed to serialize preference value");

		// let value = rmpv::decode::read_value(&mut bytes.as_slice()).unwrap();

		Self(bytes)
	}

	pub fn from_value(value: Value) -> Self {
		let mut bytes = vec![];

		rmpv::encode::write_value(&mut bytes, &value)
			.expect("Failed to serialize preference value");

		Self(bytes)
	}
}

#[derive(Debug)]
pub struct PreferenceKVs(Vec<(PreferenceKey, PreferenceValue)>);

impl IntoIterator for PreferenceKVs {
	type Item = (PreferenceKey, PreferenceValue);
	type IntoIter = std::vec::IntoIter<Self::Item>;

	fn into_iter(self) -> Self::IntoIter {
		self.0.into_iter()
	}
}

#[derive(Debug)]
pub enum Entry {
	Value(Vec<u8>),
	Nested(Entries),
}

#[allow(clippy::unwrap_used, clippy::panic)]
impl Entry {
	pub fn expect_value<T: DeserializeOwned>(self) -> T {
		match self {
			Self::Value(value) => rmp_serde::decode::from_read(value.as_slice()).unwrap(),
			_ => panic!("Expected value"),
		}
	}

	pub fn expect_nested(self) -> Entries {
		match self {
			Self::Nested(entries) => entries,
			_ => panic!("Expected nested entry"),
		}
	}
}

pub type Entries = BTreeMap<String, Entry>;

impl PreferenceKVs {
	pub fn new(values: Vec<(PreferenceKey, PreferenceValue)>) -> Self {
		Self(values)
	}

	pub fn with_prefix(mut self, prefix: &str) -> Self {
		for (key, _) in &mut self.0 {
			key.prepend_path(prefix);
		}

		self
	}

	pub fn into_upserts(self, db: &PrismaClient) -> Vec<preference::UpsertQuery> {
		self.0
			.into_iter()
			.map(|(key, value)| {
				let value = vec![preference::value::set(Some(value.0))];

				db.preference().upsert(
					preference::key::equals(key.to_string()),
					preference::create(key.to_string(), value.clone()),
					value,
				)
			})
			.collect()
	}

	pub fn parse<T: Preferences>(self) -> T {
		let entries = self
			.0
			.into_iter()
			.fold(BTreeMap::new(), |mut acc, (key, value)| {
				let key_parts = key.0;
				let key_parts_len = key_parts.len();

				{
					let mut curr_map: &mut BTreeMap<String, Entry> = &mut acc;

					for (i, part) in key_parts.into_iter().enumerate() {
						if i >= key_parts_len - 1 {
							curr_map.insert(part, Entry::Value(value.0));
							break;
						} else {
							curr_map = match curr_map
								.entry(part)
								.or_insert(Entry::Nested(BTreeMap::new()))
							{
								Entry::Nested(map) => map,
								_ => unreachable!(),
							};
						}
					}
				}

				acc
			});

		T::from_entries(entries)
	}
}



File: ./src/preferences/library.rs
-------------------------------------------------
use crate::api::search;
use crate::prisma::PrismaClient;
use serde::{Deserialize, Serialize};
use specta::Type;
use std::collections::BTreeMap;
use std::collections::HashMap;
use tracing::error;
use uuid::Uuid;

use super::*;

#[derive(Clone, Serialize, Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub struct LibraryPreferences {
	#[serde(default)]
	#[specta(optional)]
	location: HashMap<Uuid, Settings<LocationSettings>>,
}

impl LibraryPreferences {
	pub async fn write(self, db: &PrismaClient) -> prisma_client_rust::Result<()> {
		let kvs = self.to_kvs();

		db._batch(kvs.into_upserts(db)).await?;

		Ok(())
	}

	pub async fn read(db: &PrismaClient) -> prisma_client_rust::Result<Self> {
		let kvs = db.preference().find_many(vec![]).exec().await?;

		let prefs = PreferenceKVs::new(
			kvs.into_iter()
				.filter_map(|data| {
					rmpv::decode::read_value(&mut data.value?.as_slice())
						.map_err(|e| error!("{e:#?}"))
						.ok()
						.map(|value| {
							(
								PreferenceKey::new(data.key),
								PreferenceValue::from_value(value),
							)
						})
				})
				.collect(),
		);

		Ok(prefs.parse())
	}
}

#[derive(Clone, Serialize, Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub struct LocationSettings {
	explorer: ExplorerSettings<search::file_path::FilePathOrder>,
}

#[derive(Clone, Serialize, Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub struct ExplorerSettings<TOrder> {
	layout_mode: Option<ExplorerLayout>,
	grid_item_size: Option<i32>,
	grid_gap: Option<i32>,
	media_columns: Option<i32>,
	media_aspect_square: Option<bool>,
	media_view_with_descendants: Option<bool>,
	open_on_double_click: Option<DoubleClickAction>,
	show_bytes_in_grid_view: Option<bool>,
	col_visibility: Option<BTreeMap<String, bool>>,
	col_sizes: Option<BTreeMap<String, i32>>,
	// temporary
	#[serde(skip_serializing_if = "Option::is_none")]
	order: Option<Option<TOrder>>,
	#[serde(default)]
	show_hidden_files: bool,
}

#[derive(Clone, Serialize, Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub enum ExplorerLayout {
	Grid,
	List,
	Media,
}

#[derive(Clone, Serialize, Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub enum DoubleClickAction {
	OpenFile,
	QuickPreview,
}

impl Preferences for LibraryPreferences {
	fn to_kvs(self) -> PreferenceKVs {
		let Self { location } = self;

		location.to_kvs().with_prefix("location")
	}

	fn from_entries(mut entries: Entries) -> Self {
		Self {
			location: entries
				.remove("location")
				.map(|value| HashMap::from_entries(value.expect_nested()))
				.unwrap_or_default(),
		}
	}
}



File: ./src/preferences/mod.rs
-------------------------------------------------
mod kv;
mod library;

pub use kv::*;
pub use library::*;
use serde::{de::DeserializeOwned, Deserialize, Serialize};
use specta::Type;

use std::collections::HashMap;
use tracing::error;
use uuid::Uuid;

#[derive(Clone, Serialize, Deserialize, Type, Debug)]
#[specta(inline)]
pub struct Settings<V>(V);

impl<V> Preferences for HashMap<Uuid, Settings<V>>
where
	V: Serialize + DeserializeOwned,
{
	fn to_kvs(self) -> PreferenceKVs {
		PreferenceKVs::new(
			self.into_iter()
				.map(|(id, value)| {
					let mut buf = Uuid::encode_buffer();

					let id = id.as_simple().encode_lower(&mut buf);

					(PreferenceKey::new(id), PreferenceValue::new(value))
				})
				.collect(),
		)
	}

	fn from_entries(entries: Entries) -> Self {
		entries
			.into_iter()
			.filter_map(|(key, entry)| {
				Uuid::parse_str(&key)
					.map_err(|e| error!("{e:#?}"))
					.ok()
					.map(|uuid| (uuid, entry.expect_value()))
			})
			.collect()
	}
}

// Preferences are a set of types that are serialized as a list of key-value pairs,
// where nested type keys are serialized as a dot-separated path.
// They are serailized as a list because this allows preferences to be a synchronisation boundary,
// whereas their values (referred to as settings) will be overwritten.
pub trait Preferences {
	fn to_kvs(self) -> PreferenceKVs;
	fn from_entries(entries: Entries) -> Self;
}



File: ./src/util/infallible_request.rs
-------------------------------------------------
//! A HTTP response builder similar to the [http] crate but designed to be infallible.

use axum::http::{
	self, header::IntoHeaderName, response::Parts, HeaderValue, Response, StatusCode,
};

#[derive(Debug)]
pub struct InfallibleResponse(Parts);

impl InfallibleResponse {
	pub fn builder() -> Self {
		Self(Response::new(()).into_parts().0)
	}

	pub fn status(mut self, status: StatusCode) -> Self {
		self.0.status = status;
		self
	}

	pub fn header<K: IntoHeaderName>(mut self, key: K, val: HeaderValue) -> Self {
		self.0.headers.insert(key, val);
		self
	}

	pub fn body<B>(self, body: B) -> http::Response<B> {
		Response::from_parts(self.0, body)
	}
}



File: ./src/util/observable.rs
-------------------------------------------------
#![allow(dead_code)]

use std::{
	collections::hash_map::DefaultHasher,
	hash::{Hash, Hasher},
	ops::{Deref, DerefMut},
};

use tokio::sync::{Notify, RwLock, RwLockReadGuard, RwLockWriteGuard};

/// A simple JS-style observable in Rust
pub struct Observable<T> {
	t: RwLock<T>,
	notify: Notify,
}

impl<T> Observable<T>
where
	T: Hash,
{
	pub fn new(t: T) -> Self {
		Self {
			t: RwLock::new(t),
			notify: Notify::new(),
		}
	}

	pub async fn get_mut(&self) -> ObservableRef<'_, T> {
		let t = self.t.write().await;

		ObservableRef {
			start_hash: {
				let mut s = DefaultHasher::new();
				t.hash(&mut s);
				s.finish()
			},
			t,
			notify: &self.notify,
		}
	}

	pub async fn set(&self, t: T) {
		*self.get_mut().await = t;
	}

	pub async fn get(&self) -> RwLockReadGuard<'_, T> {
		self.t.read().await
	}

	/// Wait until the value changes, then return the new value
	pub async fn wait(&self) -> T
	where
		T: Clone,
	{
		self.notify.notified().await;
		self.t.read().await.clone()
	}
}

pub struct ObservableRef<'a, T>
where
	T: Hash,
{
	t: RwLockWriteGuard<'a, T>,
	notify: &'a Notify,
	start_hash: u64,
}

impl<T> Deref for ObservableRef<'_, T>
where
	T: Hash,
{
	type Target = T;

	fn deref(&self) -> &Self::Target {
		&self.t
	}
}

impl<T> DerefMut for ObservableRef<'_, T>
where
	T: Hash,
{
	fn deref_mut(&mut self) -> &mut Self::Target {
		&mut self.t
	}
}

impl<T> Drop for ObservableRef<'_, T>
where
	T: Hash,
{
	fn drop(&mut self) {
		let mut s = DefaultHasher::new();
		self.t.hash(&mut s);

		if self.start_hash != s.finish() {
			self.notify.notify_waiters();
		}
	}
}



File: ./src/util/version_manager.rs
-------------------------------------------------
use std::{
	any::type_name, fmt::Display, future::Future, num::ParseIntError, path::Path, str::FromStr,
};

use int_enum::{IntEnum, IntEnumError};
use itertools::Itertools;
use serde::{de::DeserializeOwned, Serialize};
use serde_json::{json, Map, Value};
use thiserror::Error;
use tokio::{fs, io};
use tracing::{debug, info, warn};

use super::error::FileIOError;

#[derive(Error, Debug)]
pub enum VersionManagerError<Version: IntEnum<Int = u64>> {
	#[error("version file does not exist")]
	VersionFileDoesNotExist,
	#[error("malformed version file, reason: {reason}")]
	MalformedVersionFile { reason: &'static str },
	#[error("unexpected migration: {current_version} -> {next_version}")]
	UnexpectedMigration {
		current_version: u64,
		next_version: u64,
	},
	#[error("failed to convert version to config file")]
	ConvertToConfig,

	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	ParseInt(#[from] ParseIntError),
	#[error(transparent)]
	SerdeJson(#[from] serde_json::Error),
	#[error(transparent)]
	IntConversion(#[from] IntEnumError<Version>),
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Kind {
	PlainText,
	Json(&'static str), // Version field name!
}

pub trait ManagedVersion<Version: IntEnum<Int = u64> + Display + Eq + Serialize + DeserializeOwned>:
	Serialize + DeserializeOwned + 'static
{
	const LATEST_VERSION: Version;

	const KIND: Kind;

	type MigrationError: std::error::Error + Display + From<VersionManagerError<Version>> + 'static;

	fn from_latest_version() -> Option<Self> {
		None
	}
}

/// An abstract system for saving a text file containing a version number.
/// The version number is an integer that can be converted to and from an enum.
/// The enum must implement the IntEnum trait.
pub struct VersionManager<
	Config: ManagedVersion<Version>,
	Version: IntEnum<Int = u64> + Display + Eq + Serialize + DeserializeOwned,
> {
	_marker: std::marker::PhantomData<(Config, Version)>,
}

impl<
		Config: ManagedVersion<Version>,
		Version: IntEnum<Int = u64> + Display + Eq + Serialize + DeserializeOwned,
	> VersionManager<Config, Version>
{
	async fn get_version(
		&self,
		version_file_path: impl AsRef<Path>,
	) -> Result<Version, VersionManagerError<Version>> {
		let version_file_path = version_file_path.as_ref();

		match Config::KIND {
			Kind::PlainText => match fs::read_to_string(version_file_path).await {
				Ok(contents) => {
					let version = u64::from_str(contents.trim())?;
					Version::from_int(version).map_err(Into::into)
				}
				Err(e) if e.kind() == io::ErrorKind::NotFound => {
					Err(VersionManagerError::VersionFileDoesNotExist)
				}
				Err(e) => Err(FileIOError::from((version_file_path, e)).into()),
			},
			Kind::Json(field) => match fs::read(version_file_path).await {
				Ok(bytes) => {
					let Some(version) = serde_json::from_slice::<Map<String, Value>>(&bytes)?
						.get(field)
						.and_then(|version| version.as_u64())
					else {
						return Err(VersionManagerError::MalformedVersionFile {
							reason: "missing version field",
						});
					};

					Version::from_int(version).map_err(Into::into)
				}
				Err(e) if e.kind() == io::ErrorKind::NotFound => {
					Err(VersionManagerError::VersionFileDoesNotExist)
				}
				Err(e) => Err(FileIOError::from((version_file_path, e)).into()),
			},
		}
	}

	async fn set_version(
		&self,
		version_file_path: impl AsRef<Path>,
		version: Version,
	) -> Result<(), VersionManagerError<Version>> {
		let version_file_path = version_file_path.as_ref();

		match Config::KIND {
			Kind::PlainText => fs::write(
				version_file_path,
				version.int_value().to_string().as_bytes(),
			)
			.await
			.map_err(|e| FileIOError::from((version_file_path, e)).into()),

			Kind::Json(field) => {
				let mut data_value = serde_json::from_slice::<Map<String, Value>>(
					&fs::read(version_file_path)
						.await
						.map_err(|e| FileIOError::from((version_file_path, e)))?,
				)?;

				data_value.insert(String::from(field), json!(version.int_value()));

				fs::write(version_file_path, serde_json::to_vec(&data_value)?)
					.await
					.map_err(|e| FileIOError::from((version_file_path, e)).into())
			}
		}
	}

	pub async fn migrate_and_load<Fut>(
		version_file_path: impl AsRef<Path>,
		migrate_fn: impl Fn(Version, Version) -> Fut,
	) -> Result<Config, Config::MigrationError>
	where
		Fut: Future<Output = Result<(), Config::MigrationError>>,
	{
		let version_file_path = version_file_path.as_ref();

		let this = VersionManager {
			_marker: std::marker::PhantomData::<(Config, Version)>,
		};

		let current = match this.get_version(version_file_path).await {
			Ok(version) => version,
			Err(VersionManagerError::VersionFileDoesNotExist) => {
				warn!(
					"Config file for {} does not exist, trying to create a new one with version -> {}",
					type_name::<Config>(),
					Config::LATEST_VERSION
				);

				let Some(latest_config) = Config::from_latest_version() else {
					return Err(VersionManagerError::VersionFileDoesNotExist.into());
				};

				fs::write(
					version_file_path,
					match Config::KIND {
						Kind::PlainText => Config::LATEST_VERSION
							.int_value()
							.to_string()
							.as_bytes()
							.to_vec(),
						Kind::Json(_) => serde_json::to_vec(&latest_config)
							.map_err(|e| VersionManagerError::SerdeJson(e))?,
					},
				)
				.await
				.map_err(|e| {
					VersionManagerError::FileIO(FileIOError::from((version_file_path, e)))
				})?;

				return Ok(latest_config);
			}
			Err(e) => return Err(e.into()),
		};

		if current != Config::LATEST_VERSION {
			for (current_version, next_version) in
				(current.int_value()..=Config::LATEST_VERSION.int_value()).tuple_windows()
			{
				let (current, next) = (
					Version::from_int(current_version).map_err(VersionManagerError::from)?,
					Version::from_int(next_version).map_err(VersionManagerError::from)?,
				);

				info!(
					"Running {} migrator: {current} -> {next}",
					type_name::<Config>()
				);
				migrate_fn(current, next).await?;
			}

			this.set_version(version_file_path, Config::LATEST_VERSION)
				.await?;
		} else {
			debug!("No migration required for {}", type_name::<Config>());
		}

		fs::read(version_file_path)
			.await
			.map_err(|e| {
				VersionManagerError::FileIO(FileIOError::from((version_file_path, e))).into()
			})
			.and_then(|bytes| {
				serde_json::from_slice(&bytes).map_err(|e| VersionManagerError::SerdeJson(e).into())
			})
	}
}



File: ./src/util/error.rs
-------------------------------------------------
use std::{io, path::Path};

use thiserror::Error;

#[derive(Debug, Error)]
#[error("error accessing path: '{}'", .path.display())]
pub struct FileIOError {
	pub path: Box<Path>,
	#[source]
	pub source: io::Error,
	pub maybe_context: Option<&'static str>,
}

impl<P: AsRef<Path>> From<(P, io::Error)> for FileIOError {
	fn from((path, source): (P, io::Error)) -> Self {
		Self {
			path: path.as_ref().into(),
			source,
			maybe_context: None,
		}
	}
}

impl<P: AsRef<Path>> From<(P, io::Error, &'static str)> for FileIOError {
	fn from((path, source, context): (P, io::Error, &'static str)) -> Self {
		Self {
			path: path.as_ref().into(),
			source,
			maybe_context: Some(context),
		}
	}
}

impl From<FileIOError> for rspc::Error {
	fn from(value: FileIOError) -> Self {
		Self::with_cause(
			rspc::ErrorCode::InternalServerError,
			value
				.maybe_context
				.unwrap_or("Error accessing file system")
				.to_string(),
			value,
		)
	}
}

#[derive(Debug, Error)]
#[error("received a non UTF-8 path: <lossy_path='{}'>", .0.to_string_lossy())]
pub struct NonUtf8PathError(pub Box<Path>);



File: ./src/util/debug_initializer.rs
-------------------------------------------------
// ! A system for loading a default set of data on startup. This is ONLY enabled in development builds.

use crate::{
	job::JobManagerError,
	library::Libraries,
	library::{LibraryManagerError, LibraryName},
	location::{
		delete_location, scan_location, LocationCreateArgs, LocationError, LocationManagerError,
	},
	prisma::location,
	util::AbortOnDrop,
	Node,
};

use std::{
	io,
	path::{Path, PathBuf},
	sync::Arc,
	time::Duration,
};

use prisma_client_rust::QueryError;
use serde::Deserialize;
use thiserror::Error;
use tokio::{
	fs::{self, metadata},
	time::sleep,
};
use tracing::{info, warn};
use uuid::Uuid;

use super::error::FileIOError;

#[derive(Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct LocationInitConfig {
	path: String,
}

#[derive(Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct LibraryInitConfig {
	id: Uuid,
	name: LibraryName,
	description: Option<String>,
	#[serde(default)]
	reset_locations_on_startup: bool,
	locations: Vec<LocationInitConfig>,
}

#[derive(Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct InitConfig {
	#[serde(default)]
	reset_on_startup: bool,
	libraries: Vec<LibraryInitConfig>,
	#[serde(skip, default)]
	path: PathBuf,
}

#[derive(Error, Debug)]
pub enum InitConfigError {
	#[error("error parsing the init data: {0}")]
	Json(#[from] serde_json::Error),
	#[error("job manager: {0}")]
	JobManager(#[from] JobManagerError),
	#[error("location manager: {0}")]
	LocationManager(#[from] LocationManagerError),
	#[error("library manager: {0}")]
	LibraryManager(#[from] LibraryManagerError),
	#[error("query error: {0}")]
	QueryError(#[from] QueryError),
	#[error("location error: {0}")]
	LocationError(#[from] LocationError),
	#[error("failed to get current directory from environment: {0}")]
	CurrentDir(io::Error),

	#[error(transparent)]
	FileIO(#[from] FileIOError),
}

impl InitConfig {
	pub async fn load(data_dir: &Path) -> Result<Option<Self>, InitConfigError> {
		let path = std::env::current_dir()
			.map_err(InitConfigError::CurrentDir)?
			.join(std::env::var("SD_INIT_DATA").unwrap_or("sd_init.json".to_string()));

		if metadata(&path).await.is_ok() {
			let config = fs::read(&path)
				.await
				.map_err(|e| FileIOError::from((&path, e, "Failed to read init config file")))?;

			let mut config = serde_json::from_slice::<InitConfig>(&config)?;

			config.path = path;

			if config.reset_on_startup && metadata(data_dir).await.is_ok() {
				warn!("previous 'SD_DATA_DIR' was removed on startup!");
				fs::remove_dir_all(data_dir).await.map_err(|e| {
					FileIOError::from((data_dir, e, "Failed to remove data directory"))
				})?;
			}

			return Ok(Some(config));
		}

		Ok(None)
	}

	pub async fn apply(
		self,
		library_manager: &Arc<Libraries>,
		node: &Arc<Node>,
	) -> Result<(), InitConfigError> {
		info!("Initializing app from file: {:?}", self.path);

		for lib in self.libraries {
			let name = lib.name.to_string();
			let _guard = AbortOnDrop(tokio::spawn(async move {
				loop {
					info!("Initializing library '{name}' from 'sd_init.json'...");
					sleep(Duration::from_secs(1)).await;
				}
			}));

			let library = if let Some(lib) = library_manager.get_library(&lib.id).await {
				lib
			} else {
				let library = library_manager
					.create_with_uuid(lib.id, lib.name, lib.description, true, None, node)
					.await?;

				let Some(lib) = library_manager.get_library(&library.id).await else {
					warn!(
						"Debug init error: library '{}' was not found after being created!",
						library.config().await.name.as_ref()
					);
					return Ok(());
				};

				lib
			};

			if lib.reset_locations_on_startup {
				let locations = library.db.location().find_many(vec![]).exec().await?;

				for location in locations {
					warn!("deleting location: {:?}", location.path);
					delete_location(node, &library, location.id).await?;
				}
			}

			for loc in lib.locations {
				if let Some(location) = library
					.db
					.location()
					.find_first(vec![location::path::equals(Some(loc.path.clone()))])
					.exec()
					.await?
				{
					warn!("deleting location: {:?}", location.path);
					delete_location(node, &library, location.id).await?;
				}

				let sd_file = PathBuf::from(&loc.path).join(".spacedrive");

				if let Err(e) = fs::remove_file(sd_file).await {
					if e.kind() != io::ErrorKind::NotFound {
						warn!("failed to remove '.spacedrive' file: {:?}", e);
					}
				}

				if let Some(location) = (LocationCreateArgs {
					path: PathBuf::from(loc.path.clone()),
					dry_run: false,
					indexer_rules_ids: Vec::new(),
				})
				.create(node, &library)
				.await?
				{
					scan_location(node, &library, location).await?;
				} else {
					warn!(
						"Debug init error: location '{}' was not found after being created!",
						loc.path
					);
				}
			}
		}

		info!("Initialized app from file: {}", self.path.display());

		Ok(())
	}
}



File: ./src/util/maybe_undefined.rs
-------------------------------------------------
//! Copied from: https://docs.rs/async-graphql/latest/async_graphql/types/enum.MaybeUndefined.html
#![allow(unused)]

use serde::{Deserialize, Deserializer, Serialize, Serializer};
use specta::Type;

// This exports an incorrect Typescript type. https://github.com/oscartbeaumont/specta/issues/157
#[derive(Debug, Clone, Type)]
#[specta(untagged)]
pub enum MaybeUndefined<T> {
	Undefined,
	Null,
	Value(T),
}

impl<T> MaybeUndefined<T> {
	// `Null | Value(T)` will return `true` else `false`.
	pub fn is_defined(&self) -> bool {
		!matches!(self, Self::Undefined)
	}

	pub fn unwrap_or(self, t: T) -> T {
		match self {
			Self::Value(v) => v,
			_ => t,
		}
	}
}

impl<T> From<MaybeUndefined<T>> for Option<Option<T>> {
	fn from(v: MaybeUndefined<T>) -> Option<Option<T>> {
		match v {
			MaybeUndefined::Undefined => None,
			MaybeUndefined::Null => Some(None),
			MaybeUndefined::Value(v) => Some(Some(v)),
		}
	}
}

impl<T, E> MaybeUndefined<Result<T, E>> {
	/// Transposes a `MaybeUndefined` of a [`Result`] into a [`Result`] of a
	/// `MaybeUndefined`.
	///
	/// [`MaybeUndefined::Undefined`] will be mapped to
	/// [`Ok`]`(`[`MaybeUndefined::Undefined`]`)`. [`MaybeUndefined::Null`]
	/// will be mapped to [`Ok`]`(`[`MaybeUndefined::Null`]`)`.
	/// [`MaybeUndefined::Value`]`(`[`Ok`]`(_))` and
	/// [`MaybeUndefined::Value`]`(`[`Err`]`(_))` will be mapped to
	/// [`Ok`]`(`[`MaybeUndefined::Value`]`(_))` and [`Err`]`(_)`.
	#[inline]
	pub fn transpose(self) -> Result<MaybeUndefined<T>, E> {
		match self {
			MaybeUndefined::Undefined => Ok(MaybeUndefined::Undefined),
			MaybeUndefined::Null => Ok(MaybeUndefined::Null),
			MaybeUndefined::Value(Ok(v)) => Ok(MaybeUndefined::Value(v)),
			MaybeUndefined::Value(Err(e)) => Err(e),
		}
	}
}

impl<T: Serialize> Serialize for MaybeUndefined<T> {
	fn serialize<S: Serializer>(&self, serializer: S) -> Result<S::Ok, S::Error> {
		match self {
			MaybeUndefined::Value(value) => value.serialize(serializer),
			_ => serializer.serialize_none(),
		}
	}
}

impl<'de, T> Deserialize<'de> for MaybeUndefined<T>
where
	T: Deserialize<'de>,
{
	fn deserialize<D>(deserializer: D) -> Result<MaybeUndefined<T>, D::Error>
	where
		D: Deserializer<'de>,
	{
		Option::<T>::deserialize(deserializer).map(|value| match value {
			Some(value) => MaybeUndefined::Value(value),
			None => MaybeUndefined::Null,
		})
	}
}



File: ./src/util/db.rs
-------------------------------------------------
use crate::prisma::{self, PrismaClient};
use prisma_client_rust::{migrations::*, NewClientError};
use thiserror::Error;

/// MigrationError represents an error that occurring while opening a initialising and running migrations on the database.
#[derive(Error, Debug)]
pub enum MigrationError {
	#[error("An error occurred while initialising a new database connection: {0}")]
	NewClient(#[from] Box<NewClientError>),
	#[cfg(debug_assertions)]
	#[error("An error occurred during migration: {0}")]
	MigrateFailed(#[from] DbPushError),
	#[cfg(not(debug_assertions))]
	#[error("An error occurred during migration: {0}")]
	MigrateFailed(#[from] MigrateDeployError),
}

/// load_and_migrate will load the database from the given path and migrate it to the latest version of the schema.
pub async fn load_and_migrate(db_url: &str) -> Result<PrismaClient, MigrationError> {
	let client = prisma::new_client_with_url(db_url)
		.await
		.map_err(Box::new)?;

	#[cfg(debug_assertions)]
	{
		let mut builder = client._db_push();

		if std::env::var("SD_ACCEPT_DATA_LOSS")
			.map(|v| v == "true")
			.unwrap_or(false)
		{
			builder = builder.accept_data_loss();
		}

		if std::env::var("SD_FORCE_RESET_DB")
			.map(|v| v == "true")
			.unwrap_or(false)
		{
			builder = builder.force_reset();
		}

		let res = builder.await;

		match res {
			Ok(_) => {}
			Err(e @ DbPushError::PossibleDataLoss(_)) => {
				eprintln!("Pushing Prisma schema may result in data loss. Use `SD_ACCEPT_DATA_LOSS=true` to force it.");
				Err(e)?;
			}
			Err(e) => Err(e)?,
		}
	}

	#[cfg(not(debug_assertions))]
	client._migrate_deploy().await?;

	Ok(client)
}

pub fn inode_from_db(db_inode: &[u8]) -> u64 {
	u64::from_le_bytes(db_inode.try_into().expect("corrupted inode in database"))
}

pub fn inode_to_db(inode: u64) -> Vec<u8> {
	inode.to_le_bytes().to_vec()
}

#[derive(Error, Debug)]
#[error("Missing field {0}")]
pub struct MissingFieldError(&'static str);

impl MissingFieldError {
	#[must_use]
	pub const fn new(value: &'static str) -> Self {
		Self(value)
	}
}

impl From<MissingFieldError> for rspc::Error {
	fn from(value: MissingFieldError) -> Self {
		rspc::Error::with_cause(
			rspc::ErrorCode::InternalServerError,
			"Missing crucial data in the database".to_string(),
			value,
		)
	}
}

pub trait OptionalField: Sized {
	type Out;

	fn transform(self) -> Option<Self::Out>;
}

impl<T> OptionalField for Option<T> {
	type Out = T;

	fn transform(self) -> Option<T> {
		self
	}
}
impl<'a, T> OptionalField for &'a Option<T> {
	type Out = &'a T;

	fn transform(self) -> Option<Self::Out> {
		self.as_ref()
	}
}

pub fn maybe_missing<T: OptionalField>(
	data: T,
	field: &'static str,
) -> Result<T::Out, MissingFieldError> {
	data.transform().ok_or(MissingFieldError(field))
}



File: ./src/util/mod.rs
-------------------------------------------------
mod abort_on_drop;
pub mod db;
#[cfg(debug_assertions)]
pub mod debug_initializer;
pub mod error;
pub mod http;
mod infallible_request;
mod maybe_undefined;
pub mod mpscrr;
mod observable;
pub mod version_manager;

pub use abort_on_drop::*;
pub use infallible_request::*;
pub use maybe_undefined::*;
pub use observable::*;



File: ./src/util/abort_on_drop.rs
-------------------------------------------------
use futures::{pin_mut, Future, Stream};

pub struct AbortOnDrop<T>(pub tokio::task::JoinHandle<T>);

impl<T> Drop for AbortOnDrop<T> {
	fn drop(&mut self) {
		self.0.abort()
	}
}

impl<T> Future for AbortOnDrop<T> {
	type Output = Result<T, tokio::task::JoinError>;

	fn poll(
		mut self: std::pin::Pin<&mut Self>,
		cx: &mut std::task::Context<'_>,
	) -> std::task::Poll<Self::Output> {
		let handle = &mut self.0;

		pin_mut!(handle);

		handle.poll(cx)
	}
}

impl<T> Stream for AbortOnDrop<T> {
	type Item = ();

	fn poll_next(
		mut self: std::pin::Pin<&mut Self>,
		cx: &mut std::task::Context<'_>,
	) -> std::task::Poll<Option<Self::Item>> {
		let handle = &mut self.0;

		pin_mut!(handle);

		handle.poll(cx).map(|_| None)
	}
}



File: ./src/util/http.rs
-------------------------------------------------
use reqwest::Response;

pub fn ensure_response(resp: Response) -> Result<Response, rspc::Error> {
	resp.error_for_status()
		.map_err(|e| rspc::Error::new(rspc::ErrorCode::InternalServerError, e.to_string()))
}



File: ./src/util/mpscrr.rs
-------------------------------------------------
//! A multi-producer single-consumer channel (mpsc) with a strongly consistent emit method.
//!
//! What does this mean? Well, any call to [Sender::emit] will not resolve it's future until all active [Receiver]'s have received the value and returned from their callback.
//!
//! Why would you want this? U want to emit a message on a channel and ensure it has been processed by the subscribers before continuing.
//!
//! Things to be aware of:
//!  - Receiver's are lazily registered. Eg. `let rx2 = rx.clone();` will only be required to receive values if [Receiver::subscribe_one] or [Receiver::subscribe] is called on it.
//!  - Panic in a receiver will cause the sender to ignore that receiver. It will not infinitely block on it.
//!
//! ## Example
//!
//! ```rust
//! use sd_core::util::mpscrr;
//!
//! # tokio::runtime::Runtime::new().unwrap().block_on(async {
//! let (tx, mut rx) = mpscrr::unbounded_channel::<i32, i32>();
//!
//! tokio::spawn(async move {
//!     rx.subscribe(|value| async move {
//!          assert_eq!(value, 42);
//!
//!          1
//!     })
//!     .await
//!     .unwrap();
//! });
//!
//! // Wait for Tokio to spawn the tasks
//! tokio::time::sleep(std::time::Duration::from_millis(200)).await;
//!
//! let result: Vec<i32> = tx.emit(42).await;
//! assert_eq!(result, vec![1]);
//! # });
//! ```
//!

// We ignore Mutex poising as the code is written such that it will not break any invariants if it panics.
// Keep this in mind!

use std::{
	fmt,
	future::Future,
	sync::{
		atomic::{AtomicBool, Ordering},
		Arc, PoisonError, RwLock,
	},
};

use futures::future::join_all;
use slotmap::{DefaultKey, SlotMap};
use tokio::sync::{mpsc, oneshot};

pub type Pair<T, U> = (Sender<T, U>, Receiver<T, U>);

type MpscInnerTy<T, U> = (T, oneshot::Sender<U>);

type Slots<T, U> =
	Arc<RwLock<SlotMap<DefaultKey, (mpsc::UnboundedSender<MpscInnerTy<T, U>>, Arc<AtomicBool>)>>>;

enum SenderError {
	/// Receiver was dropped, so remove it.
	Finished(DefaultKey),
	/// Receiver failed to respond but is still assumed active.
	Ignored,
}

/// Returned by a [Receiver] when the [Sender] is dropped while trying to receive a value.
pub struct RecvError {}

impl fmt::Debug for RecvError {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		f.write_str("RecvError")
	}
}

#[derive(Debug)]
pub struct Sender<T, U>(Slots<T, U>);

impl<T: Clone, U> Sender<T, U> {
	pub async fn emit(&self, value: T) -> Vec<U> {
		// This is annoying AF but holding a mutex guard over await boundary is `!Sync` which will break code using this.
		let map = self
			.0
			.read()
			.unwrap_or_else(PoisonError::into_inner)
			.iter()
			.map(|(k, v)| (k, v.clone()))
			.collect::<Vec<_>>();

		join_all(map.into_iter().filter_map(|(key, (sender, active))| {
			if !active.load(Ordering::Relaxed) {
				// The receiver has no callback registered so we ignore it.
				return None;
			}

			let value = value.clone();
			Some(async move {
				let (tx, rx) = oneshot::channel();
				if sender.send((value, tx)).is_err() {
					// The receiver was dropped so we remove it from the map
					Err(SenderError::Finished(key))
				} else {
					// If oneshot was dropped we ignore this subscriber as something went wrong with it.
					// It is assumed the mpsc is fine and if it's not it will be cleared up by it's `Drop` or the next `emit`.
					rx.await.map_err(|_| SenderError::Ignored)
				}
			})
		}))
		.await
		.into_iter()
		.filter_map(|x| {
			x.map_err(|err| match err {
				SenderError::Finished(key) => {
					self.0
						.write()
						.unwrap_or_else(PoisonError::into_inner)
						.remove(key);
				}
				SenderError::Ignored => {}
			})
			.ok()
		})
		.collect::<Vec<_>>()
	}
}

#[derive(Debug)]
pub struct Receiver<T, U> {
	slots: Slots<T, U>,
	entry: DefaultKey,
	rx: mpsc::UnboundedReceiver<MpscInnerTy<T, U>>,
	active: Arc<AtomicBool>,
}

impl<T, U> Receiver<T, U> {
	/// This method will call the callback for the next value sent to the channel.
	///
	/// It will block until the next message than return.
	///
	/// If the sender is dropped this will return an error else it will return itself.
	/// This is to avoid using the subscription after the sender is dropped.
	pub async fn subscribe_one<'a, Fu: Future<Output = U> + 'a>(
		mut self,
		func: impl FnOnce(T) -> Fu + 'a,
	) -> Result<Self, RecvError> {
		let _bomb = Bomb::new(&self.active);

		let (value, tx) = self.rx.recv().await.ok_or(RecvError {})?;
		tx.send(func(value).await).map_err(|_| RecvError {})?;

		drop(_bomb);
		Ok(self)
	}

	/// This method will call the callback for every value sent to the channel.
	///
	/// It will block the active task until the sender is dropped.
	///
	/// If the sender is dropped this will return an error.
	pub async fn subscribe<'a, Fu: Future<Output = U> + 'a>(
		mut self,
		mut func: impl FnMut(T) -> Fu + 'a,
	) -> Result<(), RecvError> {
		let _bomb = Bomb::new(&self.active);

		loop {
			let (value, tx) = self.rx.recv().await.ok_or(RecvError {})?;
			tx.send(func(value).await).map_err(|_| RecvError {})?;
		}
	}
}

impl<T, U> Drop for Receiver<T, U> {
	fn drop(&mut self) {
		self.slots
			.write()
			.unwrap_or_else(PoisonError::into_inner)
			.remove(self.entry);
	}
}

/// Construct a new unbounded channel.
pub fn unbounded_channel<T, U>() -> (Sender<T, U>, Receiver<T, U>) {
	let mut map = SlotMap::new();

	// Create first receiver
	let (tx, rx) = mpsc::unbounded_channel();
	let active: Arc<AtomicBool> = Arc::default();
	let entry = map.insert((tx, active.clone()));

	let slots = Arc::new(RwLock::new(map));
	(
		Sender(slots.clone()),
		Receiver {
			slots,
			entry,
			rx,
			active,
		},
	)
}

impl<T, U> Clone for Receiver<T, U> {
	fn clone(&self) -> Self {
		let (tx, rx) = mpsc::unbounded_channel();
		let active: Arc<AtomicBool> = Arc::default();
		let entry = self
			.slots
			.write()
			.unwrap_or_else(PoisonError::into_inner)
			.insert((tx, active.clone()));

		Self {
			slots: self.slots.clone(),
			entry,
			rx,
			active,
		}
	}
}

// Bomb exists so on panic the `active` flag is reset to false.
struct Bomb<'a>(&'a AtomicBool);

impl<'a> Bomb<'a> {
	pub fn new(b: &'a AtomicBool) -> Self {
		b.store(true, Ordering::Relaxed);
		Self(b)
	}
}

impl<'a> Drop for Bomb<'a> {
	fn drop(&mut self) {
		self.0.store(false, Ordering::Relaxed);
	}
}

#[cfg(test)]
mod tests {
	use std::{sync::Arc, time::Duration};

	use aovec::Aovec;

	// Not using super because `use super as mpscrr` doesn't work :(
	use crate::util::mpscrr;

	#[derive(Debug, Clone, PartialEq, Eq)]
	enum Step {
		Send,
		RecvA,
		RecvB,
		SendComplete,
	}

	#[tokio::test]
	async fn test_mpscrr() {
		let stack = Arc::new(Aovec::new(5));

		let (tx, rx) = mpscrr::unbounded_channel::<u8, u8>();

		tokio::spawn({
			let rx = rx.clone();
			let stack = stack.clone();

			async move {
				rx.subscribe(|value| {
					let stack = stack.clone();

					async move {
						assert_eq!(value, 42);
						stack.push(Step::RecvA);
						1
					}
				})
				.await
				.unwrap();

				// assert!(true, "recv a closed");
			}
		});

		tokio::spawn({
			let rx = rx.clone();
			let stack = stack.clone();

			async move {
				rx.subscribe(|value| {
					let stack = stack.clone();

					async move {
						assert_eq!(value, 42);
						stack.push(Step::RecvB);
						2
					}
				})
				.await
				.unwrap();

				// assert!(true, "recv b closed");
			}
		});

		// Test unsubscribed receiver doesn't cause `.emit` to hang
		let rx3 = rx;

		tokio::time::sleep(Duration::from_millis(200)).await; // Wait for Tokio to spawn the tasks

		stack.push(Step::Send);
		let result = tx.emit(42).await;
		stack.push(Step::SendComplete);
		drop(rx3);

		// Check responses -> U shouldn't should NEVER assume order but we do here for simplicity
		assert_eq!(result, vec![1, 2]);
		// Check the order of operations
		assert_eq!(
			&aovec_to_vec(&stack),
			&[Step::Send, Step::RecvA, Step::RecvB, Step::SendComplete,]
		)
	}

	fn aovec_to_vec<T: Clone>(a: &Aovec<T>) -> Vec<T> {
		let mut v = Vec::with_capacity(a.len());
		for i in 0..a.len() {
			v.push(a.get(i).unwrap().clone());
		}
		v
	}
}



File: ./src/env.rs
-------------------------------------------------
pub struct Env {
	pub api_url: String,
	pub client_id: String,
}



File: ./src/location/archive/archive_job.rs
-------------------------------------------------



File: ./src/location/archive/mod.rs
-------------------------------------------------



File: ./src/location/error.rs
-------------------------------------------------
use crate::{
	prisma::location,
	util::{
		db::MissingFieldError,
		error::{FileIOError, NonUtf8PathError},
	},
};

use std::path::Path;

use rspc::{self, ErrorCode};
use thiserror::Error;
use uuid::Uuid;

use super::{
	file_path_helper::FilePathError, manager::LocationManagerError, metadata::LocationMetadataError,
};

/// Error type for location related errors
#[derive(Error, Debug)]
pub enum LocationError {
	// Not Found errors
	#[error("location not found <path='{}'>", .0.display())]
	PathNotFound(Box<Path>),
	#[error("location not found <uuid='{0}'>")]
	UuidNotFound(Uuid),
	#[error("location not found <id='{0}'>")]
	IdNotFound(location::id::Type),

	// User errors
	#[error("location not a directory <path='{}'>", .0.display())]
	NotDirectory(Box<Path>),
	#[error("could not find directory in location <path='{}'>", .0.display())]
	DirectoryNotFound(Box<Path>),
	#[error(
		"library exists in the location metadata file, must relink <old_path='{}', new_path='{}'>",
		.old_path.display(),
		.new_path.display(),
	)]
	NeedRelink {
		old_path: Box<Path>,
		new_path: Box<Path>,
	},
	#[error(
		"this location belongs to another library, must update .spacedrive file <path='{}'>",
		.0.display()
	)]
	AddLibraryToMetadata(Box<Path>),
	#[error("location metadata file not found <path='{}'>", .0.display())]
	MetadataNotFound(Box<Path>),
	#[error("location already exists in database <path='{}'>", .0.display())]
	LocationAlreadyExists(Box<Path>),
	#[error("nested location currently not supported <path='{}'>", .0.display())]
	NestedLocation(Box<Path>),
	#[error(transparent)]
	NonUtf8Path(#[from] NonUtf8PathError),

	// Internal Errors
	#[error(transparent)]
	LocationMetadata(#[from] LocationMetadataError),
	#[error("failed to read location path metadata info: {0}")]
	LocationPathFilesystemMetadataAccess(FileIOError),
	#[error("missing metadata file for location <path='{}'>", .0.display())]
	MissingMetadataFile(Box<Path>),
	#[error("failed to open file from local OS: {0}")]
	FileRead(FileIOError),
	#[error("failed to read mounted volumes from local OS: {0}")]
	VolumeReadError(String),
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error(transparent)]
	LocationManager(#[from] LocationManagerError),
	#[error(transparent)]
	FilePath(#[from] FilePathError),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error("location missing path <id='{0}'>")]
	MissingPath(location::id::Type),
	#[error("missing-field: {0}")]
	MissingField(#[from] MissingFieldError),
}

impl From<LocationError> for rspc::Error {
	fn from(err: LocationError) -> Self {
		use LocationError::*;

		match err {
			// Not found errors
			PathNotFound(_)
			| UuidNotFound(_)
			| IdNotFound(_)
			| FilePath(FilePathError::IdNotFound(_) | FilePathError::NotFound(_)) => {
				Self::with_cause(ErrorCode::NotFound, err.to_string(), err)
			}

			// User's fault errors
			NotDirectory(_) | NestedLocation(_) | LocationAlreadyExists(_) => {
				Self::with_cause(ErrorCode::BadRequest, err.to_string(), err)
			}

			// Custom error message is used to differenciate these errors in the frontend
			// TODO: A better solution would be for rspc to support sending custom data alongside errors
			NeedRelink { .. } => {
				Self::with_cause(ErrorCode::Conflict, "NEED_RELINK".to_owned(), err)
			}
			AddLibraryToMetadata(_) => {
				Self::with_cause(ErrorCode::Conflict, "ADD_LIBRARY".to_owned(), err)
			}

			// Internal errors
			MissingField(missing_error) => missing_error.into(),
			_ => Self::with_cause(ErrorCode::InternalServerError, err.to_string(), err),
		}
	}
}



File: ./src/location/metadata.rs
-------------------------------------------------
use crate::library::LibraryId;

use std::{
	collections::{HashMap, HashSet},
	path::{Path, PathBuf},
};

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::{fs, io};
use tracing::error;
use uuid::Uuid;

use super::LocationPubId;

static SPACEDRIVE_LOCATION_METADATA_FILE: &str = ".spacedrive";

#[derive(Serialize, Deserialize, Default, Debug)]
struct LocationMetadata {
	pub_id: LocationPubId,
	name: String,
	path: PathBuf,
	created_at: DateTime<Utc>,
	updated_at: DateTime<Utc>,
}

#[derive(Serialize, Deserialize, Default, Debug)]
struct SpacedriveLocationMetadata {
	libraries: HashMap<LibraryId, LocationMetadata>,
	created_at: DateTime<Utc>,
	updated_at: DateTime<Utc>,
}

pub struct SpacedriveLocationMetadataFile {
	path: PathBuf,
	metadata: SpacedriveLocationMetadata,
}

impl SpacedriveLocationMetadataFile {
	pub async fn try_load(
		location_path: impl AsRef<Path>,
	) -> Result<Option<Self>, LocationMetadataError> {
		let metadata_file_name = location_path
			.as_ref()
			.join(SPACEDRIVE_LOCATION_METADATA_FILE);

		match fs::read(&metadata_file_name).await {
			Ok(data) => Ok(Some(Self {
				metadata: match serde_json::from_slice(&data) {
					Ok(data) => data,
					Err(e) => {
						#[cfg(debug_assertions)]
						{
							error!(
								"Failed to deserialize corrupted metadata file, \
								we will remove it and create a new one; File: {}; Error: {e}",
								metadata_file_name.display()
							);
							fs::remove_file(&metadata_file_name).await.map_err(|e| {
								LocationMetadataError::Delete(
									e,
									location_path.as_ref().to_path_buf(),
								)
							})?;

							return Ok(None);
						}

						#[cfg(not(debug_assertions))]
						return Err(LocationMetadataError::Deserialize(
							e,
							location_path.as_ref().to_path_buf(),
						));
					}
				},
				path: metadata_file_name,
			})),
			Err(e) if e.kind() == io::ErrorKind::NotFound => Ok(None),
			Err(e) => Err(LocationMetadataError::Read(
				e,
				location_path.as_ref().to_path_buf(),
			)),
		}
	}

	pub async fn create_and_save(
		library_id: LibraryId,
		location_pub_id: Uuid,
		location_path: impl AsRef<Path>,
		location_name: String,
	) -> Result<(), LocationMetadataError> {
		Self {
			path: location_path
				.as_ref()
				.join(SPACEDRIVE_LOCATION_METADATA_FILE),
			metadata: SpacedriveLocationMetadata {
				libraries: [(
					library_id,
					LocationMetadata {
						pub_id: location_pub_id,
						name: location_name,
						path: location_path.as_ref().to_path_buf(),
						created_at: Utc::now(),
						updated_at: Utc::now(),
					},
				)]
				.into_iter()
				.collect(),
				created_at: Utc::now(),
				updated_at: Utc::now(),
			},
		}
		.write_metadata()
		.await
	}

	pub async fn relink(
		&mut self,
		library_id: LibraryId,
		location_path: impl AsRef<Path>,
	) -> Result<(), LocationMetadataError> {
		let location_metadata = self
			.metadata
			.libraries
			.get_mut(&library_id)
			.ok_or(LocationMetadataError::LibraryNotFound(library_id))?;

		let new_path = location_path.as_ref().to_path_buf();
		if location_metadata.path == new_path {
			return Err(LocationMetadataError::RelinkSamePath(new_path));
		}

		location_metadata.path = new_path;
		location_metadata.updated_at = Utc::now();
		self.path = location_path
			.as_ref()
			.join(SPACEDRIVE_LOCATION_METADATA_FILE);

		self.write_metadata().await
	}

	pub async fn update(
		&mut self,
		library_id: LibraryId,
		location_name: String,
	) -> Result<(), LocationMetadataError> {
		let location_metadata = self
			.metadata
			.libraries
			.get_mut(&library_id)
			.ok_or(LocationMetadataError::LibraryNotFound(library_id))?;

		location_metadata.name = location_name;
		location_metadata.updated_at = Utc::now();

		self.write_metadata().await
	}

	pub async fn add_library(
		&mut self,
		library_id: LibraryId,
		location_pub_id: Uuid,
		location_path: impl AsRef<Path>,
		location_name: String,
	) -> Result<(), LocationMetadataError> {
		self.metadata.libraries.insert(
			library_id,
			LocationMetadata {
				pub_id: location_pub_id,
				name: location_name,
				path: location_path.as_ref().to_path_buf(),
				created_at: Utc::now(),
				updated_at: Utc::now(),
			},
		);

		self.metadata.updated_at = Utc::now();
		self.write_metadata().await
	}

	pub fn has_library(&self, library_id: LibraryId) -> bool {
		self.metadata.libraries.contains_key(&library_id)
	}

	pub fn location_path(&self, library_id: LibraryId) -> Option<&Path> {
		self.metadata
			.libraries
			.get(&library_id)
			.map(|l| l.path.as_path())
	}

	pub fn is_empty(&self) -> bool {
		self.metadata.libraries.is_empty()
	}

	pub async fn remove_library(
		&mut self,
		library_id: LibraryId,
	) -> Result<(), LocationMetadataError> {
		self.metadata
			.libraries
			.remove(&library_id)
			.ok_or(LocationMetadataError::LibraryNotFound(library_id))?;

		self.metadata.updated_at = Utc::now();

		if !self.metadata.libraries.is_empty() {
			self.write_metadata().await
		} else {
			fs::remove_file(&self.path)
				.await
				.map_err(|e| LocationMetadataError::Delete(e, self.path.clone()))
		}
	}

	pub async fn clean_stale_libraries(
		&mut self,
		existing_libraries_ids: &HashSet<LibraryId>,
	) -> Result<(), LocationMetadataError> {
		let previous_libraries_count = self.metadata.libraries.len();
		self.metadata
			.libraries
			.retain(|library_id, _| existing_libraries_ids.contains(library_id));

		if self.metadata.libraries.len() != previous_libraries_count {
			self.metadata.updated_at = Utc::now();

			if !self.metadata.libraries.is_empty() {
				self.write_metadata().await
			} else {
				fs::remove_file(&self.path)
					.await
					.map_err(|e| LocationMetadataError::Delete(e, self.path.clone()))
			}
		} else {
			Ok(())
		}
	}

	pub fn location_pub_id(&self, library_id: LibraryId) -> Result<Uuid, LocationMetadataError> {
		self.metadata
			.libraries
			.get(&library_id)
			.ok_or(LocationMetadataError::LibraryNotFound(library_id))
			.map(|m| m.pub_id)
	}

	async fn write_metadata(&self) -> Result<(), LocationMetadataError> {
		fs::write(
			&self.path,
			serde_json::to_vec(&self.metadata)
				.map_err(|e| LocationMetadataError::Serialize(e, self.path.clone()))?,
		)
		.await
		.map_err(|e| LocationMetadataError::Write(e, self.path.clone()))
	}
}

#[derive(Error, Debug)]
pub enum LocationMetadataError {
	#[error("Library not found: {0}")]
	LibraryNotFound(LibraryId),
	#[error("Failed to read location metadata file (path: {1:?}); (error: {0:?})")]
	Read(io::Error, PathBuf),
	#[error("Failed to delete location metadata file (path: {1:?}); (error: {0:?})")]
	Delete(io::Error, PathBuf),
	#[error("Failed to serialize metadata file for location (at path: {1:?}); (error: {0:?})")]
	Serialize(serde_json::Error, PathBuf),
	#[error("Failed to write location metadata file (path: {1:?}); (error: {0:?})")]
	Write(io::Error, PathBuf),
	#[error("Failed to deserialize metadata file for location (at path: {1:?}); (error: {0:?})")]
	Deserialize(serde_json::Error, PathBuf),
	#[error("Failed to relink, as the new location path is the same as the old path: {0}")]
	RelinkSamePath(PathBuf),
}



File: ./src/location/non_indexed.rs
-------------------------------------------------
use crate::{
	api::locations::ExplorerItem,
	library::Library,
	object::{
		cas::generate_cas_id,
		media::thumbnail::{get_ephemeral_thumb_key, BatchToProcess, GenerateThumbnailArgs},
	},
	prisma::location,
	util::error::FileIOError,
	Node,
};

use std::{
	collections::HashMap,
	path::{Path, PathBuf},
	sync::Arc,
};

use sd_file_ext::{extensions::Extension, kind::ObjectKind};

use chrono::{DateTime, Utc};
use rspc::ErrorCode;
use sd_utils::chain_optional_iter;
use serde::Serialize;
use specta::Type;
use thiserror::Error;
use tokio::{fs, io};
use tracing::{error, warn};

use super::{
	file_path_helper::{path_is_hidden, MetadataExt},
	indexer::rules::{
		seed::{no_hidden, no_os_protected},
		IndexerRule, RuleKind,
	},
	normalize_path,
};

#[derive(Debug, Error)]
pub enum NonIndexedLocationError {
	#[error("path not found: {}", .0.display())]
	NotFound(PathBuf),

	#[error(transparent)]
	FileIO(#[from] FileIOError),

	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
}

impl From<NonIndexedLocationError> for rspc::Error {
	fn from(err: NonIndexedLocationError) -> Self {
		match err {
			NonIndexedLocationError::NotFound(_) => {
				rspc::Error::with_cause(ErrorCode::NotFound, err.to_string(), err)
			}
			_ => rspc::Error::with_cause(ErrorCode::InternalServerError, err.to_string(), err),
		}
	}
}

impl<P: AsRef<Path>> From<(P, io::Error)> for NonIndexedLocationError {
	fn from((path, source): (P, io::Error)) -> Self {
		if source.kind() == io::ErrorKind::NotFound {
			Self::NotFound(path.as_ref().into())
		} else {
			Self::FileIO(FileIOError::from((path, source)))
		}
	}
}

#[derive(Serialize, Type, Debug)]
pub struct NonIndexedFileSystemEntries {
	pub entries: Vec<ExplorerItem>,
	pub errors: Vec<rspc::Error>,
}

#[derive(Serialize, Type, Debug)]
pub struct NonIndexedPathItem {
	pub path: String,
	pub name: String,
	pub extension: String,
	pub kind: i32,
	pub is_dir: bool,
	pub date_created: DateTime<Utc>,
	pub date_modified: DateTime<Utc>,
	pub size_in_bytes_bytes: Vec<u8>,
	pub hidden: bool,
}

pub async fn walk(
	full_path: impl AsRef<Path>,
	with_hidden_files: bool,
	node: Arc<Node>,
	library: Arc<Library>,
) -> Result<NonIndexedFileSystemEntries, NonIndexedLocationError> {
	let path = full_path.as_ref();
	let mut read_dir = fs::read_dir(path).await.map_err(|e| (path, e))?;

	let mut directories = vec![];
	let mut errors = vec![];
	let mut entries = vec![];

	let rules = chain_optional_iter(
		[IndexerRule::from(no_os_protected())],
		[(!with_hidden_files).then(|| IndexerRule::from(no_hidden()))],
	);

	let mut thumbnails_to_generate = vec![];
	// Generating thumbnails for PDFs is kinda slow, so we're leaving them for last in the batch
	let mut document_thumbnails_to_generate = vec![];

	while let Some(entry) = read_dir.next_entry().await.map_err(|e| (path, e))? {
		let Ok((entry_path, name)) = normalize_path(entry.path())
			.map_err(|e| errors.push(NonIndexedLocationError::from((path, e)).into()))
		else {
			continue;
		};

		if let Ok(rule_results) = IndexerRule::apply_all(&rules, &entry_path)
			.await
			.map_err(|e| errors.push(e.into()))
		{
			// No OS Protected and No Hidden rules, must always be from this kind, should panic otherwise
			if rule_results[&RuleKind::RejectFilesByGlob]
				.iter()
				.any(|reject| !reject)
			{
				continue;
			}
		} else {
			continue;
		}

		let Ok(metadata) = entry
			.metadata()
			.await
			.map_err(|e| errors.push(NonIndexedLocationError::from((path, e)).into()))
		else {
			continue;
		};

		if metadata.is_dir() {
			directories.push((entry_path, name, metadata));
		} else {
			let path = Path::new(&entry_path);

			let Some(name) = path
				.file_stem()
				.and_then(|s| s.to_str().map(str::to_string))
			else {
				warn!("Failed to extract name from path: {}", &entry_path);
				continue;
			};

			let extension = path
				.extension()
				.and_then(|s| s.to_str().map(str::to_string))
				.unwrap_or_default();

			let kind = Extension::resolve_conflicting(&path, false)
				.await
				.map(Into::into)
				.unwrap_or(ObjectKind::Unknown);

			let should_generate_thumbnail = {
				#[cfg(feature = "ffmpeg")]
				{
					matches!(
						kind,
						ObjectKind::Image | ObjectKind::Video | ObjectKind::Document
					)
				}

				#[cfg(not(feature = "ffmpeg"))]
				{
					matches!(kind, ObjectKind::Image | ObjectKind::Document)
				}
			};

			let thumbnail_key = if should_generate_thumbnail {
				if let Ok(cas_id) = generate_cas_id(&path, metadata.len())
					.await
					.map_err(|e| errors.push(NonIndexedLocationError::from((path, e)).into()))
				{
					if kind == ObjectKind::Document {
						document_thumbnails_to_generate.push(GenerateThumbnailArgs::new(
							extension.clone(),
							cas_id.clone(),
							path.to_path_buf(),
						));
					} else {
						thumbnails_to_generate.push(GenerateThumbnailArgs::new(
							extension.clone(),
							cas_id.clone(),
							path.to_path_buf(),
						));
					}

					Some(get_ephemeral_thumb_key(&cas_id))
				} else {
					None
				}
			} else {
				None
			};

			entries.push(ExplorerItem::NonIndexedPath {
				has_local_thumbnail: thumbnail_key.is_some(),
				thumbnail_key,
				item: NonIndexedPathItem {
					hidden: path_is_hidden(Path::new(&entry_path), &metadata),
					path: entry_path,
					name,
					extension,
					kind: kind as i32,
					is_dir: false,
					date_created: metadata.created_or_now().into(),
					date_modified: metadata.modified_or_now().into(),
					size_in_bytes_bytes: metadata.len().to_be_bytes().to_vec(),
				},
			});
		}
	}

	thumbnails_to_generate.extend(document_thumbnails_to_generate);

	node.thumbnailer
		.new_ephemeral_thumbnails_batch(BatchToProcess::new(thumbnails_to_generate, false, false))
		.await;

	let mut locations = library
		.db
		.location()
		.find_many(vec![location::path::in_vec(
			directories
				.iter()
				.map(|(path, _, _)| path.clone())
				.collect(),
		)])
		.exec()
		.await?
		.into_iter()
		.flat_map(|location| {
			location
				.path
				.clone()
				.map(|location_path| (location_path, location))
		})
		.collect::<HashMap<_, _>>();

	for (directory, name, metadata) in directories {
		if let Some(location) = locations.remove(&directory) {
			entries.push(ExplorerItem::Location {
				has_local_thumbnail: false,
				thumbnail_key: None,
				item: location,
			});
		} else {
			entries.push(ExplorerItem::NonIndexedPath {
				has_local_thumbnail: false,
				thumbnail_key: None,
				item: NonIndexedPathItem {
					hidden: path_is_hidden(Path::new(&directory), &metadata),
					path: directory,
					name,
					extension: String::new(),
					kind: ObjectKind::Folder as i32,
					is_dir: true,
					date_created: metadata.created_or_now().into(),
					date_modified: metadata.modified_or_now().into(),
					size_in_bytes_bytes: metadata.len().to_be_bytes().to_vec(),
				},
			});
		}
	}

	Ok(NonIndexedFileSystemEntries { entries, errors })
}



File: ./src/location/manager/watcher/macos.rs
-------------------------------------------------
//! On MacOS, we use the FSEvents backend of notify-rs and Rename events are pretty complicated;
//! There are just (ModifyKind::Name(RenameMode::Any) events and nothing else.
//! This means that we have to link the old path with the new path to know which file was renamed.
//! But you can't forget that renames events aren't always the case that I file name was modified,
//! but its path was modified. So we have to check if the file was moved. When a file is moved
//! inside the same location, we received 2 events: one for the old path and one for the new path.
//! But when a file is moved to another location, we only receive the old path event... This
//! way we have to handle like a file deletion, and the same applies for when a file is moved to our
//! current location from anywhere else, we just receive the new path rename event, which means a
//! creation.

use crate::{
	invalidate_query,
	library::Library,
	location::{
		file_path_helper::{
			check_file_path_exists, get_inode, FilePathError, IsolatedFilePathData,
		},
		manager::LocationManagerError,
	},
	prisma::location,
	util::error::FileIOError,
	Node,
};

use std::{
	collections::HashMap,
	path::{Path, PathBuf},
	sync::Arc,
};

use async_trait::async_trait;
use notify::{
	event::{CreateKind, DataChange, MetadataKind, ModifyKind, RenameMode},
	Event, EventKind,
};
use tokio::{fs, io, time::Instant};
use tracing::{error, trace, warn};

use super::{
	utils::{
		create_dir, create_file, extract_inode_from_path, extract_location_path,
		recalculate_directories_size, remove, rename, update_file,
	},
	EventHandler, INode, InstantAndPath, HUNDRED_MILLIS, ONE_SECOND,
};

#[derive(Debug)]
pub(super) struct MacOsEventHandler<'lib> {
	location_id: location::id::Type,
	library: &'lib Arc<Library>,
	node: &'lib Arc<Node>,
	files_to_update: HashMap<PathBuf, Instant>,
	reincident_to_update_files: HashMap<PathBuf, Instant>,
	last_events_eviction_check: Instant,
	latest_created_dir: Option<PathBuf>,
	old_paths_map: HashMap<INode, InstantAndPath>,
	new_paths_map: HashMap<INode, InstantAndPath>,
	paths_map_buffer: Vec<(INode, InstantAndPath)>,
	to_recalculate_size: HashMap<PathBuf, Instant>,
	path_and_instant_buffer: Vec<(PathBuf, Instant)>,
}

#[async_trait]
impl<'lib> EventHandler<'lib> for MacOsEventHandler<'lib> {
	fn new(
		location_id: location::id::Type,
		library: &'lib Arc<Library>,
		node: &'lib Arc<Node>,
	) -> Self
	where
		Self: Sized,
	{
		Self {
			location_id,
			library,
			node,
			files_to_update: HashMap::new(),
			reincident_to_update_files: HashMap::new(),
			last_events_eviction_check: Instant::now(),
			latest_created_dir: None,
			old_paths_map: HashMap::new(),
			new_paths_map: HashMap::new(),
			paths_map_buffer: Vec::new(),
			to_recalculate_size: HashMap::new(),
			path_and_instant_buffer: Vec::new(),
		}
	}

	async fn handle_event(&mut self, event: Event) -> Result<(), LocationManagerError> {
		trace!("Received MacOS event: {:#?}", event);

		let Event {
			kind, mut paths, ..
		} = event;

		match kind {
			EventKind::Create(CreateKind::Folder) => {
				let path = &paths[0];
				if let Some(ref latest_created_dir) = self.latest_created_dir.take() {
					if path == latest_created_dir {
						// NOTE: This is a MacOS specific event that happens when a folder is created
						// trough Finder. It creates a folder but 2 events are triggered in
						// FSEvents. So we store and check the latest created folder to avoid
						// hiting a unique constraint in the database
						return Ok(());
					}
				}

				// Don't need to dispatch a recalculate directory event as `create_dir` dispatches
				// a `scan_location_sub_path` function, which recalculates the size already

				create_dir(
					self.location_id,
					path,
					&fs::metadata(path)
						.await
						.map_err(|e| FileIOError::from((path, e)))?,
					self.node,
					self.library,
				)
				.await?;
				self.latest_created_dir = Some(paths.remove(0));
			}
			EventKind::Create(CreateKind::File)
			| EventKind::Modify(ModifyKind::Data(DataChange::Content))
			| EventKind::Modify(ModifyKind::Metadata(
				MetadataKind::WriteTime | MetadataKind::Extended,
			)) => {
				// When we receive a create, modify data or metadata events of the abore kinds
				// we just mark the file to be updated in a near future
				// each consecutive event of these kinds that we receive for the same file
				// we just store the path again in the map below, with a new instant
				// that effectively resets the timer for the file to be updated
				let path = paths.remove(0);
				if self.files_to_update.contains_key(&path) {
					if let Some(old_instant) =
						self.files_to_update.insert(path.clone(), Instant::now())
					{
						self.reincident_to_update_files
							.entry(path)
							.or_insert(old_instant);
					}
				} else {
					self.files_to_update.insert(path, Instant::now());
				}
			}
			EventKind::Modify(ModifyKind::Name(RenameMode::Any)) => {
				self.handle_single_rename_event(paths.remove(0)).await?;
			}

			EventKind::Remove(_) => {
				let path = paths.remove(0);
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				remove(self.location_id, &path, self.library).await?;
			}
			other_event_kind => {
				trace!("Other MacOS event that we don't handle for now: {other_event_kind:#?}");
			}
		}

		Ok(())
	}

	async fn tick(&mut self) {
		if self.last_events_eviction_check.elapsed() > HUNDRED_MILLIS {
			if let Err(e) = self.handle_to_update_eviction().await {
				error!("Error while handling recently created or update files eviction: {e:#?}");
			}

			// Cleaning out recently renamed files that are older than 100 milliseconds
			if let Err(e) = self.handle_rename_create_eviction().await {
				error!("Failed to create file_path on MacOS : {e:#?}");
			}

			if let Err(e) = self.handle_rename_remove_eviction().await {
				error!("Failed to remove file_path: {e:#?}");
			}

			if !self.to_recalculate_size.is_empty() {
				if let Err(e) = recalculate_directories_size(
					&mut self.to_recalculate_size,
					&mut self.path_and_instant_buffer,
					self.location_id,
					self.library,
				)
				.await
				{
					error!("Failed to recalculate directories size: {e:#?}");
				}
			}

			self.last_events_eviction_check = Instant::now();
		}
	}
}

impl MacOsEventHandler<'_> {
	async fn handle_to_update_eviction(&mut self) -> Result<(), LocationManagerError> {
		self.path_and_instant_buffer.clear();
		let mut should_invalidate = false;

		for (path, created_at) in self.files_to_update.drain() {
			if created_at.elapsed() < HUNDRED_MILLIS * 5 {
				self.path_and_instant_buffer.push((path, created_at));
			} else {
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				self.reincident_to_update_files.remove(&path);
				update_file(self.location_id, &path, self.node, self.library).await?;
				should_invalidate = true;
			}
		}

		self.files_to_update
			.extend(self.path_and_instant_buffer.drain(..));

		self.path_and_instant_buffer.clear();

		// We have to check if we have any reincident files to update and update them after a bigger
		// timeout, this way we keep track of files being update frequently enough to bypass our
		// eviction check above
		for (path, created_at) in self.reincident_to_update_files.drain() {
			if created_at.elapsed() < ONE_SECOND * 10 {
				self.path_and_instant_buffer.push((path, created_at));
			} else {
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				self.files_to_update.remove(&path);
				update_file(self.location_id, &path, self.node, self.library).await?;
				should_invalidate = true;
			}
		}

		if should_invalidate {
			invalidate_query!(self.library, "search.paths");
		}

		self.reincident_to_update_files
			.extend(self.path_and_instant_buffer.drain(..));

		Ok(())
	}

	async fn handle_rename_create_eviction(&mut self) -> Result<(), LocationManagerError> {
		// Just to make sure that our buffer is clean
		self.paths_map_buffer.clear();
		let mut should_invalidate = false;

		for (inode, (instant, path)) in self.new_paths_map.drain() {
			if instant.elapsed() > HUNDRED_MILLIS {
				if !self.files_to_update.contains_key(&path) {
					let metadata = fs::metadata(&path)
						.await
						.map_err(|e| FileIOError::from((&path, e)))?;

					if metadata.is_dir() {
						// Don't need to dispatch a recalculate directory event as `create_dir` dispatches
						// a `scan_location_sub_path` function, which recalculates the size already
						create_dir(self.location_id, &path, &metadata, self.node, self.library)
							.await?;
					} else {
						if let Some(parent) = path.parent() {
							if parent != Path::new("") {
								self.to_recalculate_size
									.insert(parent.to_path_buf(), Instant::now());
							}
						}
						create_file(self.location_id, &path, &metadata, self.node, self.library)
							.await?;
					}

					trace!("Created file_path due timeout: {}", path.display());
					should_invalidate = true;
				}
			} else {
				self.paths_map_buffer.push((inode, (instant, path)));
			}
		}

		if should_invalidate {
			invalidate_query!(self.library, "search.paths");
		}

		self.new_paths_map.extend(self.paths_map_buffer.drain(..));

		Ok(())
	}

	async fn handle_rename_remove_eviction(&mut self) -> Result<(), LocationManagerError> {
		// Just to make sure that our buffer is clean
		self.paths_map_buffer.clear();
		let mut should_invalidate = false;

		for (inode, (instant, path)) in self.old_paths_map.drain() {
			if instant.elapsed() > HUNDRED_MILLIS {
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				remove(self.location_id, &path, self.library).await?;
				trace!("Removed file_path due timeout: {}", path.display());
				should_invalidate = true;
			} else {
				self.paths_map_buffer.push((inode, (instant, path)));
			}
		}

		if should_invalidate {
			invalidate_query!(self.library, "search.paths");
		}

		self.old_paths_map.extend(self.paths_map_buffer.drain(..));

		Ok(())
	}

	async fn handle_single_rename_event(
		&mut self,
		path: PathBuf, // this is used internally only once, so we can use just PathBuf
	) -> Result<(), LocationManagerError> {
		match fs::metadata(&path).await {
			Ok(meta) => {
				// File or directory exists, so this can be a "new path" to an actual rename/move or a creation
				trace!("Path exists: {}", path.display());

				let inode = get_inode(&meta);
				let location_path = extract_location_path(self.location_id, self.library).await?;

				if !check_file_path_exists::<FilePathError>(
					&IsolatedFilePathData::new(
						self.location_id,
						&location_path,
						&path,
						meta.is_dir(),
					)?,
					&self.library.db,
				)
				.await?
				{
					if let Some((_, old_path)) = self.old_paths_map.remove(&inode) {
						trace!(
							"Got a match new -> old: {} -> {}",
							path.display(),
							old_path.display()
						);

						// We found a new path for this old path, so we can rename it
						rename(self.location_id, &path, &old_path, meta, self.library).await?;
					} else {
						trace!("No match for new path yet: {}", path.display());
						self.new_paths_map.insert(inode, (Instant::now(), path));
					}
				} else {
					warn!(
						"Received rename event for a file that already exists in the database: {}",
						path.display()
					);
				}
			}
			Err(e) if e.kind() == io::ErrorKind::NotFound => {
				// File or directory does not exist in the filesystem, if it exists in the database,
				// then we try pairing it with the old path from our map

				trace!("Path doesn't exists: {}", path.display());

				let inode =
					match extract_inode_from_path(self.location_id, &path, self.library).await {
						Ok(inode) => inode,
						Err(LocationManagerError::FilePath(FilePathError::NotFound(_))) => {
							// temporary file, we can ignore it
							return Ok(());
						}
						Err(e) => return Err(e),
					};

				if let Some((_, new_path)) = self.new_paths_map.remove(&inode) {
					trace!(
						"Got a match old -> new: {} -> {}",
						path.display(),
						new_path.display()
					);

					// We found a new path for this old path, so we can rename it
					rename(
						self.location_id,
						&new_path,
						&path,
						fs::metadata(&new_path)
							.await
							.map_err(|e| FileIOError::from((&new_path, e)))?,
						self.library,
					)
					.await?;
				} else {
					trace!("No match for old path yet: {}", path.display());
					// We didn't find a new path for this old path, so we store ir for later
					self.old_paths_map.insert(inode, (Instant::now(), path));
				}
			}
			Err(e) => return Err(FileIOError::from((path, e)).into()),
		}

		Ok(())
	}
}



File: ./src/location/manager/watcher/mod.rs
-------------------------------------------------
use crate::{library::Library, prisma::location, util::db::maybe_missing, Node};

use std::{
	collections::HashSet,
	path::{Path, PathBuf},
	sync::Arc,
	time::Duration,
};

use async_trait::async_trait;
use notify::{Config, Event, RecommendedWatcher, RecursiveMode, Watcher};
use tokio::{
	runtime::Handle,
	select,
	sync::{mpsc, oneshot},
	task::{block_in_place, JoinHandle},
	time::{interval_at, Instant, MissedTickBehavior},
};
use tracing::{debug, error, warn};
use uuid::Uuid;

use super::LocationManagerError;

mod linux;
mod macos;
mod windows;

mod utils;

use utils::check_event;

#[cfg(target_os = "linux")]
type Handler<'lib> = linux::LinuxEventHandler<'lib>;

#[cfg(target_os = "macos")]
type Handler<'lib> = macos::MacOsEventHandler<'lib>;

#[cfg(target_os = "windows")]
type Handler<'lib> = windows::WindowsEventHandler<'lib>;

pub(super) type IgnorePath = (PathBuf, bool);

type INode = u64;
type InstantAndPath = (Instant, PathBuf);

const ONE_SECOND: Duration = Duration::from_secs(1);
const HUNDRED_MILLIS: Duration = Duration::from_millis(100);

#[async_trait]
trait EventHandler<'lib> {
	fn new(
		location_id: location::id::Type,
		library: &'lib Arc<Library>,
		node: &'lib Arc<Node>,
	) -> Self
	where
		Self: Sized;

	/// Handle a file system event.
	async fn handle_event(&mut self, event: Event) -> Result<(), LocationManagerError>;

	/// As Event Handlers have some inner state, from time to time we need to call this tick method
	/// so the event handler can update its state.
	async fn tick(&mut self);
}

#[derive(Debug)]
pub(super) struct LocationWatcher {
	id: i32,
	path: String,
	watcher: RecommendedWatcher,
	ignore_path_tx: mpsc::UnboundedSender<IgnorePath>,
	handle: Option<JoinHandle<()>>,
	stop_tx: Option<oneshot::Sender<()>>,
}

impl LocationWatcher {
	pub(super) async fn new(
		location: location::Data,
		library: Arc<Library>,
		node: Arc<Node>,
	) -> Result<Self, LocationManagerError> {
		let (events_tx, events_rx) = mpsc::unbounded_channel();
		let (ignore_path_tx, ignore_path_rx) = mpsc::unbounded_channel();
		let (stop_tx, stop_rx) = oneshot::channel();

		let watcher = RecommendedWatcher::new(
			move |result| {
				if !events_tx.is_closed() {
					if events_tx.send(result).is_err() {
						error!(
						"Unable to send watcher event to location manager for location: <id='{}'>",
						location.id
					);
					}
				} else {
					error!(
						"Tried to send location file system events to a closed channel: <id='{}'",
						location.id
					);
				}
			},
			Config::default(),
		)?;

		let handle = tokio::spawn(Self::handle_watch_events(
			location.id,
			Uuid::from_slice(&location.pub_id)?,
			node,
			library,
			events_rx,
			ignore_path_rx,
			stop_rx,
		));

		Ok(Self {
			id: location.id,
			path: maybe_missing(location.path, "location.path")?,
			watcher,
			ignore_path_tx,
			handle: Some(handle),
			stop_tx: Some(stop_tx),
		})
	}

	async fn handle_watch_events(
		location_id: location::id::Type,
		location_pub_id: Uuid,
		node: Arc<Node>,
		library: Arc<Library>,
		mut events_rx: mpsc::UnboundedReceiver<notify::Result<Event>>,
		mut ignore_path_rx: mpsc::UnboundedReceiver<IgnorePath>,
		mut stop_rx: oneshot::Receiver<()>,
	) {
		let mut event_handler = Handler::new(location_id, &library, &node);

		let mut paths_to_ignore = HashSet::new();

		let mut handler_interval = interval_at(Instant::now() + HUNDRED_MILLIS, HUNDRED_MILLIS);
		// In case of doubt check: https://docs.rs/tokio/latest/tokio/time/enum.MissedTickBehavior.html
		handler_interval.set_missed_tick_behavior(MissedTickBehavior::Delay);

		loop {
			select! {
				Some(event) = events_rx.recv() => {
					match event {
						Ok(event) => {
							if let Err(e) = Self::handle_single_event(
								location_id,
								location_pub_id,
								event,
								&mut event_handler,
								&node,
								&library,
								&paths_to_ignore,
							).await {
								error!("Failed to handle location file system event: \
									<id='{location_id}', error='{e:#?}'>",
								);
							}
						}
						Err(e) => {
							error!("watch error: {:#?}", e);
						}
					}
				}

				Some((path, ignore)) = ignore_path_rx.recv() => {
					if ignore {
						paths_to_ignore.insert(path);
					} else {
						paths_to_ignore.remove(&path);
					}
				}

				_ = handler_interval.tick() => {
					event_handler.tick().await;
				}

				_ = &mut stop_rx => {
					debug!("Stop Location Manager event handler for location: <id='{}'>", location_id);
					break
				}
			}
		}
	}

	async fn handle_single_event<'lib>(
		location_id: location::id::Type,
		location_pub_id: Uuid,
		event: Event,
		event_handler: &mut impl EventHandler<'lib>,
		node: &'lib Node,
		_library: &'lib Library,
		ignore_paths: &HashSet<PathBuf>,
	) -> Result<(), LocationManagerError> {
		if !check_event(&event, ignore_paths) {
			return Ok(());
		}

		// let Some(location) = find_location(library, location_id)
		// 	.include(location_with_indexer_rules::include())
		// 	.exec()
		// 	.await?
		// else {
		// 	warn!("Tried to handle event for unknown location: <id='{location_id}'>");
		//     return Ok(());
		// };

		if !node.locations.is_online(&location_pub_id).await {
			warn!("Tried to handle event for offline location: <id='{location_id}'>");
			return Ok(());
		}

		event_handler.handle_event(event).await
	}

	pub(super) fn ignore_path(
		&self,
		path: PathBuf,
		ignore: bool,
	) -> Result<(), LocationManagerError> {
		self.ignore_path_tx.send((path, ignore)).map_err(Into::into)
	}

	pub(super) fn check_path(&self, path: impl AsRef<Path>) -> bool {
		Path::new(&self.path) == path.as_ref()
	}

	pub(super) fn watch(&mut self) {
		let path = &self.path;

		if let Err(e) = self
			.watcher
			.watch(Path::new(path), RecursiveMode::Recursive)
		{
			error!("Unable to watch location: (path: {path}, error: {e:#?})");
		} else {
			debug!("Now watching location: (path: {path})");
		}
	}

	pub(super) fn unwatch(&mut self) {
		let path = &self.path;
		if let Err(e) = self.watcher.unwatch(Path::new(path)) {
			/**************************************** TODO: ****************************************
			 * According to an unit test, this error may occur when a subdirectory is removed	   *
			 * and we try to unwatch the parent directory then we have to check the implications   *
			 * of unwatch error for this case.   												   *
			 **************************************************************************************/
			error!("Unable to unwatch location: (path: {path}, error: {e:#?})",);
		} else {
			debug!("Stop watching location: (path: {path})");
		}
	}
}

impl Drop for LocationWatcher {
	fn drop(&mut self) {
		if let Some(stop_tx) = self.stop_tx.take() {
			if stop_tx.send(()).is_err() {
				error!(
					"Failed to send stop signal to location watcher: <id='{}'>",
					self.id
				);
			}

			// FIXME: change this Drop to async drop in the future
			if let Some(handle) = self.handle.take() {
				if let Err(e) = block_in_place(move || Handle::current().block_on(handle)) {
					error!("Failed to join watcher task: {e:#?}")
				}
			}
		}
	}
}

/***************************************************************************************************
* Some tests to validate our assumptions of events through different file systems				   *
****************************************************************************************************
*	Events dispatched on Linux:																	   *
*		Create File:																			   *
*			1) EventKind::Create(CreateKind::File)												   *
*			2) EventKind::Modify(ModifyKind::Metadata(MetadataKind::Any))						   *
*				or EventKind::Modify(ModifyKind::Data(DataChange::Any))							   *
*			3) EventKind::Access(AccessKind::Close(AccessMode::Write)))							   *
*		Create Directory:																		   *
*			1) EventKind::Create(CreateKind::Folder)											   *
*		Update File:																			   *
*			1) EventKind::Modify(ModifyKind::Data(DataChange::Any))								   *
*			2) EventKind::Access(AccessKind::Close(AccessMode::Write)))							   *
*		Update File (rename):																	   *
*			1) EventKind::Modify(ModifyKind::Name(RenameMode::From))							   *
*			2) EventKind::Modify(ModifyKind::Name(RenameMode::To))								   *
*			3) EventKind::Modify(ModifyKind::Name(RenameMode::Both))							   *
*		Update Directory (rename):																   *
*			1) EventKind::Modify(ModifyKind::Name(RenameMode::From))							   *
*			2) EventKind::Modify(ModifyKind::Name(RenameMode::To))								   *
*			3) EventKind::Modify(ModifyKind::Name(RenameMode::Both))							   *
*		Delete File:																			   *
*			1) EventKind::Remove(RemoveKind::File)												   *
*		Delete Directory:																		   *
*			1) EventKind::Remove(RemoveKind::Folder)											   *
*																								   *
*	Events dispatched on MacOS:																	   *
*		Create File:																			   *
*			1) EventKind::Create(CreateKind::File)												   *
*			2) EventKind::Modify(ModifyKind::Data(DataChange::Content))							   *
*		Create Directory:																		   *
*			1) EventKind::Create(CreateKind::Folder)											   *
*		Update File:																			   *
*			1) EventKind::Modify(ModifyKind::Data(DataChange::Content))							   *
*		Update File (rename):																	   *
*			1) EventKind::Modify(ModifyKind::Name(RenameMode::Any)) -- From						   *
*			2) EventKind::Modify(ModifyKind::Name(RenameMode::Any))	-- To						   *
*		Update Directory (rename):																   *
*			1) EventKind::Modify(ModifyKind::Name(RenameMode::Any)) -- From						   *
*			2) EventKind::Modify(ModifyKind::Name(RenameMode::Any))	-- To						   *
*		Delete File:																			   *
*			1) EventKind::Remove(RemoveKind::File)												   *
*		Delete Directory:																		   *
*			1) EventKind::Remove(RemoveKind::Folder)											   *
*																								   *
*	Events dispatched on Windows:																   *
*		Create File:																			   *
*			1) EventKind::Create(CreateKind::Any)												   *
*			2) EventKind::Modify(ModifyKind::Any)												   *
*		Create Directory:																		   *
*			1) EventKind::Create(CreateKind::Any)												   *
*		Update File:																			   *
*			1) EventKind::Modify(ModifyKind::Any)												   *
*		Update File (rename):																	   *
*			1) EventKind::Modify(ModifyKind::Name(RenameMode::From))							   *
*			2) EventKind::Modify(ModifyKind::Name(RenameMode::To))								   *
*		Update Directory (rename):																   *
*			1) EventKind::Modify(ModifyKind::Name(RenameMode::From))							   *
*			2) EventKind::Modify(ModifyKind::Name(RenameMode::To))								   *
*		Delete File:																			   *
*			1) EventKind::Remove(RemoveKind::Any)												   *
*		Delete Directory:																		   *
*			1) EventKind::Remove(RemoveKind::Any)												   *
*																								   *
*	Events dispatched on Android:																   *
*	TODO																						   *
*																								   *
*	Events dispatched on iOS:																	   *
*	TODO																						   *
*																								   *
***************************************************************************************************/
#[cfg(test)]
#[allow(clippy::unwrap_used, clippy::panic)]
mod tests {
	use std::{
		io::ErrorKind,
		path::{Path, PathBuf},
		time::Duration,
	};

	use notify::{
		event::{CreateKind, ModifyKind, RemoveKind, RenameMode},
		Config, Event, EventKind, RecommendedWatcher, Watcher,
	};
	use tempfile::{tempdir, TempDir};
	use tokio::{fs, io::AsyncWriteExt, sync::mpsc, time::sleep};
	use tracing::{debug, error};
	// use tracing_test::traced_test;

	#[cfg(target_os = "macos")]
	use notify::event::DataChange;

	#[cfg(target_os = "linux")]
	use notify::event::{AccessKind, AccessMode};

	async fn setup_watcher() -> (
		TempDir,
		RecommendedWatcher,
		mpsc::UnboundedReceiver<notify::Result<Event>>,
	) {
		let (events_tx, events_rx) = mpsc::unbounded_channel();

		let watcher = RecommendedWatcher::new(
			move |result| {
				events_tx
					.send(result)
					.expect("Unable to send watcher event");
			},
			Config::default(),
		)
		.expect("Failed to create watcher");

		(tempdir().unwrap(), watcher, events_rx)
	}

	async fn expect_event(
		mut events_rx: mpsc::UnboundedReceiver<notify::Result<Event>>,
		path: impl AsRef<Path>,
		expected_event: EventKind,
	) {
		let path = path.as_ref();
		debug!(
			"Expecting event: {expected_event:#?} at path: {}",
			path.display()
		);
		let mut tries = 0;
		loop {
			match events_rx.try_recv() {
				Ok(maybe_event) => {
					let event = maybe_event.expect("Failed to receive event");
					debug!("Received event: {event:#?}");
					// Using `ends_with` and removing root path here due to a weird edge case on CI tests at MacOS
					if event.paths[0].ends_with(path.iter().skip(1).collect::<PathBuf>())
						&& event.kind == expected_event
					{
						debug!("Received expected event: {expected_event:#?}");
						break;
					}
				}
				Err(e) => {
					debug!("No event yet: {e:#?}");
					tries += 1;
					sleep(Duration::from_millis(100)).await;
				}
			}

			if tries == 10 {
				panic!("No expected event received after 10 tries");
			}
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn create_file_event() {
		let (root_dir, mut watcher, events_rx) = setup_watcher().await;

		watcher
			.watch(root_dir.path(), notify::RecursiveMode::Recursive)
			.expect("Failed to watch root directory");
		debug!("Now watching {}", root_dir.path().display());

		let file_path = root_dir.path().join("test.txt");
		fs::write(&file_path, "test").await.unwrap();

		#[cfg(target_os = "windows")]
		expect_event(events_rx, &file_path, EventKind::Modify(ModifyKind::Any)).await;

		#[cfg(target_os = "macos")]
		expect_event(
			events_rx,
			&file_path,
			EventKind::Modify(ModifyKind::Data(DataChange::Content)),
		)
		.await;

		#[cfg(target_os = "linux")]
		expect_event(
			events_rx,
			&file_path,
			EventKind::Access(AccessKind::Close(AccessMode::Write)),
		)
		.await;

		debug!("Unwatching root directory: {}", root_dir.path().display());
		if let Err(e) = watcher.unwatch(root_dir.path()) {
			error!("Failed to unwatch root directory: {e:#?}");
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn create_dir_event() {
		let (root_dir, mut watcher, events_rx) = setup_watcher().await;

		watcher
			.watch(root_dir.path(), notify::RecursiveMode::Recursive)
			.expect("Failed to watch root directory");
		debug!("Now watching {}", root_dir.path().display());

		let dir_path = root_dir.path().join("inner");
		fs::create_dir(&dir_path)
			.await
			.expect("Failed to create directory");

		#[cfg(target_os = "windows")]
		expect_event(events_rx, &dir_path, EventKind::Create(CreateKind::Any)).await;

		#[cfg(target_os = "macos")]
		expect_event(events_rx, &dir_path, EventKind::Create(CreateKind::Folder)).await;

		#[cfg(target_os = "linux")]
		expect_event(events_rx, &dir_path, EventKind::Create(CreateKind::Folder)).await;

		debug!("Unwatching root directory: {}", root_dir.path().display());
		if let Err(e) = watcher.unwatch(root_dir.path()) {
			error!("Failed to unwatch root directory: {e:#?}");
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn update_file_event() {
		let (root_dir, mut watcher, events_rx) = setup_watcher().await;

		let file_path = root_dir.path().join("test.txt");
		fs::write(&file_path, "test").await.unwrap();

		watcher
			.watch(root_dir.path(), notify::RecursiveMode::Recursive)
			.expect("Failed to watch root directory");
		debug!("Now watching {}", root_dir.path().display());

		let mut file = fs::OpenOptions::new()
			.append(true)
			.open(&file_path)
			.await
			.expect("Failed to open file");

		// Writing then sync data before closing the file
		file.write_all(b"\nanother test")
			.await
			.expect("Failed to write to file");
		file.sync_all().await.expect("Failed to flush file");
		drop(file);

		#[cfg(target_os = "windows")]
		expect_event(events_rx, &file_path, EventKind::Modify(ModifyKind::Any)).await;

		#[cfg(target_os = "macos")]
		expect_event(
			events_rx,
			&file_path,
			EventKind::Modify(ModifyKind::Data(DataChange::Content)),
		)
		.await;

		#[cfg(target_os = "linux")]
		expect_event(
			events_rx,
			&file_path,
			EventKind::Access(AccessKind::Close(AccessMode::Write)),
		)
		.await;

		debug!("Unwatching root directory: {}", root_dir.path().display());
		if let Err(e) = watcher.unwatch(root_dir.path()) {
			error!("Failed to unwatch root directory: {e:#?}");
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn update_file_rename_event() {
		let (root_dir, mut watcher, events_rx) = setup_watcher().await;

		let file_path = root_dir.path().join("test.txt");
		fs::write(&file_path, "test").await.unwrap();

		watcher
			.watch(root_dir.path(), notify::RecursiveMode::Recursive)
			.expect("Failed to watch root directory");
		debug!("Now watching {}", root_dir.path().display());

		let new_file_name = root_dir.path().join("test2.txt");

		fs::rename(&file_path, &new_file_name)
			.await
			.expect("Failed to rename file");

		#[cfg(target_os = "windows")]
		expect_event(
			events_rx,
			&new_file_name,
			EventKind::Modify(ModifyKind::Name(RenameMode::To)),
		)
		.await;

		#[cfg(target_os = "macos")]
		expect_event(
			events_rx,
			&file_path,
			EventKind::Modify(ModifyKind::Name(RenameMode::Any)),
		)
		.await;

		#[cfg(target_os = "linux")]
		expect_event(
			events_rx,
			&file_path,
			EventKind::Modify(ModifyKind::Name(RenameMode::Both)),
		)
		.await;

		debug!("Unwatching root directory: {}", root_dir.path().display());
		if let Err(e) = watcher.unwatch(root_dir.path()) {
			error!("Failed to unwatch root directory: {e:#?}");
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn update_dir_event() {
		let (root_dir, mut watcher, events_rx) = setup_watcher().await;

		let dir_path = root_dir.path().join("inner");
		fs::create_dir(&dir_path)
			.await
			.expect("Failed to create directory");

		watcher
			.watch(root_dir.path(), notify::RecursiveMode::Recursive)
			.expect("Failed to watch root directory");
		debug!("Now watching {}", root_dir.path().display());

		let new_dir_name = root_dir.path().join("inner2");

		fs::rename(&dir_path, &new_dir_name)
			.await
			.expect("Failed to rename directory");

		#[cfg(target_os = "windows")]
		expect_event(
			events_rx,
			&new_dir_name,
			EventKind::Modify(ModifyKind::Name(RenameMode::To)),
		)
		.await;

		#[cfg(target_os = "macos")]
		expect_event(
			events_rx,
			&dir_path,
			EventKind::Modify(ModifyKind::Name(RenameMode::Any)),
		)
		.await;

		#[cfg(target_os = "linux")]
		expect_event(
			events_rx,
			&dir_path,
			EventKind::Modify(ModifyKind::Name(RenameMode::Both)),
		)
		.await;

		debug!("Unwatching root directory: {}", root_dir.path().display());
		if let Err(e) = watcher.unwatch(root_dir.path()) {
			error!("Failed to unwatch root directory: {e:#?}");
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn delete_file_event() {
		let (root_dir, mut watcher, events_rx) = setup_watcher().await;

		let file_path = root_dir.path().join("test.txt");
		fs::write(&file_path, "test").await.unwrap();

		watcher
			.watch(root_dir.path(), notify::RecursiveMode::Recursive)
			.expect("Failed to watch root directory");
		debug!("Now watching {}", root_dir.path().display());

		fs::remove_file(&file_path)
			.await
			.expect("Failed to remove file");

		#[cfg(target_os = "windows")]
		expect_event(events_rx, &file_path, EventKind::Remove(RemoveKind::Any)).await;

		#[cfg(target_os = "macos")]
		expect_event(events_rx, &file_path, EventKind::Remove(RemoveKind::File)).await;

		#[cfg(target_os = "linux")]
		expect_event(events_rx, &file_path, EventKind::Remove(RemoveKind::File)).await;

		debug!("Unwatching root directory: {}", root_dir.path().display());
		if let Err(e) = watcher.unwatch(root_dir.path()) {
			error!("Failed to unwatch root directory: {e:#?}");
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn delete_dir_event() {
		let (root_dir, mut watcher, events_rx) = setup_watcher().await;

		let dir_path = root_dir.path().join("inner");
		fs::create_dir(&dir_path)
			.await
			.expect("Failed to create directory");

		if let Err(e) = fs::metadata(&dir_path).await {
			if e.kind() == ErrorKind::NotFound {
				panic!("Directory not found");
			} else {
				panic!("{e}");
			}
		}

		watcher
			.watch(root_dir.path(), notify::RecursiveMode::Recursive)
			.expect("Failed to watch root directory");
		debug!("Now watching {}", root_dir.path().display());

		debug!("First unwatching the inner directory before removing it");
		if let Err(e) = watcher.unwatch(&dir_path) {
			error!("Failed to unwatch inner directory: {e:#?}");
		}

		fs::remove_dir(&dir_path)
			.await
			.expect("Failed to remove directory");

		#[cfg(target_os = "windows")]
		expect_event(events_rx, &dir_path, EventKind::Remove(RemoveKind::Any)).await;

		#[cfg(target_os = "macos")]
		expect_event(events_rx, &dir_path, EventKind::Remove(RemoveKind::Folder)).await;

		#[cfg(target_os = "linux")]
		expect_event(events_rx, &dir_path, EventKind::Remove(RemoveKind::Folder)).await;

		debug!("Unwatching root directory: {}", root_dir.path().display());
		if let Err(e) = watcher.unwatch(root_dir.path()) {
			error!("Failed to unwatch root directory: {e:#?}");
		}
	}
}



File: ./src/location/manager/watcher/windows.rs
-------------------------------------------------
//! Windows file system event handler implementation has some caveats die to how
//! file system events are emitted on Windows.
//!
//! For example: When a file is moved to another
//! directory, we receive a remove event and then a create event, so to avoid having to actually
//! remove and create the `file_path` in the database, we have to wait some time after receiving
//! a remove event to see if a create event is emitted. If it is, we just update the `file_path`
//! in the database. If not, we remove the file from the database.

use crate::{
	invalidate_query,
	library::Library,
	location::{
		file_path_helper::{get_inode_from_path, FilePathError},
		manager::LocationManagerError,
	},
	prisma::location,
	util::error::FileIOError,
	Node,
};

use std::{
	collections::{BTreeMap, HashMap},
	path::{Path, PathBuf},
	sync::Arc,
};

use async_trait::async_trait;
use notify::{
	event::{CreateKind, ModifyKind, RenameMode},
	Event, EventKind,
};
use tokio::{fs, time::Instant};
use tracing::{error, trace};

use super::{
	utils::{
		create_dir, extract_inode_from_path, recalculate_directories_size, remove, rename,
		update_file,
	},
	EventHandler, INode, InstantAndPath, HUNDRED_MILLIS, ONE_SECOND,
};

/// Windows file system event handler
#[derive(Debug)]
pub(super) struct WindowsEventHandler<'lib> {
	location_id: location::id::Type,
	library: &'lib Arc<Library>,
	node: &'lib Arc<Node>,
	last_events_eviction_check: Instant,
	rename_from_map: BTreeMap<INode, InstantAndPath>,
	rename_to_map: BTreeMap<INode, InstantAndPath>,
	files_to_remove: HashMap<INode, InstantAndPath>,
	files_to_remove_buffer: Vec<(INode, InstantAndPath)>,
	files_to_update: HashMap<PathBuf, Instant>,
	reincident_to_update_files: HashMap<PathBuf, Instant>,
	to_recalculate_size: HashMap<PathBuf, Instant>,
	path_and_instant_buffer: Vec<(PathBuf, Instant)>,
}

#[async_trait]
impl<'lib> EventHandler<'lib> for WindowsEventHandler<'lib> {
	fn new(
		location_id: location::id::Type,
		library: &'lib Arc<Library>,
		node: &'lib Arc<Node>,
	) -> Self
	where
		Self: Sized,
	{
		Self {
			location_id,
			library,
			node,
			last_events_eviction_check: Instant::now(),
			rename_from_map: BTreeMap::new(),
			rename_to_map: BTreeMap::new(),
			files_to_remove: HashMap::new(),
			files_to_remove_buffer: Vec::new(),
			files_to_update: HashMap::new(),
			reincident_to_update_files: HashMap::new(),
			to_recalculate_size: HashMap::new(),
			path_and_instant_buffer: Vec::new(),
		}
	}

	async fn handle_event(&mut self, event: Event) -> Result<(), LocationManagerError> {
		trace!("Received Windows event: {:#?}", event);
		let Event {
			kind, mut paths, ..
		} = event;

		match kind {
			EventKind::Create(CreateKind::Any) => {
				let inode = match get_inode_from_path(&paths[0]).await {
					Ok(inode) => inode,
					Err(FilePathError::FileIO(FileIOError { source, .. }))
						if source.raw_os_error() == Some(32) =>
					{
						// This is still being manipulated by another process, so we can just ignore it for now
						// as we will probably receive update events later
						self.files_to_update.insert(paths.remove(0), Instant::now());

						return Ok(());
					}
					Err(e) => {
						return Err(e.into());
					}
				};

				if let Some((_, old_path)) = self.files_to_remove.remove(&inode) {
					// if previously we added a file to be removed with the same inode
					// of this "newly created" created file, it means that the file was just moved to another location
					// so we can treat if just as a file rename, like in other OSes

					trace!(
						"Got a rename instead of remove/create: {} -> {}",
						old_path.display(),
						paths[0].display(),
					);

					// We found a new path for this old path, so we can rename it instead of removing and creating it
					rename(
						self.location_id,
						&paths[0],
						&old_path,
						fs::metadata(&paths[0])
							.await
							.map_err(|e| FileIOError::from((&paths[0], e)))?,
						self.library,
					)
					.await?;
				} else {
					let path = paths.remove(0);
					let metadata = fs::metadata(&path)
						.await
						.map_err(|e| FileIOError::from((&path, e)))?;

					if metadata.is_dir() {
						// Don't need to dispatch a recalculate directory event as `create_dir` dispatches
						// a `scan_location_sub_path` function, which recalculates the size already
						create_dir(self.location_id, path, &metadata, self.node, self.library)
							.await?;
					} else if self.files_to_update.contains_key(&path) {
						if let Some(old_instant) =
							self.files_to_update.insert(path.clone(), Instant::now())
						{
							self.reincident_to_update_files
								.entry(path)
								.or_insert(old_instant);
						}
					} else {
						self.files_to_update.insert(path, Instant::now());
					}
				}
			}
			EventKind::Modify(ModifyKind::Any) => {
				let path = paths.remove(0);
				if self.files_to_update.contains_key(&path) {
					if let Some(old_instant) =
						self.files_to_update.insert(path.clone(), Instant::now())
					{
						self.reincident_to_update_files
							.entry(path)
							.or_insert(old_instant);
					}
				} else {
					self.files_to_update.insert(path, Instant::now());
				}
			}
			EventKind::Modify(ModifyKind::Name(RenameMode::From)) => {
				let path = paths.remove(0);

				let inode = extract_inode_from_path(self.location_id, &path, self.library).await?;

				if let Some((_, new_path)) = self.rename_to_map.remove(&inode) {
					// We found a new path for this old path, so we can rename it
					rename(
						self.location_id,
						&new_path,
						&path,
						fs::metadata(&new_path)
							.await
							.map_err(|e| FileIOError::from((&new_path, e)))?,
						self.library,
					)
					.await?;
				} else {
					self.rename_from_map.insert(inode, (Instant::now(), path));
				}
			}
			EventKind::Modify(ModifyKind::Name(RenameMode::To)) => {
				let path = paths.remove(0);

				let inode = get_inode_from_path(&path).await?;

				if let Some((_, old_path)) = self.rename_from_map.remove(&inode) {
					// We found a old path for this new path, so we can rename it
					rename(
						self.location_id,
						&path,
						&old_path,
						fs::metadata(&path)
							.await
							.map_err(|e| FileIOError::from((&path, e)))?,
						self.library,
					)
					.await?;
				} else {
					self.rename_to_map.insert(inode, (Instant::now(), path));
				}
			}
			EventKind::Remove(_) => {
				let path = paths.remove(0);
				self.files_to_remove.insert(
					extract_inode_from_path(self.location_id, &path, self.library).await?,
					(Instant::now(), path),
				);
			}

			other_event_kind => {
				trace!("Other Windows event that we don't handle for now: {other_event_kind:#?}");
			}
		}

		Ok(())
	}

	async fn tick(&mut self) {
		if self.last_events_eviction_check.elapsed() > HUNDRED_MILLIS {
			if let Err(e) = self.handle_to_update_eviction().await {
				error!("Error while handling recently created or update files eviction: {e:#?}");
			}

			self.rename_from_map.retain(|_, (created_at, path)| {
				let to_retain = created_at.elapsed() < HUNDRED_MILLIS;
				if !to_retain {
					trace!("Removing from rename from map: {:#?}", path.display())
				}
				to_retain
			});
			self.rename_to_map.retain(|_, (created_at, path)| {
				let to_retain = created_at.elapsed() < HUNDRED_MILLIS;
				if !to_retain {
					trace!("Removing from rename to map: {:#?}", path.display())
				}
				to_retain
			});

			if let Err(e) = self.handle_removes_eviction().await {
				error!("Failed to remove file_path: {e:#?}");
			}

			if !self.to_recalculate_size.is_empty() {
				if let Err(e) = recalculate_directories_size(
					&mut self.to_recalculate_size,
					&mut self.path_and_instant_buffer,
					self.location_id,
					self.library,
				)
				.await
				{
					error!("Failed to recalculate directories size: {e:#?}");
				}
			}

			self.last_events_eviction_check = Instant::now();
		}
	}
}

impl WindowsEventHandler<'_> {
	async fn handle_to_update_eviction(&mut self) -> Result<(), LocationManagerError> {
		self.path_and_instant_buffer.clear();
		let mut should_invalidate = false;

		for (path, created_at) in self.files_to_update.drain() {
			if created_at.elapsed() < HUNDRED_MILLIS * 5 {
				self.path_and_instant_buffer.push((path, created_at));
			} else {
				self.reincident_to_update_files.remove(&path);
				handle_update(
					self.location_id,
					&path,
					self.node,
					&mut self.to_recalculate_size,
					self.library,
				)
				.await?;
				should_invalidate = true;
			}
		}

		self.files_to_update
			.extend(self.path_and_instant_buffer.drain(..));

		self.path_and_instant_buffer.clear();

		// We have to check if we have any reincident files to update and update them after a bigger
		// timeout, this way we keep track of files being update frequently enough to bypass our
		// eviction check above
		for (path, created_at) in self.reincident_to_update_files.drain() {
			if created_at.elapsed() < ONE_SECOND * 10 {
				self.path_and_instant_buffer.push((path, created_at));
			} else {
				self.files_to_update.remove(&path);
				handle_update(
					self.location_id,
					&path,
					self.node,
					&mut self.to_recalculate_size,
					self.library,
				)
				.await?;
				should_invalidate = true;
			}
		}

		if should_invalidate {
			invalidate_query!(self.library, "search.paths");
		}

		self.reincident_to_update_files
			.extend(self.path_and_instant_buffer.drain(..));

		Ok(())
	}

	async fn handle_removes_eviction(&mut self) -> Result<(), LocationManagerError> {
		self.files_to_remove_buffer.clear();
		let mut should_invalidate = false;

		for (inode, (instant, path)) in self.files_to_remove.drain() {
			if instant.elapsed() > HUNDRED_MILLIS {
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				remove(self.location_id, &path, self.library).await?;
				should_invalidate = true;
				trace!("Removed file_path due timeout: {}", path.display());
			} else {
				self.files_to_remove_buffer.push((inode, (instant, path)));
			}
		}
		if should_invalidate {
			invalidate_query!(self.library, "search.paths");
		}

		for (key, value) in self.files_to_remove_buffer.drain(..) {
			self.files_to_remove.insert(key, value);
		}

		Ok(())
	}
}

async fn handle_update<'lib>(
	location_id: location::id::Type,
	path: &PathBuf,
	node: &'lib Arc<Node>,
	to_recalculate_size: &mut HashMap<PathBuf, Instant>,
	library: &'lib Arc<Library>,
) -> Result<(), LocationManagerError> {
	let metadata = fs::metadata(&path)
		.await
		.map_err(|e| FileIOError::from((&path, e)))?;
	if metadata.is_file() {
		if let Some(parent) = path.parent() {
			if parent != Path::new("") {
				to_recalculate_size.insert(parent.to_path_buf(), Instant::now());
			}
		}
		update_file(location_id, path, node, library).await?;
	}

	Ok(())
}



File: ./src/location/manager/watcher/linux.rs
-------------------------------------------------
//! Linux has the best behaving file system events, with just some small caveats:
//! When we move files or directories, we receive 3 events: Rename From, Rename To and Rename Both.
//! But when we move a file or directory to the outside from the watched location, we just receive
//! the Rename From event, so we have to keep track of all rename events to match them against each
//! other. If we have dangling Rename From events, we have to remove them after some time.
//! Aside from that, when a directory is moved to our watched location from the outside, we receive
//! a Create Dir event, this one is actually ok at least.

use crate::{
	invalidate_query, library::Library, location::manager::LocationManagerError, prisma::location,
	util::error::FileIOError, Node,
};

use std::{
	collections::{BTreeMap, HashMap},
	path::{Path, PathBuf},
	sync::Arc,
};

use async_trait::async_trait;
use notify::{
	event::{CreateKind, DataChange, ModifyKind, RenameMode},
	Event, EventKind,
};
use tokio::{fs, time::Instant};
use tracing::{error, trace};

use super::{
	utils::{create_dir, recalculate_directories_size, remove, rename, update_file},
	EventHandler, HUNDRED_MILLIS, ONE_SECOND,
};

#[derive(Debug)]
pub(super) struct LinuxEventHandler<'lib> {
	location_id: location::id::Type,
	library: &'lib Arc<Library>,
	node: &'lib Arc<Node>,
	last_events_eviction_check: Instant,
	rename_from: HashMap<PathBuf, Instant>,
	recently_renamed_from: BTreeMap<PathBuf, Instant>,
	files_to_update: HashMap<PathBuf, Instant>,
	reincident_to_update_files: HashMap<PathBuf, Instant>,
	to_recalculate_size: HashMap<PathBuf, Instant>,
	path_and_instant_buffer: Vec<(PathBuf, Instant)>,
}

#[async_trait]
impl<'lib> EventHandler<'lib> for LinuxEventHandler<'lib> {
	fn new(
		location_id: location::id::Type,
		library: &'lib Arc<Library>,
		node: &'lib Arc<Node>,
	) -> Self {
		Self {
			location_id,
			library,
			node,
			last_events_eviction_check: Instant::now(),
			rename_from: HashMap::new(),
			recently_renamed_from: BTreeMap::new(),
			files_to_update: HashMap::new(),
			reincident_to_update_files: HashMap::new(),
			to_recalculate_size: HashMap::new(),
			path_and_instant_buffer: Vec::new(),
		}
	}

	async fn handle_event(&mut self, event: Event) -> Result<(), LocationManagerError> {
		trace!("Received Linux event: {:#?}", event);

		let Event {
			kind, mut paths, ..
		} = event;

		match kind {
			EventKind::Create(CreateKind::File)
			| EventKind::Modify(ModifyKind::Data(DataChange::Any)) => {
				// When we receive a create, modify data or metadata events of the abore kinds
				// we just mark the file to be updated in a near future
				// each consecutive event of these kinds that we receive for the same file
				// we just store the path again in the map below, with a new instant
				// that effectively resets the timer for the file to be updated
				let path = paths.remove(0);
				if self.files_to_update.contains_key(&path) {
					if let Some(old_instant) =
						self.files_to_update.insert(path.clone(), Instant::now())
					{
						self.reincident_to_update_files
							.entry(path)
							.or_insert(old_instant);
					}
				} else {
					self.files_to_update.insert(path, Instant::now());
				}
			}

			EventKind::Create(CreateKind::Folder) => {
				let path = &paths[0];

				// Don't need to dispatch a recalculate directory event as `create_dir` dispatches
				// a `scan_location_sub_path` function, which recalculates the size already

				create_dir(
					self.location_id,
					path,
					&fs::metadata(path)
						.await
						.map_err(|e| FileIOError::from((path, e)))?,
					self.node,
					self.library,
				)
				.await?;
			}
			EventKind::Modify(ModifyKind::Name(RenameMode::From)) => {
				// Just in case we can't garantee that we receive the Rename From event before the
				// Rename Both event. Just a safeguard
				if self.recently_renamed_from.remove(&paths[0]).is_none() {
					self.rename_from.insert(paths.remove(0), Instant::now());
				}
			}

			EventKind::Modify(ModifyKind::Name(RenameMode::Both)) => {
				let from_path = &paths[0];
				let to_path = &paths[1];

				self.rename_from.remove(from_path);
				rename(
					self.location_id,
					to_path,
					from_path,
					fs::metadata(to_path)
						.await
						.map_err(|e| FileIOError::from((to_path, e)))?,
					self.library,
				)
				.await?;
				self.recently_renamed_from
					.insert(paths.swap_remove(0), Instant::now());
			}
			EventKind::Remove(_) => {
				let path = paths.remove(0);
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}

				remove(self.location_id, &path, self.library).await?;
			}
			other_event_kind => {
				trace!("Other Linux event that we don't handle for now: {other_event_kind:#?}");
			}
		}

		Ok(())
	}

	async fn tick(&mut self) {
		if self.last_events_eviction_check.elapsed() > HUNDRED_MILLIS {
			if let Err(e) = self.handle_to_update_eviction().await {
				error!("Error while handling recently created or update files eviction: {e:#?}");
			}

			if let Err(e) = self.handle_rename_from_eviction().await {
				error!("Failed to remove file_path: {e:#?}");
			}

			self.recently_renamed_from
				.retain(|_, instant| instant.elapsed() < HUNDRED_MILLIS);

			if !self.to_recalculate_size.is_empty() {
				if let Err(e) = recalculate_directories_size(
					&mut self.to_recalculate_size,
					&mut self.path_and_instant_buffer,
					self.location_id,
					self.library,
				)
				.await
				{
					error!("Failed to recalculate directories size: {e:#?}");
				}
			}

			self.last_events_eviction_check = Instant::now();
		}
	}
}

impl LinuxEventHandler<'_> {
	async fn handle_to_update_eviction(&mut self) -> Result<(), LocationManagerError> {
		self.path_and_instant_buffer.clear();
		let mut should_invalidate = false;

		for (path, created_at) in self.files_to_update.drain() {
			if created_at.elapsed() < HUNDRED_MILLIS * 5 {
				self.path_and_instant_buffer.push((path, created_at));
			} else {
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				self.reincident_to_update_files.remove(&path);
				update_file(self.location_id, &path, self.node, self.library).await?;
				should_invalidate = true;
			}
		}

		self.files_to_update
			.extend(self.path_and_instant_buffer.drain(..));

		self.path_and_instant_buffer.clear();

		// We have to check if we have any reincident files to update and update them after a bigger
		// timeout, this way we keep track of files being update frequently enough to bypass our
		// eviction check above
		for (path, created_at) in self.reincident_to_update_files.drain() {
			if created_at.elapsed() < ONE_SECOND * 10 {
				self.path_and_instant_buffer.push((path, created_at));
			} else {
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				self.files_to_update.remove(&path);
				update_file(self.location_id, &path, self.node, self.library).await?;
				should_invalidate = true;
			}
		}

		if should_invalidate {
			invalidate_query!(self.library, "search.paths");
		}

		self.reincident_to_update_files
			.extend(self.path_and_instant_buffer.drain(..));

		Ok(())
	}

	async fn handle_rename_from_eviction(&mut self) -> Result<(), LocationManagerError> {
		self.path_and_instant_buffer.clear();
		let mut should_invalidate = false;

		for (path, instant) in self.rename_from.drain() {
			if instant.elapsed() > HUNDRED_MILLIS {
				if let Some(parent) = path.parent() {
					if parent != Path::new("") {
						self.to_recalculate_size
							.insert(parent.to_path_buf(), Instant::now());
					}
				}
				remove(self.location_id, &path, self.library).await?;
				should_invalidate = true;
				trace!("Removed file_path due timeout: {}", path.display());
			} else {
				self.path_and_instant_buffer.push((path, instant));
			}
		}

		if should_invalidate {
			invalidate_query!(self.library, "search.paths");
		}

		for (path, instant) in self.path_and_instant_buffer.drain(..) {
			self.rename_from.insert(path, instant);
		}

		Ok(())
	}
}



File: ./src/location/manager/watcher/utils.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	library::Library,
	location::{
		delete_directory,
		file_path_helper::{
			check_file_path_exists, create_file_path, file_path_with_object,
			filter_existing_file_path_params,
			isolated_file_path_data::extract_normalized_materialized_path_str,
			loose_find_existing_file_path_params, path_is_hidden, FilePathError, FilePathMetadata,
			IsolatedFilePathData, MetadataExt,
		},
		find_location,
		indexer::reverse_update_directories_sizes,
		location_with_indexer_rules,
		manager::LocationManagerError,
		scan_location_sub_path, update_location_size,
	},
	object::{
		file_identifier::FileMetadata,
		media::{
			media_data_extractor::{can_extract_media_data_for_image, extract_media_data},
			media_data_image_to_query_params,
			thumbnail::get_indexed_thumbnail_path,
		},
		validation::hash::file_checksum,
	},
	prisma::{file_path, location, object},
	util::{
		db::{inode_from_db, inode_to_db, maybe_missing},
		error::FileIOError,
	},
	Node,
};

#[cfg(target_family = "unix")]
use crate::location::file_path_helper::get_inode;

#[cfg(target_family = "windows")]
use crate::location::file_path_helper::get_inode_from_path;

use std::{
	collections::{HashMap, HashSet},
	ffi::OsStr,
	fs::Metadata,
	path::{Path, PathBuf},
	str::FromStr,
	sync::Arc,
};

use sd_file_ext::{extensions::ImageExtension, kind::ObjectKind};

use chrono::{DateTime, FixedOffset, Local, Utc};
use notify::Event;
use prisma_client_rust::{raw, PrismaValue};
use sd_prisma::{prisma::media_data, prisma_sync};
use sd_sync::OperationFactory;
use sd_utils::uuid_to_bytes;
use serde_json::json;
use tokio::{
	fs,
	io::{self, ErrorKind},
	spawn,
	time::Instant,
};
use tracing::{debug, error, trace, warn};
use uuid::Uuid;

use super::{INode, HUNDRED_MILLIS};

pub(super) fn check_event(event: &Event, ignore_paths: &HashSet<PathBuf>) -> bool {
	// if path includes .DS_Store, .spacedrive file creation or is in the `ignore_paths` set, we ignore
	!event.paths.iter().any(|p| {
		p.file_name()
			.and_then(OsStr::to_str)
			.map_or(false, |name| name == ".DS_Store" || name == ".spacedrive")
			|| ignore_paths.contains(p)
	})
}

pub(super) async fn create_dir(
	location_id: location::id::Type,
	path: impl AsRef<Path>,
	metadata: &Metadata,
	node: &Arc<Node>,
	library: &Arc<Library>,
) -> Result<(), LocationManagerError> {
	let location = find_location(library, location_id)
		.include(location_with_indexer_rules::include())
		.exec()
		.await?
		.ok_or(LocationManagerError::MissingLocation(location_id))?;

	let path = path.as_ref();

	let location_path = maybe_missing(&location.path, "location.path")?;

	trace!(
		"Location: <root_path ='{}'> creating directory: {}",
		location_path,
		path.display()
	);

	let iso_file_path = IsolatedFilePathData::new(location.id, location_path, path, true)?;

	let parent_iso_file_path = iso_file_path.parent();
	if !parent_iso_file_path.is_root()
		&& !check_file_path_exists::<FilePathError>(&parent_iso_file_path, &library.db).await?
	{
		warn!(
			"Watcher found a directory without parent: {}",
			&iso_file_path
		);
		return Ok(());
	};

	let children_materialized_path = iso_file_path
		.materialized_path_for_children()
		.expect("We're in the create dir function lol");

	debug!("Creating path: {}", iso_file_path);

	create_file_path(
		library,
		iso_file_path,
		None,
		FilePathMetadata::from_path(&path, metadata).await?,
	)
	.await?;

	// scan the new directory
	scan_location_sub_path(node, library, location, &children_materialized_path).await?;

	invalidate_query!(library, "search.paths");
	invalidate_query!(library, "search.objects");

	Ok(())
}

pub(super) async fn create_file(
	location_id: location::id::Type,
	path: impl AsRef<Path>,
	metadata: &Metadata,
	node: &Arc<Node>,
	library: &Arc<Library>,
) -> Result<(), LocationManagerError> {
	inner_create_file(
		location_id,
		extract_location_path(location_id, library).await?,
		path,
		metadata,
		node,
		library,
	)
	.await
}

async fn inner_create_file(
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	path: impl AsRef<Path>,
	metadata: &Metadata,
	node: &Arc<Node>,
	library @ Library {
		id: library_id,
		db,
		sync,
		..
	}: &Library,
) -> Result<(), LocationManagerError> {
	let path = path.as_ref();
	let location_path = location_path.as_ref();

	trace!(
		"Location: <root_path ='{}'> creating file: {}",
		location_path.display(),
		path.display()
	);

	let iso_file_path = IsolatedFilePathData::new(location_id, location_path, path, false)?;
	let extension = iso_file_path.extension.to_string();

	let metadata = FilePathMetadata::from_path(&path, metadata).await?;

	// First we check if already exist a file with this same inode number
	// if it does, we just update it
	if let Some(file_path) = db
		.file_path()
		.find_unique(file_path::location_id_inode(
			location_id,
			inode_to_db(metadata.inode),
		))
		.include(file_path_with_object::include())
		.exec()
		.await?
	{
		trace!("File already exists with that inode: {}", iso_file_path);
		return inner_update_file(location_path, &file_path, path, node, library, None).await;

	// If we can't find an existing file with the same inode, we check if there is a file with the same path
	} else if let Some(file_path) = db
		.file_path()
		.find_unique(file_path::location_id_materialized_path_name_extension(
			location_id,
			iso_file_path.materialized_path.to_string(),
			iso_file_path.name.to_string(),
			iso_file_path.extension.to_string(),
		))
		.include(file_path_with_object::include())
		.exec()
		.await?
	{
		trace!(
			"File already exists with that iso_file_path: {}",
			iso_file_path
		);
		return inner_update_file(
			location_path,
			&file_path,
			path,
			node,
			library,
			Some(metadata.inode),
		)
		.await;
	}

	let parent_iso_file_path = iso_file_path.parent();
	if !parent_iso_file_path.is_root()
		&& !check_file_path_exists::<FilePathError>(&parent_iso_file_path, db).await?
	{
		warn!("Watcher found a file without parent: {}", &iso_file_path);
		return Ok(());
	};

	// generate provisional object
	let FileMetadata {
		cas_id,
		kind,
		fs_metadata,
	} = FileMetadata::new(&location_path, &iso_file_path).await?;

	debug!("Creating path: {}", iso_file_path);

	let created_file = create_file_path(library, iso_file_path, cas_id.clone(), metadata).await?;

	object::select!(object_ids { id pub_id });

	let existing_object = db
		.object()
		.find_first(vec![object::file_paths::some(vec![
			file_path::cas_id::equals(cas_id.clone()),
			file_path::pub_id::not(created_file.pub_id.clone()),
		])])
		.select(object_ids::select())
		.exec()
		.await?;

	let object_ids::Data {
		id: object_id,
		pub_id: object_pub_id,
	} = if let Some(object) = existing_object {
		object
	} else {
		let pub_id = uuid_to_bytes(Uuid::new_v4());
		let date_created: DateTime<FixedOffset> =
			DateTime::<Local>::from(fs_metadata.created_or_now()).into();
		let int_kind = kind as i32;
		sync.write_ops(
			db,
			(
				sync.shared_create(
					prisma_sync::object::SyncId {
						pub_id: pub_id.clone(),
					},
					[
						(object::date_created::NAME, json!(date_created)),
						(object::kind::NAME, json!(int_kind)),
					],
				),
				db.object()
					.create(
						pub_id.to_vec(),
						vec![
							object::date_created::set(Some(date_created)),
							object::kind::set(Some(int_kind)),
						],
					)
					.select(object_ids::select()),
			),
		)
		.await?
	};

	sync.write_op(
		db,
		sync.shared_update(
			prisma_sync::location::SyncId {
				pub_id: created_file.pub_id.clone(),
			},
			file_path::object::NAME,
			json!(prisma_sync::object::SyncId {
				pub_id: object_pub_id.clone()
			}),
		),
		db.file_path().update(
			file_path::pub_id::equals(created_file.pub_id.clone()),
			vec![file_path::object::connect(object::pub_id::equals(
				object_pub_id,
			))],
		),
	)
	.await?;

	if !extension.is_empty() && matches!(kind, ObjectKind::Image | ObjectKind::Video) {
		// Running in a detached task as thumbnail generation can take a while and we don't want to block the watcher

		if let Some(cas_id) = cas_id {
			spawn({
				let extension = extension.clone();
				let path = path.to_path_buf();
				let node = node.clone();
				let library_id = *library_id;

				async move {
					if let Err(e) = node
						.thumbnailer
						.generate_single_indexed_thumbnail(&extension, cas_id, path, library_id)
						.await
					{
						error!("Failed to generate thumbnail in the watcher: {e:#?}");
					}
				}
			});
		}

		// TODO: Currently we only extract media data for images, remove this if later
		if matches!(kind, ObjectKind::Image) {
			if let Ok(image_extension) = ImageExtension::from_str(&extension) {
				if can_extract_media_data_for_image(&image_extension) {
					if let Ok(media_data) = extract_media_data(path)
						.await
						.map_err(|e| error!("Failed to extract media data: {e:#?}"))
					{
						if let Ok(media_data_params) = media_data_image_to_query_params(media_data)
							.map_err(|e| {
								error!("Failed to prepare media data create params: {e:#?}")
							}) {
							db.media_data()
								.upsert(
									media_data::object_id::equals(object_id),
									media_data::create(
										object::id::equals(object_id),
										media_data_params.clone(),
									),
									media_data_params,
								)
								.exec()
								.await?;
						}
					}
				}
			}
		}
	}

	invalidate_query!(library, "search.paths");
	invalidate_query!(library, "search.objects");

	Ok(())
}

pub(super) async fn update_file(
	location_id: location::id::Type,
	full_path: impl AsRef<Path>,
	node: &Arc<Node>,
	library: &Arc<Library>,
) -> Result<(), LocationManagerError> {
	let full_path = full_path.as_ref();

	let metadata = match fs::metadata(full_path).await {
		Ok(metadata) => metadata,
		Err(e) if e.kind() == io::ErrorKind::NotFound => {
			// If the file doesn't exist anymore, it was just a temporary file
			return Ok(());
		}
		Err(e) => return Err(FileIOError::from((full_path, e)).into()),
	};

	let location_path = extract_location_path(location_id, library).await?;

	if let Some(ref file_path) = library
		.db
		.file_path()
		.find_first(filter_existing_file_path_params(
			&IsolatedFilePathData::new(location_id, &location_path, full_path, false)?,
		))
		// include object for orphan check
		.include(file_path_with_object::include())
		.exec()
		.await?
	{
		inner_update_file(location_path, file_path, full_path, node, library, None).await
	} else {
		inner_create_file(
			location_id,
			location_path,
			full_path,
			&metadata,
			node,
			library,
		)
		.await
	}
	.map(|_| {
		invalidate_query!(library, "search.paths");
		invalidate_query!(library, "search.objects");
	})
}

async fn inner_update_file(
	location_path: impl AsRef<Path>,
	file_path: &file_path_with_object::Data,
	full_path: impl AsRef<Path>,
	node: &Arc<Node>,
	library @ Library { db, sync, .. }: &Library,
	maybe_new_inode: Option<INode>,
) -> Result<(), LocationManagerError> {
	let full_path = full_path.as_ref();
	let location_path = location_path.as_ref();

	let current_inode =
		inode_from_db(&maybe_missing(file_path.inode.as_ref(), "file_path.inode")?[0..8]);

	trace!(
		"Location: <root_path ='{}'> updating file: {}",
		location_path.display(),
		full_path.display()
	);

	let iso_file_path = IsolatedFilePathData::try_from(file_path)?;

	let FileMetadata {
		cas_id,
		fs_metadata,
		kind,
	} = FileMetadata::new(&location_path, &iso_file_path).await?;

	let inode = if let Some(inode) = maybe_new_inode {
		inode
	} else {
		#[cfg(target_family = "unix")]
		{
			get_inode(&fs_metadata)
		}

		#[cfg(target_family = "windows")]
		{
			// FIXME: This is a workaround for Windows, because we can't get the inode from the metadata
			get_inode_from_path(full_path).await?
		}
	};

	let is_hidden = path_is_hidden(full_path, &fs_metadata);
	if file_path.cas_id != cas_id {
		let (sync_params, db_params): (Vec<_>, Vec<_>) = {
			use file_path::*;

			[
				(
					(cas_id::NAME, json!(file_path.cas_id)),
					Some(cas_id::set(file_path.cas_id.clone())),
				),
				(
					(
						size_in_bytes_bytes::NAME,
						json!(fs_metadata.len().to_be_bytes().to_vec()),
					),
					Some(size_in_bytes_bytes::set(Some(
						fs_metadata.len().to_be_bytes().to_vec(),
					))),
				),
				{
					let date = DateTime::<Utc>::from(fs_metadata.modified_or_now()).into();

					(
						(date_modified::NAME, json!(date)),
						Some(date_modified::set(Some(date))),
					)
				},
				{
					// TODO: Should this be a skip rather than a null-set?
					let checksum = if file_path.integrity_checksum.is_some() {
						// If a checksum was already computed, we need to recompute it
						Some(
							file_checksum(full_path)
								.await
								.map_err(|e| FileIOError::from((full_path, e)))?,
						)
					} else {
						None
					};

					(
						(integrity_checksum::NAME, json!(checksum)),
						Some(integrity_checksum::set(checksum)),
					)
				},
				{
					if current_inode != inode {
						(
							(inode::NAME, json!(inode)),
							Some(inode::set(Some(inode_to_db(inode)))),
						)
					} else {
						((inode::NAME, serde_json::Value::Null), None)
					}
				},
				{
					if is_hidden != file_path.hidden.unwrap_or_default() {
						(
							(hidden::NAME, json!(inode)),
							Some(hidden::set(Some(is_hidden))),
						)
					} else {
						((hidden::NAME, serde_json::Value::Null), None)
					}
				},
			]
			.into_iter()
			.filter_map(|(sync_param, maybe_db_param)| {
				maybe_db_param.map(|db_param| (sync_param, db_param))
			})
			.unzip()
		};

		// file content changed
		sync.write_ops(
			db,
			(
				sync_params
					.into_iter()
					.map(|(field, value)| {
						sync.shared_update(
							prisma_sync::file_path::SyncId {
								pub_id: file_path.pub_id.clone(),
							},
							field,
							value,
						)
					})
					.collect(),
				db.file_path().update(
					file_path::pub_id::equals(file_path.pub_id.clone()),
					db_params,
				),
			),
		)
		.await?;

		if let Some(ref object) = file_path.object {
			let int_kind = kind as i32;

			if db
				.file_path()
				.count(vec![file_path::object_id::equals(Some(object.id))])
				.exec()
				.await? == 1
			{
				if object.kind.map(|k| k != int_kind).unwrap_or_default() {
					sync.write_op(
						db,
						sync.shared_update(
							prisma_sync::object::SyncId {
								pub_id: object.pub_id.clone(),
							},
							object::kind::NAME,
							json!(int_kind),
						),
						db.object().update(
							object::id::equals(object.id),
							vec![object::kind::set(Some(int_kind))],
						),
					)
					.await?;
				}
			} else {
				let pub_id = uuid_to_bytes(Uuid::new_v4());
				let date_created: DateTime<FixedOffset> =
					DateTime::<Local>::from(fs_metadata.created_or_now()).into();

				sync.write_ops(
					db,
					(
						sync.shared_create(
							prisma_sync::object::SyncId {
								pub_id: pub_id.clone(),
							},
							[
								(object::date_created::NAME, json!(date_created)),
								(object::kind::NAME, json!(int_kind)),
							],
						),
						db.object().create(
							pub_id.to_vec(),
							vec![
								object::date_created::set(Some(date_created)),
								object::kind::set(Some(int_kind)),
							],
						),
					),
				)
				.await?;

				sync.write_op(
					db,
					sync.shared_update(
						prisma_sync::location::SyncId {
							pub_id: file_path.pub_id.clone(),
						},
						file_path::object::NAME,
						json!(prisma_sync::object::SyncId {
							pub_id: pub_id.clone()
						}),
					),
					db.file_path().update(
						file_path::pub_id::equals(file_path.pub_id.clone()),
						vec![file_path::object::connect(object::pub_id::equals(pub_id))],
					),
				)
				.await?;
			}

			if let Some(old_cas_id) = &file_path.cas_id {
				// if this file had a thumbnail previously, we update it to match the new content
				if library.thumbnail_exists(node, old_cas_id).await? {
					if let Some(ext) = file_path.extension.clone() {
						// Running in a detached task as thumbnail generation can take a while and we don't want to block the watcher
						if let Some(cas_id) = cas_id {
							let node = Arc::clone(node);
							let path = full_path.to_path_buf();
							let library_id = library.id;
							let old_cas_id = old_cas_id.clone();
							spawn(async move {
								let was_overwritten = old_cas_id == cas_id;
								if let Err(e) = node
									.thumbnailer
									.generate_single_indexed_thumbnail(
										&ext, cas_id, path, library_id,
									)
									.await
								{
									error!("Failed to generate thumbnail in the watcher: {e:#?}");
								}

								// If only a few bytes changed, cas_id will probably remains intact
								// so we overwrote our previous thumbnail, so we can't remove it
								if !was_overwritten {
									// remove the old thumbnail as we're generating a new one
									let thumb_path =
										get_indexed_thumbnail_path(&node, &old_cas_id, library_id);
									if let Err(e) = fs::remove_file(&thumb_path).await {
										error!(
											"Failed to remove old thumbnail: {:#?}",
											FileIOError::from((thumb_path, e))
										);
									}
								}
							});
						}
					}
				}
			}

			// TODO: Change this if to include ObjectKind::Video in the future
			if let Some(ext) = &file_path.extension {
				if let Ok(image_extension) = ImageExtension::from_str(ext) {
					if can_extract_media_data_for_image(&image_extension)
						&& matches!(kind, ObjectKind::Image)
					{
						if let Ok(media_data) = extract_media_data(full_path)
							.await
							.map_err(|e| error!("Failed to extract media data: {e:#?}"))
						{
							if let Ok(media_data_params) =
								media_data_image_to_query_params(media_data).map_err(|e| {
									error!("Failed to prepare media data create params: {e:#?}")
								}) {
								db.media_data()
									.upsert(
										media_data::object_id::equals(object.id),
										media_data::create(
											object::id::equals(object.id),
											media_data_params.clone(),
										),
										media_data_params,
									)
									.exec()
									.await?;
							}
						}
					}
				}
			}
		}

		invalidate_query!(library, "search.paths");
		invalidate_query!(library, "search.objects");
	} else if is_hidden != file_path.hidden.unwrap_or_default() {
		sync.write_ops(
			db,
			(
				vec![sync.shared_update(
					prisma_sync::file_path::SyncId {
						pub_id: file_path.pub_id.clone(),
					},
					file_path::hidden::NAME,
					json!(is_hidden),
				)],
				db.file_path().update(
					file_path::pub_id::equals(file_path.pub_id.clone()),
					vec![file_path::hidden::set(Some(is_hidden))],
				),
			),
		)
		.await?;

		invalidate_query!(library, "search.paths");
	}

	Ok(())
}

pub(super) async fn rename(
	location_id: location::id::Type,
	new_path: impl AsRef<Path>,
	old_path: impl AsRef<Path>,
	new_path_metadata: Metadata,
	library: &Library,
) -> Result<(), LocationManagerError> {
	let location_path = extract_location_path(location_id, library).await?;
	let old_path = old_path.as_ref();
	let new_path = new_path.as_ref();
	let Library { db, .. } = library;

	let old_path_materialized_str =
		extract_normalized_materialized_path_str(location_id, &location_path, old_path)?;

	let new_path_materialized_str =
		extract_normalized_materialized_path_str(location_id, &location_path, new_path)?;

	// Renaming a file could potentially be a move to another directory, so we check if our parent changed
	if old_path_materialized_str != new_path_materialized_str
		&& !check_file_path_exists::<FilePathError>(
			&IsolatedFilePathData::new(location_id, &location_path, new_path, true)?.parent(),
			db,
		)
		.await?
	{
		return Err(LocationManagerError::MoveError {
			path: new_path.into(),
			reason: "parent directory does not exist".into(),
		});
	}

	if let Some(file_path) = db
		.file_path()
		.find_first(loose_find_existing_file_path_params(
			location_id,
			&location_path,
			old_path,
		)?)
		.exec()
		.await?
	{
		let is_dir = maybe_missing(file_path.is_dir, "file_path.is_dir")?;

		let new = IsolatedFilePathData::new(location_id, &location_path, new_path, is_dir)?;

		// If the renamed path is a directory, we have to update every successor
		if is_dir {
			let old = IsolatedFilePathData::new(location_id, &location_path, old_path, is_dir)?;
			// TODO: Fetch all file_paths that will be updated and dispatch sync events

			let updated = library
				.db
				._execute_raw(raw!(
					"UPDATE file_path \
						SET materialized_path = REPLACE(materialized_path, {}, {}) \
						WHERE location_id = {}",
					PrismaValue::String(format!("{}/{}/", old.materialized_path, old.name)),
					PrismaValue::String(format!("{}/{}/", new.materialized_path, new.name)),
					PrismaValue::Int(location_id as i64)
				))
				.exec()
				.await?;
			trace!("Updated {updated} file_paths");
		}

		let is_hidden = path_is_hidden(new_path, &new_path_metadata);

		library
			.db
			.file_path()
			.update(
				file_path::pub_id::equals(file_path.pub_id),
				vec![
					file_path::materialized_path::set(Some(new_path_materialized_str)),
					file_path::name::set(Some(new.name.to_string())),
					file_path::extension::set(Some(new.extension.to_string())),
					file_path::date_modified::set(Some(
						DateTime::<Utc>::from(new_path_metadata.modified_or_now()).into(),
					)),
					file_path::hidden::set(Some(is_hidden)),
				],
			)
			.exec()
			.await?;

		invalidate_query!(library, "search.paths");
		invalidate_query!(library, "search.objects");
	}

	Ok(())
}

pub(super) async fn remove(
	location_id: location::id::Type,
	full_path: impl AsRef<Path>,
	library: &Library,
) -> Result<(), LocationManagerError> {
	let full_path = full_path.as_ref();
	let location_path = extract_location_path(location_id, library).await?;

	// if it doesn't exist either way, then we don't care
	let Some(file_path) = library
		.db
		.file_path()
		.find_first(loose_find_existing_file_path_params(
			location_id,
			&location_path,
			full_path,
		)?)
		.exec()
		.await?
	else {
		return Ok(());
	};

	remove_by_file_path(location_id, full_path, &file_path, library).await
}

pub(super) async fn remove_by_file_path(
	location_id: location::id::Type,
	path: impl AsRef<Path>,
	file_path: &file_path::Data,
	library: &Library,
) -> Result<(), LocationManagerError> {
	// check file still exists on disk
	match fs::metadata(path.as_ref()).await {
		Ok(_) => {
			todo!("file has changed in some way, re-identify it")
		}
		Err(e) if e.kind() == ErrorKind::NotFound => {
			let db = &library.db;

			let is_dir = maybe_missing(file_path.is_dir, "file_path.is_dir")?;

			// if is doesn't, we can remove it safely from our db
			if is_dir {
				delete_directory(
					library,
					location_id,
					Some(&IsolatedFilePathData::try_from(file_path)?),
				)
				.await?;
			} else {
				db.file_path()
					.delete(file_path::pub_id::equals(file_path.pub_id.clone()))
					.exec()
					.await?;

				if let Some(object_id) = file_path.object_id {
					db.object()
						.delete_many(vec![
							object::id::equals(object_id),
							// https://www.prisma.io/docs/reference/api-reference/prisma-client-reference#none
							object::file_paths::none(vec![]),
						])
						.exec()
						.await?;
				}
			}
		}
		Err(e) => return Err(FileIOError::from((path, e)).into()),
	}

	invalidate_query!(library, "search.paths");
	invalidate_query!(library, "search.objects");

	Ok(())
}

pub(super) async fn extract_inode_from_path(
	location_id: location::id::Type,
	path: impl AsRef<Path>,
	library: &Library,
) -> Result<INode, LocationManagerError> {
	let path = path.as_ref();
	let location = find_location(library, location_id)
		.select(location::select!({ path }))
		.exec()
		.await?
		.ok_or(LocationManagerError::MissingLocation(location_id))?;

	let location_path = maybe_missing(&location.path, "location.path")?;

	library
		.db
		.file_path()
		.find_first(loose_find_existing_file_path_params(
			location_id,
			location_path,
			path,
		)?)
		.select(file_path::select!({ inode }))
		.exec()
		.await?
		.map_or(
			Err(FilePathError::NotFound(path.into()).into()),
			|file_path| {
				Ok(inode_from_db(
					&maybe_missing(file_path.inode.as_ref(), "file_path.inode")?[0..8],
				))
			},
		)
}

pub(super) async fn extract_location_path(
	location_id: location::id::Type,
	library: &Library,
) -> Result<PathBuf, LocationManagerError> {
	find_location(library, location_id)
		.select(location::select!({ path }))
		.exec()
		.await?
		.map_or(
			Err(LocationManagerError::MissingLocation(location_id)),
			// NOTE: The following usage of `PathBuf` doesn't incur a new allocation so it's fine
			|location| Ok(maybe_missing(location.path, "location.path")?.into()),
		)
}

pub(super) async fn recalculate_directories_size(
	candidates: &mut HashMap<PathBuf, Instant>,
	buffer: &mut Vec<(PathBuf, Instant)>,
	location_id: location::id::Type,
	library: &Library,
) -> Result<(), LocationManagerError> {
	let mut location_path_cache = None;
	let mut should_invalidate = false;
	let mut should_update_location_size = false;
	buffer.clear();

	for (path, instant) in candidates.drain() {
		if instant.elapsed() > HUNDRED_MILLIS * 5 {
			if location_path_cache.is_none() {
				location_path_cache = Some(PathBuf::from(maybe_missing(
					find_location(library, location_id)
						.select(location::select!({ path }))
						.exec()
						.await?
						.ok_or(LocationManagerError::MissingLocation(location_id))?
						.path,
					"location.path",
				)?))
			}

			if let Some(location_path) = &location_path_cache {
				if path != *location_path {
					trace!(
						"Reverse calculating directory sizes starting at {} until {}",
						path.display(),
						location_path.display(),
					);
					reverse_update_directories_sizes(path, location_id, location_path, library)
						.await?;
					should_invalidate = true;
				} else {
					should_update_location_size = true;
				}
			}
		} else {
			buffer.push((path, instant));
		}
	}

	if should_update_location_size {
		update_location_size(location_id, library).await?;
	}

	if should_invalidate {
		invalidate_query!(library, "search.paths");
		invalidate_query!(library, "search.objects");
	}

	candidates.extend(buffer.drain(..));

	Ok(())
}



File: ./src/location/manager/helpers.rs
-------------------------------------------------
use crate::{
	library::{Library, LibraryId},
	prisma::location,
	util::db::maybe_missing,
	Node,
};

use std::{
	collections::{HashMap, HashSet},
	path::{Path, PathBuf},
	sync::Arc,
	time::Duration,
};

use tokio::{fs, io::ErrorKind, sync::oneshot, time::sleep};
use tracing::{error, warn};
use uuid::Uuid;

use super::{watcher::LocationWatcher, LocationManagerError};

type LocationAndLibraryKey = (location::id::Type, LibraryId);

const LOCATION_CHECK_INTERVAL: Duration = Duration::from_secs(5);

pub(super) async fn check_online(
	location: &location::Data,
	node: &Node,
	library: &Library,
) -> Result<bool, LocationManagerError> {
	let pub_id = Uuid::from_slice(&location.pub_id)?;

	let location_path = maybe_missing(&location.path, "location.path").map(Path::new)?;

	// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
	if location.instance_id == Some(library.config().await.instance_id) {
		match fs::metadata(&location_path).await {
			Ok(_) => {
				node.locations.add_online(pub_id).await;
				Ok(true)
			}
			Err(e) if e.kind() == ErrorKind::NotFound => {
				node.locations.remove_online(&pub_id).await;
				Ok(false)
			}
			Err(e) => {
				error!("Failed to check if location is online: {:#?}", e);
				Ok(false)
			}
		}
	} else {
		// In this case, we don't have a `local_path`, but this location was marked as online
		node.locations.remove_online(&pub_id).await;
		Err(LocationManagerError::NonLocalLocation(location.id))
	}
}

pub(super) async fn location_check_sleep(
	location_id: location::id::Type,
	library: Arc<Library>,
) -> (location::id::Type, Arc<Library>) {
	sleep(LOCATION_CHECK_INTERVAL).await;
	(location_id, library)
}

pub(super) fn watch_location(
	location: location::Data,
	library_id: LibraryId,
	locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
) {
	let location_id = location.id;
	let location_path = location.path.as_ref();
	let Some(location_path) = location_path.map(Path::new) else {
		return;
	};

	if let Some(mut watcher) = locations_unwatched.remove(&(location_id, library_id)) {
		if watcher.check_path(location_path) {
			watcher.watch();
		}

		locations_watched.insert((location_id, library_id), watcher);
	}
}

pub(super) fn unwatch_location(
	location: location::Data,
	library_id: LibraryId,
	locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
) {
	let location_id = location.id;
	let location_path = location.path.as_ref();
	let Some(location_path) = location_path.map(Path::new) else {
		return;
	};

	if let Some(mut watcher) = locations_watched.remove(&(location_id, library_id)) {
		if watcher.check_path(location_path) {
			watcher.unwatch();
		}

		locations_unwatched.insert((location_id, library_id), watcher);
	}
}

pub(super) fn drop_location(
	location_id: location::id::Type,
	library_id: LibraryId,
	message: &str,
	locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
) {
	warn!("{message}: <id='{location_id}', library_id='{library_id}'>",);
	if let Some(mut watcher) = locations_watched.remove(&(location_id, library_id)) {
		watcher.unwatch();
	} else {
		locations_unwatched.remove(&(location_id, library_id));
	}
}

pub(super) async fn get_location(
	location_id: location::id::Type,
	library: &Library,
) -> Option<location::Data> {
	library
		.db
		.location()
		.find_unique(location::id::equals(location_id))
		.exec()
		.await
		.unwrap_or_else(|err| {
			error!("Failed to get location data from location_id: {:#?}", err);
			None
		})
}

pub(super) async fn handle_remove_location_request(
	location_id: location::id::Type,
	library: Arc<Library>,
	response_tx: oneshot::Sender<Result<(), LocationManagerError>>,
	forced_unwatch: &mut HashSet<LocationAndLibraryKey>,
	locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	to_remove: &mut HashSet<LocationAndLibraryKey>,
) {
	let key = (location_id, library.id);
	if let Some(location) = get_location(location_id, &library).await {
		// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
		if location.instance_id == Some(library.config().await.instance_id) {
			unwatch_location(location, library.id, locations_watched, locations_unwatched);
			locations_unwatched.remove(&key);
			forced_unwatch.remove(&key);
		} else {
			drop_location(
		 		location_id,
		 		library.id,
		 		"Dropping location from location manager, because we don't have a `local_path` anymore",
		 		locations_watched,
		 		locations_unwatched
		 	);
		}
	} else {
		drop_location(
			location_id,
			library.id,
			"Removing location from manager, as we failed to fetch from db",
			locations_watched,
			locations_unwatched,
		);
	}

	// Marking location as removed, so we don't try to check it when the time comes
	to_remove.insert(key);

	let _ = response_tx.send(Ok(())); // ignore errors, we handle errors on receiver
}

pub(super) async fn handle_stop_watcher_request(
	location_id: location::id::Type,
	library: Arc<Library>,
	response_tx: oneshot::Sender<Result<(), LocationManagerError>>,
	forced_unwatch: &mut HashSet<LocationAndLibraryKey>,
	locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
) {
	async fn inner(
		location_id: location::id::Type,
		library: Arc<Library>,
		forced_unwatch: &mut HashSet<LocationAndLibraryKey>,
		locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
		locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	) -> Result<(), LocationManagerError> {
		let key = (location_id, library.id);
		if !forced_unwatch.contains(&key) && locations_watched.contains_key(&key) {
			get_location(location_id, &library)
				.await
				.ok_or_else(|| LocationManagerError::FailedToStopOrReinitWatcher {
					reason: String::from("failed to fetch location from db"),
				})
				.map(|location| {
					unwatch_location(location, library.id, locations_watched, locations_unwatched);
					forced_unwatch.insert(key);
				})
		} else {
			Ok(())
		}
	}

	let _ = response_tx.send(
		inner(
			location_id,
			library,
			forced_unwatch,
			locations_watched,
			locations_unwatched,
		)
		.await,
	); // ignore errors, we handle errors on receiver
}

pub(super) async fn handle_reinit_watcher_request(
	location_id: location::id::Type,
	library: Arc<Library>,
	response_tx: oneshot::Sender<Result<(), LocationManagerError>>,
	forced_unwatch: &mut HashSet<LocationAndLibraryKey>,
	locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
) {
	async fn inner(
		location_id: location::id::Type,
		library: Arc<Library>,
		forced_unwatch: &mut HashSet<LocationAndLibraryKey>,
		locations_watched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
		locations_unwatched: &mut HashMap<LocationAndLibraryKey, LocationWatcher>,
	) -> Result<(), LocationManagerError> {
		let key = (location_id, library.id);
		if forced_unwatch.contains(&key) && locations_unwatched.contains_key(&key) {
			get_location(location_id, &library)
				.await
				.ok_or_else(|| LocationManagerError::FailedToStopOrReinitWatcher {
					reason: String::from("failed to fetch location from db"),
				})
				.map(|location| {
					watch_location(location, library.id, locations_watched, locations_unwatched);
					forced_unwatch.remove(&key);
				})
		} else {
			Ok(())
		}
	}

	let _ = response_tx.send(
		inner(
			location_id,
			library,
			forced_unwatch,
			locations_watched,
			locations_unwatched,
		)
		.await,
	); // ignore errors, we handle errors on receiver
}

pub(super) fn handle_ignore_path_request(
	location_id: location::id::Type,
	library: Arc<Library>,
	path: PathBuf,
	ignore: bool,
	response_tx: oneshot::Sender<Result<(), LocationManagerError>>,
	locations_watched: &HashMap<LocationAndLibraryKey, LocationWatcher>,
) {
	let _ = response_tx.send(
		if let Some(watcher) = locations_watched.get(&(location_id, library.id)) {
			watcher.ignore_path(path, ignore)
		} else {
			Ok(())
		},
	); // ignore errors, we handle errors on receiver
}



File: ./src/location/manager/mod.rs
-------------------------------------------------
use crate::{
	job::JobManagerError,
	library::{Library, LibraryManagerEvent},
	prisma::location,
	util::{db::MissingFieldError, error::FileIOError},
	Node,
};

use std::{
	collections::BTreeSet,
	path::{Path, PathBuf},
	sync::Arc,
};

use futures::executor::block_on;
use thiserror::Error;
use tokio::sync::{
	broadcast::{self, Receiver},
	oneshot, RwLock,
};
use tracing::error;

#[cfg(feature = "location-watcher")]
use tokio::sync::mpsc;
use uuid::Uuid;

use super::file_path_helper::FilePathError;

#[cfg(feature = "location-watcher")]
mod watcher;

#[cfg(feature = "location-watcher")]
mod helpers;

#[derive(Clone, Copy, Debug)]
#[allow(dead_code)]
enum ManagementMessageAction {
	Add,
	Remove,
}

#[derive(Debug)]
#[allow(dead_code)]
pub struct LocationManagementMessage {
	location_id: location::id::Type,
	library: Arc<Library>,
	action: ManagementMessageAction,
	response_tx: oneshot::Sender<Result<(), LocationManagerError>>,
}

#[derive(Debug)]
#[allow(dead_code)]
enum WatcherManagementMessageAction {
	Stop,
	Reinit,
	IgnoreEventsForPath { path: PathBuf, ignore: bool },
}

#[derive(Debug)]
#[allow(dead_code)]
pub struct WatcherManagementMessage {
	location_id: location::id::Type,
	library: Arc<Library>,
	action: WatcherManagementMessageAction,
	response_tx: oneshot::Sender<Result<(), LocationManagerError>>,
}

#[derive(Error, Debug)]
pub enum LocationManagerError {
	#[cfg(feature = "location-watcher")]
	#[error("Unable to send location management message to location manager actor: (error: {0})")]
	ActorSendLocationError(#[from] mpsc::error::SendError<LocationManagementMessage>),

	#[cfg(feature = "location-watcher")]
	#[error("Unable to send path to be ignored by watcher actor: (error: {0})")]
	ActorIgnorePathError(#[from] mpsc::error::SendError<watcher::IgnorePath>),

	#[cfg(feature = "location-watcher")]
	#[error("Unable to watcher management message to watcher manager actor: (error: {0})")]
	ActorIgnorePathMessageError(#[from] mpsc::error::SendError<WatcherManagementMessage>),

	#[error("Unable to receive actor response: (error: {0})")]
	ActorResponseError(#[from] oneshot::error::RecvError),

	#[cfg(feature = "location-watcher")]
	#[error("Watcher error: (error: {0})")]
	WatcherError(#[from] notify::Error),

	#[error("Failed to stop or reinit a watcher: {reason}")]
	FailedToStopOrReinitWatcher { reason: String },

	#[error("Missing location from database: <id='{0}'>")]
	MissingLocation(location::id::Type),

	#[error("Non local location: <id='{0}'>")]
	NonLocalLocation(location::id::Type),

	#[error("failed to move file '{}' for reason: {reason}", .path.display())]
	MoveError { path: Box<Path>, reason: String },

	#[error("Tried to update a non-existing file: <path='{0}'>")]
	UpdateNonExistingFile(PathBuf),
	#[error("Database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error("File path related error (error: {0})")]
	FilePath(#[from] FilePathError),
	#[error("Corrupted location pub_id on database: (error: {0})")]
	CorruptedLocationPubId(#[from] uuid::Error),
	#[error("Job Manager error: (error: {0})")]
	JobManager(#[from] JobManagerError),
	#[error("missing-field")]
	MissingField(#[from] MissingFieldError),

	#[error(transparent)]
	FileIO(#[from] FileIOError),
}

type OnlineLocations = BTreeSet<Vec<u8>>;

#[must_use = "'LocationManagerActor::start' must be used to start the actor"]
pub struct LocationManagerActor {
	#[cfg(feature = "location-watcher")]
	location_management_rx: mpsc::Receiver<LocationManagementMessage>,
	#[cfg(feature = "location-watcher")]
	watcher_management_rx: mpsc::Receiver<WatcherManagementMessage>,
	#[cfg(feature = "location-watcher")]
	stop_rx: oneshot::Receiver<()>,
}

impl LocationManagerActor {
	pub fn start(self, node: Arc<Node>) {
		tokio::spawn({
			let node = node.clone();
			let rx = node.libraries.rx.clone();
			async move {
				if let Err(err) = rx
					.subscribe(|event| {
						let node = node.clone();
						async move {
							match event {
								LibraryManagerEvent::Load(library) => {
									for location in library
										.db
										.location()
										.find_many(vec![])
										.exec()
										.await
										.unwrap_or_else(|e| {
											error!(
													"Failed to get locations from database for location manager: {:#?}",
													e
												);
											vec![]
										}) {
										if let Err(e) =
											node.locations.add(location.id, library.clone()).await
										{
											error!(
												"Failed to add location to location manager: {:#?}",
												e
											);
										}
									}
								}
								LibraryManagerEvent::Edit(_) => {}
								LibraryManagerEvent::InstancesModified(_) => {}
								LibraryManagerEvent::Delete(_) => {
									#[cfg(debug_assertions)]
									error!("TODO: Remove locations from location manager"); // TODO
								}
							}
						}
					})
					.await
				{
					error!("Core may become unstable! LocationManager's library manager subscription aborted with error: {err:?}");
				}
			}
		});

		#[cfg(feature = "location-watcher")]
		tokio::spawn(Locations::run_locations_checker(
			self.location_management_rx,
			self.watcher_management_rx,
			self.stop_rx,
			node,
		));

		#[cfg(not(feature = "location-watcher"))]
		tracing::warn!("Location watcher is disabled, locations will not be checked");
	}
}

pub struct Locations {
	online_locations: RwLock<OnlineLocations>,
	pub online_tx: broadcast::Sender<OnlineLocations>,
	#[cfg(feature = "location-watcher")]
	location_management_tx: mpsc::Sender<LocationManagementMessage>,
	#[cfg(feature = "location-watcher")]
	watcher_management_tx: mpsc::Sender<WatcherManagementMessage>,
	stop_tx: Option<oneshot::Sender<()>>,
}

impl Locations {
	pub fn new() -> (Self, LocationManagerActor) {
		let online_tx = broadcast::channel(16).0;

		#[cfg(feature = "location-watcher")]
		{
			let (location_management_tx, location_management_rx) = mpsc::channel(128);
			let (watcher_management_tx, watcher_management_rx) = mpsc::channel(128);
			let (stop_tx, stop_rx) = oneshot::channel();

			(
				Self {
					online_locations: Default::default(),
					online_tx,
					location_management_tx,
					watcher_management_tx,
					stop_tx: Some(stop_tx),
				},
				LocationManagerActor {
					location_management_rx,
					watcher_management_rx,
					stop_rx,
				},
			)
		}

		#[cfg(not(feature = "location-watcher"))]
		{
			tracing::warn!("Location watcher is disabled, locations will not be checked");
			(
				Self {
					online_tx,
					online_locations: Default::default(),
					stop_tx: None,
				},
				LocationManagerActor {},
			)
		}
	}

	#[inline]
	#[allow(unused_variables)]
	async fn location_management_message(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
		action: ManagementMessageAction,
	) -> Result<(), LocationManagerError> {
		#[cfg(feature = "location-watcher")]
		{
			let (tx, rx) = oneshot::channel();

			self.location_management_tx
				.send(LocationManagementMessage {
					location_id,
					library,
					action,
					response_tx: tx,
				})
				.await?;

			rx.await?
		}

		#[cfg(not(feature = "location-watcher"))]
		Ok(())
	}

	#[inline]
	#[allow(unused_variables)]
	async fn watcher_management_message(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
		action: WatcherManagementMessageAction,
	) -> Result<(), LocationManagerError> {
		#[cfg(feature = "location-watcher")]
		{
			let (tx, rx) = oneshot::channel();

			self.watcher_management_tx
				.send(WatcherManagementMessage {
					location_id,
					library,
					action,
					response_tx: tx,
				})
				.await?;

			rx.await?
		}

		#[cfg(not(feature = "location-watcher"))]
		Ok(())
	}

	pub async fn add(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
	) -> Result<(), LocationManagerError> {
		self.location_management_message(location_id, library, ManagementMessageAction::Add)
			.await
	}

	pub async fn remove(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
	) -> Result<(), LocationManagerError> {
		self.location_management_message(location_id, library, ManagementMessageAction::Remove)
			.await
	}

	pub async fn stop_watcher(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
	) -> Result<(), LocationManagerError> {
		self.watcher_management_message(location_id, library, WatcherManagementMessageAction::Stop)
			.await
	}

	pub async fn reinit_watcher(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
	) -> Result<(), LocationManagerError> {
		self.watcher_management_message(
			location_id,
			library,
			WatcherManagementMessageAction::Reinit,
		)
		.await
	}

	pub async fn temporary_stop(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
	) -> Result<StopWatcherGuard, LocationManagerError> {
		self.stop_watcher(location_id, library.clone()).await?;

		Ok(StopWatcherGuard {
			location_id,
			library: Some(library),
			manager: self,
		})
	}

	pub async fn temporary_ignore_events_for_path(
		&self,
		location_id: location::id::Type,
		library: Arc<Library>,
		path: impl AsRef<Path>,
	) -> Result<IgnoreEventsForPathGuard, LocationManagerError> {
		let path = path.as_ref().to_path_buf();

		self.watcher_management_message(
			location_id,
			library.clone(),
			WatcherManagementMessageAction::IgnoreEventsForPath {
				path: path.clone(),
				ignore: true,
			},
		)
		.await?;

		Ok(IgnoreEventsForPathGuard {
			location_id,
			library: Some(library),
			manager: self,
			path: Some(path),
		})
	}

	#[cfg(feature = "location-watcher")]
	async fn run_locations_checker(
		mut location_management_rx: mpsc::Receiver<LocationManagementMessage>,
		mut watcher_management_rx: mpsc::Receiver<WatcherManagementMessage>,
		mut stop_rx: oneshot::Receiver<()>,
		node: Arc<Node>,
	) -> Result<(), LocationManagerError> {
		use std::collections::{HashMap, HashSet};

		use futures::stream::{FuturesUnordered, StreamExt};
		use tokio::select;
		use tracing::{info, warn};

		use helpers::{
			check_online, drop_location, get_location, handle_ignore_path_request,
			handle_reinit_watcher_request, handle_remove_location_request,
			handle_stop_watcher_request, location_check_sleep, unwatch_location, watch_location,
		};
		use watcher::LocationWatcher;

		let mut to_check_futures = FuturesUnordered::new();
		let mut to_remove = HashSet::new();
		let mut locations_watched = HashMap::new();
		let mut locations_unwatched = HashMap::new();
		let mut forced_unwatch = HashSet::new();

		loop {
			select! {
				// Location management messages
				Some(LocationManagementMessage{
					location_id,
					library,
					action,
					response_tx
				}) = location_management_rx.recv() => {
					match action {

						// To add a new location
						ManagementMessageAction::Add => {
							response_tx.send(
							if let Some(location) = get_location(location_id, &library).await {
								match check_online(&location, &node, &library).await {
									Ok(is_online) => {

										LocationWatcher::new(location, library.clone(), node.clone())
										.await
										.map(|mut watcher| {
											if is_online {
												watcher.watch();
												locations_watched.insert(
													(location_id, library.id),
													watcher
												);
											} else {
												locations_unwatched.insert(
													(location_id, library.id),
													watcher
												);
											}

											to_check_futures.push(
												location_check_sleep(location_id, library)
											);
										}
									)
									},
									Err(e) => {
										error!("Error while checking online status of location {location_id}: {e}");
										Ok(()) // TODO: Probs should be error but that will break startup when location is offline
									}
								}
							} else {
								warn!(
									"Location not found in database to be watched: {}",
									location_id
								);
								Ok(()) // TODO: Probs should be error but that will break startup when location is offline
							}).ok(); // ignore errors, we handle errors on receiver
						},

						// To remove an location
						ManagementMessageAction::Remove => {
							handle_remove_location_request(
								location_id,
								library,
								response_tx,
								&mut forced_unwatch,
								&mut locations_watched,
								&mut locations_unwatched,
								&mut to_remove,
							).await;
						},
					}
				}

				// Watcher management messages
				Some(WatcherManagementMessage{
					location_id,
					library,
					action,
					response_tx,
				}) = watcher_management_rx.recv() => {
					match action {
						// To stop a watcher
						WatcherManagementMessageAction::Stop => {
							handle_stop_watcher_request(
								location_id,
								library,
								response_tx,
								&mut forced_unwatch,
								&mut locations_watched,
								&mut locations_unwatched,
							).await;
						},

						// To reinit a stopped watcher
						WatcherManagementMessageAction::Reinit => {
							handle_reinit_watcher_request(
								location_id,
								library,
								response_tx,
								&mut forced_unwatch,
								&mut locations_watched,
								&mut locations_unwatched,
							).await;
						},

						// To ignore or not events for a path
						WatcherManagementMessageAction::IgnoreEventsForPath { path, ignore } => {
							handle_ignore_path_request(
								location_id,
								library,
								path,
								ignore,
								response_tx,
								&locations_watched,
							);
						},
					}
				}

				// Periodically checking locations
				Some((location_id, library)) = to_check_futures.next() => {
					let key = (location_id, library.id);

					if to_remove.contains(&key) {
						// The time to check came for an already removed library, so we just ignore it
						to_remove.remove(&key);
					} else if let Some(location) = get_location(location_id, &library).await {
						// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
						if location.instance_id == Some(library.config().await.instance_id) {
							let is_online = match check_online(&location, &node, &library).await {
								Ok(is_online) => is_online,
								Err(e) => {
									error!("Error while checking online status of location {location_id}: {e}");
									continue;
								}
							};

							if is_online
								&& !forced_unwatch.contains(&key)
							{
								watch_location(
									location,
									library.id,
									&mut locations_watched,
									&mut locations_unwatched,
								);
							} else {
								unwatch_location(
									location,
									library.id,
									&mut locations_watched,
									&mut locations_unwatched,
								);
							}
							to_check_futures.push(location_check_sleep(location_id, library));
						} else {
							drop_location(
								location_id,
								library.id,
								"Dropping location from location manager, because \
								it isn't a location in the current node",
								&mut locations_watched,
								&mut locations_unwatched
							);
							forced_unwatch.remove(&key);
						}
					} else {
						drop_location(
							location_id,
							library.id,
							"Removing location from manager, as we failed to fetch from db",
							&mut locations_watched,
							&mut locations_unwatched,
						);
						forced_unwatch.remove(&key);
					}
				}

				_ = &mut stop_rx => {
					info!("Stopping location manager");
					break;
				}
			}
		}

		Ok(())
	}

	pub async fn is_online(&self, id: &Uuid) -> bool {
		let online_locations = self.online_locations.read().await;
		online_locations.iter().any(|v| v == id.as_bytes())
	}

	pub async fn get_online(&self) -> OnlineLocations {
		self.online_locations.read().await.clone()
	}

	async fn broadcast_online(&self) {
		self.online_tx.send(self.get_online().await).ok();
	}

	pub async fn add_online(&self, id: Uuid) {
		{
			self.online_locations
				.write()
				.await
				.insert(id.as_bytes().to_vec());
		}
		self.broadcast_online().await;
	}

	pub async fn remove_online(&self, id: &Uuid) {
		{
			let mut online_locations = self.online_locations.write().await;
			online_locations.retain(|v| v != id.as_bytes());
		}
		self.broadcast_online().await;
	}

	pub fn online_rx(&self) -> Receiver<OnlineLocations> {
		self.online_tx.subscribe()
	}
}

impl Drop for Locations {
	fn drop(&mut self) {
		if let Some(stop_tx) = self.stop_tx.take() {
			if stop_tx.send(()).is_err() {
				error!("Failed to send stop signal to location manager");
			}
		}
	}
}

#[must_use = "this `StopWatcherGuard` must be held for some time, so the watcher is stopped"]
pub struct StopWatcherGuard<'m> {
	manager: &'m Locations,
	location_id: location::id::Type,
	library: Option<Arc<Library>>,
}

impl Drop for StopWatcherGuard<'_> {
	fn drop(&mut self) {
		if cfg!(feature = "location-watcher") {
			// FIXME: change this Drop to async drop in the future
			if let Err(e) = block_on(self.manager.reinit_watcher(
				self.location_id,
				self.library.take().expect("library should be set"),
			)) {
				error!("Failed to reinit watcher on stop watcher guard drop: {e}");
			}
		}
	}
}

#[must_use = "this `IgnoreEventsForPathGuard` must be held for some time, so the watcher can ignore events for the desired path"]
pub struct IgnoreEventsForPathGuard<'m> {
	manager: &'m Locations,
	path: Option<PathBuf>,
	location_id: location::id::Type,
	library: Option<Arc<Library>>,
}

impl Drop for IgnoreEventsForPathGuard<'_> {
	fn drop(&mut self) {
		if cfg!(feature = "location-watcher") {
			// FIXME: change this Drop to async drop in the future
			if let Err(e) = block_on(self.manager.watcher_management_message(
				self.location_id,
				self.library.take().expect("library should be set"),
				WatcherManagementMessageAction::IgnoreEventsForPath {
					path: self.path.take().expect("path should be set"),
					ignore: false,
				},
			)) {
				error!("Failed to un-ignore path on watcher guard drop: {e}");
			}
		}
	}
}



File: ./src/location/mod.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{JobBuilder, JobError, JobManagerError},
	library::Library,
	location::file_path_helper::filter_existing_file_path_params,
	object::{
		file_identifier::{self, file_identifier_job::FileIdentifierJobInit},
		media::{media_processor, MediaProcessorJobInit},
	},
	prisma::{file_path, indexer_rules_in_location, location, PrismaClient},
	util::{
		db::{maybe_missing, MissingFieldError},
		error::{FileIOError, NonUtf8PathError},
	},
	Node,
};

use std::{
	collections::HashSet,
	path::{Component, Path, PathBuf},
	sync::Arc,
};

use chrono::Utc;
use futures::future::TryFutureExt;
use normpath::PathExt;
use prisma_client_rust::{operator::and, or, QueryError};
use sd_prisma::prisma_sync;
use sd_sync::*;
use sd_utils::uuid_to_bytes;
use serde::Deserialize;
use serde_json::json;
use specta::Type;
use tokio::{fs, io, time::Instant};
use tracing::{debug, info, warn};
use uuid::Uuid;

mod error;
pub mod file_path_helper;
pub mod indexer;
mod manager;
pub mod metadata;
pub mod non_indexed;

pub use error::LocationError;
use indexer::IndexerJobInit;
pub use manager::{LocationManagerError, Locations};
use metadata::SpacedriveLocationMetadataFile;

use file_path_helper::IsolatedFilePathData;

pub type LocationPubId = Uuid;

// Location includes!
location::include!(location_with_indexer_rules {
	indexer_rules: select { indexer_rule }
});

/// `LocationCreateArgs` is the argument received from the client using `rspc` to create a new location.
/// It has the actual path and a vector of indexer rules ids, to create many-to-many relationships
/// between the location and indexer rules.
#[derive(Type, Deserialize)]
pub struct LocationCreateArgs {
	pub path: PathBuf,
	pub dry_run: bool,
	pub indexer_rules_ids: Vec<i32>,
}

impl LocationCreateArgs {
	pub async fn create(
		self,
		node: &Node,
		library: &Arc<Library>,
	) -> Result<Option<location_with_indexer_rules::Data>, LocationError> {
		let Some(path_str) = self.path.to_str().map(str::to_string) else {
			return Err(LocationError::NonUtf8Path(NonUtf8PathError(
				self.path.into_boxed_path(),
			)));
		};

		let path_metadata = match fs::metadata(&self.path).await {
			Ok(metadata) => metadata,
			Err(e) if e.kind() == io::ErrorKind::NotFound => {
				return Err(LocationError::PathNotFound(self.path.into_boxed_path()))
			}
			Err(e) => {
				return Err(LocationError::LocationPathFilesystemMetadataAccess(
					FileIOError::from((self.path, e)),
				));
			}
		};

		if !path_metadata.is_dir() {
			return Err(LocationError::NotDirectory(self.path.into_boxed_path()));
		}

		if let Some(mut metadata) = SpacedriveLocationMetadataFile::try_load(&self.path).await? {
			metadata
				.clean_stale_libraries(
					&node
						.libraries
						.get_all()
						.await
						.into_iter()
						.map(|library| library.id)
						.collect(),
				)
				.await?;

			if !metadata.is_empty() {
				if let Some(old_path) = metadata.location_path(library.id) {
					if old_path == self.path {
						if library
							.db
							.location()
							.count(vec![location::path::equals(Some(path_str))])
							.exec()
							.await? > 0
						{
							// Location already exists in this library
							return Err(LocationError::LocationAlreadyExists(
								self.path.into_boxed_path(),
							));
						}
					} else {
						return Err(LocationError::NeedRelink {
							old_path: old_path.into(),
							new_path: self.path.into_boxed_path(),
						});
					}
				} else {
					return Err(LocationError::AddLibraryToMetadata(
						self.path.into_boxed_path(),
					));
				};
			}
		}

		debug!(
			"{} new location for '{}'",
			if self.dry_run {
				"Dry run: Would create"
			} else {
				"Trying to create"
			},
			self.path.display()
		);

		let uuid = Uuid::new_v4();

		let location = create_location(
			library,
			uuid,
			&self.path,
			&self.indexer_rules_ids,
			self.dry_run,
		)
		.await?;

		if let Some(location) = location {
			// Write location metadata to a .spacedrive file
			if let Err(err) = SpacedriveLocationMetadataFile::create_and_save(
				library.id,
				uuid,
				&self.path,
				location.name,
			)
			.err_into::<LocationError>()
			.and_then(|()| async move {
				node.locations
					.add(location.data.id, library.clone())
					.await
					.map_err(Into::into)
			})
			.await
			{
				delete_location(node, library, location.data.id).await?;
				Err(err)?;
			}

			info!("Created location: {:?}", &location.data);

			Ok(Some(location.data))
		} else {
			Ok(None)
		}
	}

	pub async fn add_library(
		self,
		node: &Node,
		library: &Arc<Library>,
	) -> Result<Option<location_with_indexer_rules::Data>, LocationError> {
		let Some(mut metadata) = SpacedriveLocationMetadataFile::try_load(&self.path).await? else {
			return Err(LocationError::MetadataNotFound(self.path.into_boxed_path()));
		};

		metadata
			.clean_stale_libraries(
				&node
					.libraries
					.get_all()
					.await
					.into_iter()
					.map(|library| library.id)
					.collect(),
			)
			.await?;

		if metadata.has_library(library.id) {
			return Err(LocationError::NeedRelink {
				old_path: metadata
					.location_path(library.id)
					.expect("We checked that we have this library_id")
					.into(),
				new_path: self.path.into_boxed_path(),
			});
		}

		debug!(
			"{} a new Library <id='{}'> to an already existing location '{}'",
			if self.dry_run {
				"Dry run: Would add"
			} else {
				"Trying to add"
			},
			library.id,
			self.path.display()
		);

		let uuid = Uuid::new_v4();

		let location = create_location(
			library,
			uuid,
			&self.path,
			&self.indexer_rules_ids,
			self.dry_run,
		)
		.await?;

		if let Some(location) = location {
			metadata
				.add_library(library.id, uuid, &self.path, location.name)
				.await?;

			node.locations
				.add(location.data.id, library.clone())
				.await?;

			info!(
				"Added library (library_id = {}) to location: {:?}",
				library.id, &location.data
			);

			Ok(Some(location.data))
		} else {
			Ok(None)
		}
	}
}

/// `LocationUpdateArgs` is the argument received from the client using `rspc` to update a location.
/// It contains the id of the location to be updated, possible a name to change the current location's name
/// and a vector of indexer rules ids to add or remove from the location.
///
/// It is important to note that only the indexer rule ids in this vector will be used from now on.
/// Old rules that aren't in this vector will be purged.
#[derive(Type, Deserialize)]
pub struct LocationUpdateArgs {
	id: location::id::Type,
	name: Option<String>,
	generate_preview_media: Option<bool>,
	sync_preview_media: Option<bool>,
	hidden: Option<bool>,
	indexer_rules_ids: Vec<i32>,
	path: Option<String>,
}

impl LocationUpdateArgs {
	pub async fn update(self, node: &Node, library: &Arc<Library>) -> Result<(), LocationError> {
		let Library { sync, db, .. } = &**library;

		let location = find_location(library, self.id)
			.include(location_with_indexer_rules::include())
			.exec()
			.await?
			.ok_or(LocationError::IdNotFound(self.id))?;

		let name = self.name.clone();

		let (sync_params, db_params): (Vec<_>, Vec<_>) = [
			self.name
				.filter(|name| location.name.as_ref() != Some(name))
				.map(|v| {
					(
						(location::name::NAME, json!(v)),
						location::name::set(Some(v)),
					)
				}),
			self.generate_preview_media.map(|v| {
				(
					(location::generate_preview_media::NAME, json!(v)),
					location::generate_preview_media::set(Some(v)),
				)
			}),
			self.sync_preview_media.map(|v| {
				(
					(location::sync_preview_media::NAME, json!(v)),
					location::sync_preview_media::set(Some(v)),
				)
			}),
			self.hidden.map(|v| {
				(
					(location::hidden::NAME, json!(v)),
					location::hidden::set(Some(v)),
				)
			}),
			self.path.clone().map(|v| {
				(
					(location::path::NAME, json!(v)),
					location::path::set(Some(v)),
				)
			}),
		]
		.into_iter()
		.flatten()
		.unzip();

		if !sync_params.is_empty() {
			sync.write_ops(
				db,
				(
					sync_params
						.into_iter()
						.map(|p| {
							sync.shared_update(
								prisma_sync::location::SyncId {
									pub_id: location.pub_id.clone(),
								},
								p.0,
								p.1,
							)
						})
						.collect(),
					db.location()
						.update(location::id::equals(self.id), db_params),
				),
			)
			.await?;

			// TODO(N): This will probs fall apart with removable media.
			if location.instance_id == Some(library.config().await.instance_id) {
				if let Some(path) = &location.path {
					if let Some(mut metadata) =
						SpacedriveLocationMetadataFile::try_load(path).await?
					{
						metadata
							.update(library.id, maybe_missing(name, "location.name")?)
							.await?;
					}
				}
			}

			if self.path.is_some() {
				node.locations.remove(self.id, library.clone()).await?;
				node.locations.add(self.id, library.clone()).await?;
			}
		}

		let current_rules_ids = location
			.indexer_rules
			.iter()
			.map(|r| r.indexer_rule.id)
			.collect::<HashSet<_>>();

		let new_rules_ids = self.indexer_rules_ids.into_iter().collect::<HashSet<_>>();

		if current_rules_ids != new_rules_ids {
			let rule_ids_to_add = new_rules_ids
				.difference(&current_rules_ids)
				.copied()
				.collect::<Vec<_>>();
			let rule_ids_to_remove = current_rules_ids
				.difference(&new_rules_ids)
				.copied()
				.collect::<Vec<_>>();

			if !rule_ids_to_remove.is_empty() {
				library
					.db
					.indexer_rules_in_location()
					.delete_many(vec![
						indexer_rules_in_location::location_id::equals(self.id),
						indexer_rules_in_location::indexer_rule_id::in_vec(rule_ids_to_remove),
					])
					.exec()
					.await?;
			}

			if !rule_ids_to_add.is_empty() {
				link_location_and_indexer_rules(library, self.id, &rule_ids_to_add).await?;
			}
		}

		Ok(())
	}
}

pub fn find_location(
	library: &Library,
	location_id: location::id::Type,
) -> location::FindUniqueQuery {
	library
		.db
		.location()
		.find_unique(location::id::equals(location_id))
}

async fn link_location_and_indexer_rules(
	library: &Library,
	location_id: location::id::Type,
	rules_ids: &[i32],
) -> Result<(), LocationError> {
	library
		.db
		.indexer_rules_in_location()
		.create_many(
			rules_ids
				.iter()
				.map(|id| indexer_rules_in_location::create_unchecked(location_id, *id, vec![]))
				.collect(),
		)
		.exec()
		.await?;

	Ok(())
}

pub async fn scan_location(
	node: &Arc<Node>,
	library: &Arc<Library>,
	location: location_with_indexer_rules::Data,
) -> Result<(), JobManagerError> {
	// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
	if location.instance_id != Some(library.config().await.instance_id) {
		return Ok(());
	}

	let location_base_data = location::Data::from(&location);

	JobBuilder::new(IndexerJobInit {
		location,
		sub_path: None,
	})
	.with_action("scan_location")
	.with_metadata(json!({"location": location_base_data.clone()}))
	.build()
	.queue_next(FileIdentifierJobInit {
		location: location_base_data.clone(),
		sub_path: None,
	})
	.queue_next(MediaProcessorJobInit {
		location: location_base_data,
		sub_path: None,
		regenerate_thumbnails: false,
	})
	.spawn(node, library)
	.await
	.map_err(Into::into)
}

pub async fn scan_location_sub_path(
	node: &Arc<Node>,
	library: &Arc<Library>,
	location: location_with_indexer_rules::Data,
	sub_path: impl AsRef<Path>,
) -> Result<(), JobManagerError> {
	let sub_path = sub_path.as_ref().to_path_buf();

	// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
	if location.instance_id != Some(library.config().await.instance_id) {
		return Ok(());
	}

	let location_base_data = location::Data::from(&location);

	JobBuilder::new(IndexerJobInit {
		location,
		sub_path: Some(sub_path.clone()),
	})
	.with_action("scan_location_sub_path")
	.with_metadata(json!({
		"location": location_base_data.clone(),
		"sub_path": sub_path.clone(),
	}))
	.build()
	.queue_next(FileIdentifierJobInit {
		location: location_base_data.clone(),
		sub_path: Some(sub_path.clone()),
	})
	.queue_next(MediaProcessorJobInit {
		location: location_base_data,
		sub_path: Some(sub_path),
		regenerate_thumbnails: false,
	})
	.spawn(node, library)
	.await
	.map_err(Into::into)
}

pub async fn light_scan_location(
	node: Arc<Node>,
	library: Arc<Library>,
	location: location_with_indexer_rules::Data,
	sub_path: impl AsRef<Path>,
) -> Result<(), JobError> {
	let sub_path = sub_path.as_ref().to_path_buf();

	// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
	if location.instance_id != Some(library.config().await.instance_id) {
		return Ok(());
	}

	let location_base_data = location::Data::from(&location);

	indexer::shallow(&location, &sub_path, &node, &library).await?;
	file_identifier::shallow(&location_base_data, &sub_path, &library).await?;
	media_processor::shallow(&location_base_data, &sub_path, &library, &node).await?;

	Ok(())
}

pub async fn relink_location(
	Library { db, id, sync, .. }: &Library,
	location_path: impl AsRef<Path>,
) -> Result<i32, LocationError> {
	let location_path = location_path.as_ref();
	let mut metadata = SpacedriveLocationMetadataFile::try_load(&location_path)
		.await?
		.ok_or_else(|| LocationError::MissingMetadataFile(location_path.into()))?;

	metadata.relink(*id, location_path).await?;

	let pub_id = metadata.location_pub_id(*id)?.as_ref().to_vec();
	let path = location_path
		.to_str()
		.map(str::to_string)
		.ok_or_else(|| NonUtf8PathError(location_path.into()))?;

	sync.write_op(
		db,
		sync.shared_update(
			prisma_sync::location::SyncId {
				pub_id: pub_id.clone(),
			},
			location::path::NAME,
			json!(path),
		),
		db.location().update(
			location::pub_id::equals(pub_id.clone()),
			vec![location::path::set(Some(path))],
		),
	)
	.await?;

	let location_id = db
		.location()
		.find_unique(location::pub_id::equals(pub_id))
		.select(location::select!({ id }))
		.exec()
		.await?
		.ok_or_else(|| {
			LocationError::MissingField(MissingFieldError::new("missing id of location"))
		})?;

	Ok(location_id.id)
}

#[derive(Debug)]
pub struct CreatedLocationResult {
	pub name: String,
	pub data: location_with_indexer_rules::Data,
}

pub(crate) fn normalize_path(path: impl AsRef<Path>) -> io::Result<(String, String)> {
	let mut path = path.as_ref().to_path_buf();
	let (location_path, normalized_path) = path
		// Normalize path and also check if it exists
		.normalize()
		.and_then(|normalized_path| {
			if cfg!(windows) {
				// Use normalized path as main path on Windows
				// This ensures we always receive a valid windows formated path
				// ex: /Users/JohnDoe/Downloads will become C:\Users\JohnDoe\Downloads
				// Internally `normalize` calls `GetFullPathNameW` on Windows
				// https://learn.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-getfullpathnamew
				path = normalized_path.as_path().to_path_buf();
			}

			Ok((
				// TODO: Maybe save the path bytes instead of the string representation to avoid depending on UTF-8
				path.to_str().map(str::to_string).ok_or(io::Error::new(
					io::ErrorKind::InvalidInput,
					"Found non-UTF-8 path",
				))?,
				normalized_path,
			))
		})?;

	// Not needed on Windows because the normalization already handles it
	if cfg!(not(windows)) {
		// Replace location_path with normalize_path, when the first one ends in `.` or `..`
		// This is required so localize_name doesn't panic
		if let Some(component) = path.components().next_back() {
			if matches!(component, Component::CurDir | Component::ParentDir) {
				path = normalized_path.as_path().to_path_buf();
			}
		}
	}

	// Use `to_string_lossy` because a partially corrupted but identifiable name is better than nothing
	let mut name = path.localize_name().to_string_lossy().to_string();

	// Windows doesn't have a root directory
	if cfg!(not(windows)) && name == "/" {
		name = "Root".to_string()
	}

	if name.replace(char::REPLACEMENT_CHARACTER, "") == "" {
		name = "Unknown".to_string()
	}

	Ok((location_path, name))
}

async fn create_location(
	library @ Library { db, sync, .. }: &Library,
	location_pub_id: Uuid,
	location_path: impl AsRef<Path>,
	indexer_rules_ids: &[i32],
	dry_run: bool,
) -> Result<Option<CreatedLocationResult>, LocationError> {
	let location_path = location_path.as_ref();
	let (path, name) = normalize_path(location_path)
		.map_err(|_| LocationError::DirectoryNotFound(location_path.into()))?;

	if db
		.location()
		.count(vec![location::path::equals(Some(path.clone()))])
		.exec()
		.await? > 0
	{
		return Err(LocationError::LocationAlreadyExists(location_path.into()));
	}

	if check_nested_location(&location_path, db).await? {
		return Err(LocationError::NestedLocation(location_path.into()));
	}

	if dry_run {
		return Ok(None);
	}

	let date_created = Utc::now();

	let location = sync
		.write_ops(
			db,
			(
				sync.shared_create(
					prisma_sync::location::SyncId {
						pub_id: location_pub_id.as_bytes().to_vec(),
					},
					[
						(location::name::NAME, json!(&name)),
						(location::path::NAME, json!(&path)),
						(location::date_created::NAME, json!(date_created)),
						(
							location::instance::NAME,
							json!(prisma_sync::instance::SyncId {
								pub_id: uuid_to_bytes(sync.instance)
							}),
						),
					],
				),
				db.location()
					.create(
						location_pub_id.as_bytes().to_vec(),
						vec![
							location::name::set(Some(name.clone())),
							location::path::set(Some(path)),
							location::date_created::set(Some(date_created.into())),
							location::instance_id::set(Some(library.config().await.instance_id)),
							// location::instance::connect(instance::id::equals(
							// 	library.config.instance_id.as_bytes().to_vec(),
							// )),
						],
					)
					.include(location_with_indexer_rules::include()),
			),
		)
		.await?;

	debug!("New location created in db");

	if !indexer_rules_ids.is_empty() {
		link_location_and_indexer_rules(library, location.id, indexer_rules_ids).await?;
	}

	// Updating our location variable to include information about the indexer rules
	let location = find_location(library, location.id)
		.include(location_with_indexer_rules::include())
		.exec()
		.await?
		.ok_or(LocationError::IdNotFound(location.id))?;

	invalidate_query!(library, "locations.list");

	Ok(Some(CreatedLocationResult {
		data: location,
		name,
	}))
}

pub async fn delete_location(
	node: &Node,
	library: &Arc<Library>,
	location_id: location::id::Type,
) -> Result<(), LocationError> {
	let start = Instant::now();
	node.locations.remove(location_id, library.clone()).await?;
	debug!(
		"Elapsed time to remove location from node: {:?}",
		start.elapsed()
	);

	let start = Instant::now();
	delete_directory(library, location_id, None).await?;
	debug!(
		"Elapsed time to delete location file paths: {:?}",
		start.elapsed()
	);

	let location = library
		.db
		.location()
		.find_unique(location::id::equals(location_id))
		.exec()
		.await?
		.ok_or(LocationError::IdNotFound(location_id))?;

	let start = Instant::now();
	// TODO: This should really be queued to the proper node so it will always run
	// TODO: Deal with whether a location is online or not
	// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
	if location.instance_id == Some(library.config().await.instance_id) {
		if let Some(path) = &location.path {
			if let Ok(Some(mut metadata)) = SpacedriveLocationMetadataFile::try_load(path).await {
				metadata
					.clean_stale_libraries(
						&node
							.libraries
							.get_all()
							.await
							.into_iter()
							.map(|library| library.id)
							.collect(),
					)
					.await?;

				metadata.remove_library(library.id).await?;
			}
		}
	}
	debug!(
		"Elapsed time to remove location metadata: {:?}",
		start.elapsed()
	);

	let start = Instant::now();

	library
		.db
		.indexer_rules_in_location()
		.delete_many(vec![indexer_rules_in_location::location_id::equals(
			location_id,
		)])
		.exec()
		.await?;
	debug!(
		"Elapsed time to delete indexer rules in location: {:?}",
		start.elapsed()
	);

	let start = Instant::now();

	library
		.db
		.location()
		.delete(location::id::equals(location_id))
		.exec()
		.await?;

	debug!(
		"Elapsed time to delete location from db: {:?}",
		start.elapsed()
	);

	invalidate_query!(library, "locations.list");

	info!("Location {location_id} deleted");

	Ok(())
}

/// Will delete a directory recursively with Objects if left as orphans
/// this function is used to delete a location and when ingesting directory deletion events
pub async fn delete_directory(
	library: &Library,
	location_id: location::id::Type,
	parent_iso_file_path: Option<&IsolatedFilePathData<'_>>,
) -> Result<(), QueryError> {
	let Library { db, .. } = library;

	let children_params = sd_utils::chain_optional_iter(
		[file_path::location_id::equals(Some(location_id))],
		[parent_iso_file_path.and_then(|parent| {
			parent
				.materialized_path_for_children()
				.map(|materialized_path| {
					or![
						and(filter_existing_file_path_params(parent)),
						file_path::materialized_path::starts_with(materialized_path),
					]
				})
		})],
	);

	db.file_path().delete_many(children_params).exec().await?;

	library.orphan_remover.invoke().await;

	invalidate_query!(library, "search.paths");
	invalidate_query!(library, "search.objects");

	Ok(())
}

impl From<location_with_indexer_rules::Data> for location::Data {
	fn from(data: location_with_indexer_rules::Data) -> Self {
		Self {
			id: data.id,
			pub_id: data.pub_id,
			path: data.path,
			instance_id: data.instance_id,
			name: data.name,
			total_capacity: data.total_capacity,
			available_capacity: data.available_capacity,
			is_archived: data.is_archived,
			size_in_bytes: data.size_in_bytes,
			generate_preview_media: data.generate_preview_media,
			sync_preview_media: data.sync_preview_media,
			hidden: data.hidden,
			date_created: data.date_created,
			file_paths: None,
			indexer_rules: None,
			instance: None,
		}
	}
}

impl From<&location_with_indexer_rules::Data> for location::Data {
	fn from(data: &location_with_indexer_rules::Data) -> Self {
		Self {
			id: data.id,
			pub_id: data.pub_id.clone(),
			path: data.path.clone(),
			instance_id: data.instance_id,
			name: data.name.clone(),
			total_capacity: data.total_capacity,
			available_capacity: data.available_capacity,
			size_in_bytes: data.size_in_bytes.clone(),
			is_archived: data.is_archived,
			generate_preview_media: data.generate_preview_media,
			sync_preview_media: data.sync_preview_media,
			hidden: data.hidden,
			date_created: data.date_created,
			file_paths: None,
			indexer_rules: None,
			instance: None,
		}
	}
}

async fn check_nested_location(
	location_path: impl AsRef<Path>,
	db: &PrismaClient,
) -> Result<bool, QueryError> {
	let location_path = location_path.as_ref();

	let (parents_count, potential_children) = db
		._batch((
			db.location().count(vec![location::path::in_vec(
				location_path
					.ancestors()
					.skip(1) // skip the actual location_path, we only want the parents
					.map(|p| {
						p.to_str()
							.map(str::to_string)
							.expect("Found non-UTF-8 path")
					})
					.collect(),
			)]),
			db.location().find_many(vec![location::path::starts_with(
				location_path
					.to_str()
					.map(str::to_string)
					.expect("Found non-UTF-8 path"),
			)]),
		))
		.await?;

	let comps = location_path.components().collect::<Vec<_>>();
	let is_a_child_location = potential_children.into_iter().any(|v| {
		let Some(location_path) = v.path else {
			warn!(
				"Missing location path on location <id='{}'> at check nested location",
				v.id
			);
			return false;
		};
		let comps2 = PathBuf::from(location_path);
		let comps2 = comps2.components().collect::<Vec<_>>();

		if comps.len() > comps2.len() {
			return false;
		}

		for (a, b) in comps.iter().zip(comps2.iter()) {
			if a != b {
				return false;
			}
		}

		true
	});

	Ok(parents_count > 0 || is_a_child_location)
}

pub async fn update_location_size(
	location_id: location::id::Type,
	library: &Library,
) -> Result<(), QueryError> {
	let Library { db, .. } = library;

	let total_size = db
		.file_path()
		.find_many(vec![
			file_path::location_id::equals(Some(location_id)),
			file_path::materialized_path::equals(Some("/".to_string())),
		])
		.select(file_path::select!({ size_in_bytes_bytes }))
		.exec()
		.await?
		.into_iter()
		.filter_map(|file_path| {
			file_path.size_in_bytes_bytes.map(|size_in_bytes_bytes| {
				u64::from_be_bytes([
					size_in_bytes_bytes[0],
					size_in_bytes_bytes[1],
					size_in_bytes_bytes[2],
					size_in_bytes_bytes[3],
					size_in_bytes_bytes[4],
					size_in_bytes_bytes[5],
					size_in_bytes_bytes[6],
					size_in_bytes_bytes[7],
				])
			})
		})
		.sum::<u64>();

	db.location()
		.update(
			location::id::equals(location_id),
			vec![location::size_in_bytes::set(Some(
				total_size.to_be_bytes().to_vec(),
			))],
		)
		.exec()
		.await?;

	invalidate_query!(library, "locations.list");
	invalidate_query!(library, "locations.get");

	Ok(())
}

pub async fn get_location_path_from_location_id(
	db: &PrismaClient,
	location_id: file_path::id::Type,
) -> Result<PathBuf, LocationError> {
	db.location()
		.find_unique(location::id::equals(location_id))
		.exec()
		.await
		.map_err(Into::into)
		.and_then(|maybe_location| {
			maybe_location
				.ok_or(LocationError::IdNotFound(location_id))
				.and_then(|location| {
					location
						.path
						.map(PathBuf::from)
						.ok_or(LocationError::MissingPath(location_id))
				})
		})
}



File: ./src/location/indexer/shallow.rs
-------------------------------------------------
use crate::{
	file_paths_db_fetcher_fn, invalidate_query,
	job::JobError,
	library::Library,
	location::{
		file_path_helper::{
			check_file_path_exists, ensure_sub_path_is_directory, ensure_sub_path_is_in_location,
			IsolatedFilePathData,
		},
		indexer::{
			execute_indexer_update_step, reverse_update_directories_sizes, IndexerJobUpdateStep,
		},
		scan_location_sub_path, update_location_size,
	},
	to_remove_db_fetcher_fn,
	util::db::maybe_missing,
	Node,
};

use std::{
	collections::HashSet,
	path::{Path, PathBuf},
	sync::Arc,
};

use futures::future::join_all;
use itertools::Itertools;
use tracing::{debug, error};

use super::{
	execute_indexer_save_step, iso_file_path_factory, location_with_indexer_rules,
	remove_non_existing_file_paths, rules::IndexerRule, walk::walk_single_dir, IndexerError,
	IndexerJobSaveStep,
};

/// BATCH_SIZE is the number of files to index at each step, writing the chunk of files metadata in the database.
const BATCH_SIZE: usize = 1000;

pub async fn shallow(
	location: &location_with_indexer_rules::Data,
	sub_path: &PathBuf,
	node: &Arc<Node>,
	library: &Arc<Library>,
) -> Result<(), JobError> {
	let location_id = location.id;
	let location_path = maybe_missing(&location.path, "location.path").map(Path::new)?;

	let db = library.db.clone();

	let indexer_rules = location
		.indexer_rules
		.iter()
		.map(|rule| IndexerRule::try_from(&rule.indexer_rule))
		.collect::<Result<Vec<_>, _>>()
		.map_err(IndexerError::from)?;

	let (add_root, to_walk_path) = if sub_path != Path::new("") && sub_path != Path::new("/") {
		let full_path = ensure_sub_path_is_in_location(&location_path, &sub_path)
			.await
			.map_err(IndexerError::from)?;
		ensure_sub_path_is_directory(&location_path, &sub_path)
			.await
			.map_err(IndexerError::from)?;

		(
			!check_file_path_exists::<IndexerError>(
				&IsolatedFilePathData::new(location_id, location_path, &full_path, true)
					.map_err(IndexerError::from)?,
				&db,
			)
			.await?,
			full_path,
		)
	} else {
		(false, location_path.to_path_buf())
	};

	let (walked, to_update, to_remove, errors, _s) = {
		walk_single_dir(
			&to_walk_path,
			&indexer_rules,
			|_, _| {},
			file_paths_db_fetcher_fn!(&db),
			to_remove_db_fetcher_fn!(location_id, &db),
			iso_file_path_factory(location_id, location_path),
			add_root,
		)
		.await?
	};

	let to_remove_count = to_remove.len();

	node.thumbnailer
		.remove_indexed_cas_ids(
			to_remove
				.iter()
				.filter_map(|file_path| file_path.cas_id.clone())
				.collect::<Vec<_>>(),
			library.id,
		)
		.await;

	errors.into_iter().for_each(|e| error!("{e}"));

	// TODO pass these uuids to sync system
	remove_non_existing_file_paths(to_remove, &db).await?;

	let mut new_directories_to_scan = HashSet::new();

	let mut to_create_count = 0;

	let save_steps = walked
		.chunks(BATCH_SIZE)
		.into_iter()
		.enumerate()
		.map(|(i, chunk)| {
			let walked = chunk.collect::<Vec<_>>();
			to_create_count += walked.len();

			walked
				.iter()
				.filter_map(|walked_entry| {
					walked_entry.iso_file_path.materialized_path_for_children()
				})
				.for_each(|new_dir| {
					new_directories_to_scan.insert(new_dir);
				});

			IndexerJobSaveStep {
				chunk_idx: i,
				walked,
			}
		})
		.collect::<Vec<_>>();

	for step in save_steps {
		execute_indexer_save_step(location, &step, library).await?;
	}

	for scan in join_all(
		new_directories_to_scan
			.into_iter()
			.map(|sub_path| scan_location_sub_path(node, library, location.clone(), sub_path)),
	)
	.await
	{
		if let Err(e) = scan {
			error!("{e}");
		}
	}

	let mut to_update_count = 0;

	let update_steps = to_update
		.chunks(BATCH_SIZE)
		.into_iter()
		.enumerate()
		.map(|(i, chunk)| {
			let to_update = chunk.collect::<Vec<_>>();
			to_update_count += to_update.len();

			IndexerJobUpdateStep {
				chunk_idx: i,
				to_update,
			}
		})
		.collect::<Vec<_>>();

	for step in update_steps {
		execute_indexer_update_step(&step, library).await?;
	}

	debug!(
		"Walker at shallow indexer found: \
		To create: {to_create_count}; To update: {to_update_count}; To remove: {to_remove_count};"
	);

	if to_create_count > 0 || to_update_count > 0 || to_remove_count > 0 {
		if to_walk_path != location_path {
			reverse_update_directories_sizes(to_walk_path, location_id, location_path, library)
				.await
				.map_err(IndexerError::from)?;
		}

		update_location_size(location.id, library)
			.await
			.map_err(IndexerError::from)?;

		invalidate_query!(library, "search.paths");
		invalidate_query!(library, "search.objects");
	}

	library.orphan_remover.invoke().await;

	Ok(())
}



File: ./src/location/indexer/indexer_job.rs
-------------------------------------------------
use crate::{
	file_paths_db_fetcher_fn, invalidate_query,
	job::{
		CurrentStep, JobError, JobInitOutput, JobReportUpdate, JobResult, JobRunMetadata,
		JobStepOutput, StatefulJob, WorkerContext,
	},
	library::Library,
	location::{
		file_path_helper::{
			ensure_file_path_exists, ensure_sub_path_is_directory, ensure_sub_path_is_in_location,
			IsolatedFilePathData,
		},
		location_with_indexer_rules, update_location_size,
	},
	prisma::{file_path, location},
	to_remove_db_fetcher_fn,
	util::db::maybe_missing,
};

use sd_prisma::prisma_sync;
use sd_sync::*;
use sd_utils::from_bytes_to_uuid;

use std::{
	collections::HashMap,
	hash::{Hash, Hasher},
	path::{Path, PathBuf},
	sync::Arc,
	time::Duration,
};

use itertools::Itertools;
use prisma_client_rust::operator::or;
use serde::{Deserialize, Serialize};
use serde_json::json;
use tokio::time::Instant;
use tracing::{debug, info, warn};

use super::{
	execute_indexer_save_step, execute_indexer_update_step, iso_file_path_factory,
	remove_non_existing_file_paths, reverse_update_directories_sizes,
	rules::IndexerRule,
	walk::{keep_walking, walk, ToWalkEntry, WalkResult},
	IndexerError, IndexerJobSaveStep, IndexerJobUpdateStep,
};

/// BATCH_SIZE is the number of files to index at each step, writing the chunk of files metadata in the database.
const BATCH_SIZE: usize = 1000;

/// `IndexerJobInit` receives a `location::Data` object to be indexed
/// and possibly a `sub_path` to be indexed. The `sub_path` is used when
/// we want do index just a part of a location.
#[derive(Serialize, Deserialize, Debug)]
pub struct IndexerJobInit {
	pub location: location_with_indexer_rules::Data,
	pub sub_path: Option<PathBuf>,
}

impl Hash for IndexerJobInit {
	fn hash<H: Hasher>(&self, state: &mut H) {
		self.location.id.hash(state);
		if let Some(ref sub_path) = self.sub_path {
			sub_path.hash(state);
		}
	}
}
/// `IndexerJobData` contains the state of the indexer job, which includes a `location_path` that
/// is cached and casted on `PathBuf` from `local_path` column in the `location` table. It also
/// contains some metadata for logging purposes.
#[derive(Serialize, Deserialize, Debug)]
pub struct IndexerJobData {
	location_path: PathBuf,
	indexed_path: PathBuf,
	indexer_rules: Vec<IndexerRule>,
}

#[derive(Serialize, Deserialize, Default, Debug)]
pub struct IndexerJobRunMetadata {
	db_write_time: Duration,
	scan_read_time: Duration,
	total_paths: u64,
	total_updated_paths: u64,
	total_save_steps: u64,
	total_update_steps: u64,
	indexed_count: u64,
	updated_count: u64,
	removed_count: u64,
	paths_and_sizes: HashMap<PathBuf, u64>,
}

impl JobRunMetadata for IndexerJobRunMetadata {
	fn update(&mut self, new_data: Self) {
		self.db_write_time += new_data.db_write_time;
		self.scan_read_time += new_data.scan_read_time;
		self.total_paths += new_data.total_paths;
		self.total_updated_paths += new_data.total_updated_paths;
		self.total_save_steps += new_data.total_save_steps;
		self.total_update_steps += new_data.total_update_steps;
		self.indexed_count += new_data.indexed_count;
		self.removed_count += new_data.removed_count;

		for (path, size) in new_data.paths_and_sizes {
			*self.paths_and_sizes.entry(path).or_default() += size;
		}
	}
}

#[derive(Clone)]
pub enum ScanProgress {
	ChunkCount(usize),
	SavedChunks(usize),
	UpdatedChunks(usize),
	Message(String),
}

impl IndexerJobData {
	fn on_scan_progress(ctx: &WorkerContext, progress: Vec<ScanProgress>) {
		ctx.progress(
			progress
				.into_iter()
				.map(|p| match p {
					ScanProgress::ChunkCount(c) => JobReportUpdate::TaskCount(c),
					ScanProgress::SavedChunks(p) | ScanProgress::UpdatedChunks(p) => {
						JobReportUpdate::CompletedTaskCount(p)
					}
					ScanProgress::Message(m) => JobReportUpdate::Message(m),
				})
				.collect(),
		)
	}
}

/// `IndexerJobStepInput` defines the action that should be executed in the current step
#[derive(Serialize, Deserialize, Debug)]
pub enum IndexerJobStepInput {
	Save(IndexerJobSaveStep),
	Walk(ToWalkEntry),
	Update(IndexerJobUpdateStep),
}

/// A `IndexerJob` is a stateful job that walks a directory and indexes all files.
/// First it walks the directory and generates a list of files to index, chunked into
/// batches of [`BATCH_SIZE`]. Then for each chunk it write the file metadata to the database.
#[async_trait::async_trait]
impl StatefulJob for IndexerJobInit {
	type Data = IndexerJobData;
	type Step = IndexerJobStepInput;
	type RunMetadata = IndexerJobRunMetadata;

	const NAME: &'static str = "indexer";
	const IS_BATCHED: bool = true;

	fn target_location(&self) -> location::id::Type {
		self.location.id
	}

	/// Creates a vector of valid path buffers from a directory, chunked into batches of `BATCH_SIZE`.
	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let init = self;
		let location_id = init.location.id;
		let location_path = maybe_missing(&init.location.path, "location.path").map(Path::new)?;

		let db = Arc::clone(&ctx.library.db);

		let indexer_rules = init
			.location
			.indexer_rules
			.iter()
			.map(|rule| IndexerRule::try_from(&rule.indexer_rule))
			.collect::<Result<Vec<_>, _>>()
			.map_err(IndexerError::from)?;

		let to_walk_path = match &init.sub_path {
			Some(sub_path) if sub_path != Path::new("") => {
				let full_path = ensure_sub_path_is_in_location(location_path, sub_path)
					.await
					.map_err(IndexerError::from)?;
				ensure_sub_path_is_directory(location_path, sub_path)
					.await
					.map_err(IndexerError::from)?;

				ensure_file_path_exists(
					sub_path,
					&IsolatedFilePathData::new(location_id, location_path, &full_path, true)
						.map_err(IndexerError::from)?,
					&db,
					IndexerError::SubPathNotFound,
				)
				.await?;

				full_path
			}
			_ => location_path.to_path_buf(),
		};

		let scan_start = Instant::now();
		let WalkResult {
			walked,
			to_update,
			to_walk,
			to_remove,
			errors,
			paths_and_sizes,
		} = walk(
			&to_walk_path,
			&indexer_rules,
			update_notifier_fn(ctx),
			file_paths_db_fetcher_fn!(&db),
			to_remove_db_fetcher_fn!(location_id, &db),
			iso_file_path_factory(location_id, location_path),
			50_000,
		)
		.await?;
		let scan_read_time = scan_start.elapsed();
		let to_remove = to_remove.collect::<Vec<_>>();

		debug!(
			"Walker at indexer job found {} file_paths to be removed",
			to_remove.len()
		);

		ctx.node
			.thumbnailer
			.remove_indexed_cas_ids(
				to_remove
					.iter()
					.filter_map(|file_path| file_path.cas_id.clone())
					.collect::<Vec<_>>(),
				ctx.library.id,
			)
			.await;

		let db_delete_start = Instant::now();
		// TODO pass these uuids to sync system
		let removed_count = remove_non_existing_file_paths(to_remove, &db).await?;
		let db_delete_time = db_delete_start.elapsed();

		let total_new_paths = &mut 0;
		let total_updated_paths = &mut 0;
		let to_walk_count = to_walk.len();
		let to_save_chunks = &mut 0;
		let to_update_chunks = &mut 0;

		let steps = walked
			.chunks(BATCH_SIZE)
			.into_iter()
			.enumerate()
			.map(|(i, chunk)| {
				let chunk_steps = chunk.collect::<Vec<_>>();

				*total_new_paths += chunk_steps.len() as u64;
				*to_save_chunks += 1;

				IndexerJobStepInput::Save(IndexerJobSaveStep {
					chunk_idx: i,
					walked: chunk_steps,
				})
			})
			.chain(
				to_update
					.chunks(BATCH_SIZE)
					.into_iter()
					.enumerate()
					.map(|(i, chunk)| {
						let chunk_updates = chunk.collect::<Vec<_>>();

						*total_updated_paths += chunk_updates.len() as u64;
						*to_update_chunks += 1;

						IndexerJobStepInput::Update(IndexerJobUpdateStep {
							chunk_idx: i,
							to_update: chunk_updates,
						})
					}),
			)
			.chain(to_walk.into_iter().map(IndexerJobStepInput::Walk))
			.collect::<Vec<_>>();

		debug!("Walker at indexer job found {total_updated_paths} file_paths to be updated");

		IndexerJobData::on_scan_progress(
			ctx,
			vec![
				ScanProgress::ChunkCount(*to_save_chunks + *to_update_chunks),
				ScanProgress::Message(format!(
					"Starting saving {total_new_paths} files or directories, \
					{total_updated_paths} files or directories to update, \
					there still {to_walk_count} directories to index",
				)),
			],
		);

		*data = Some(IndexerJobData {
			location_path: location_path.to_path_buf(),
			indexed_path: to_walk_path,
			indexer_rules,
		});

		Ok((
			IndexerJobRunMetadata {
				db_write_time: db_delete_time,
				scan_read_time,
				total_paths: *total_new_paths,
				total_updated_paths: *total_updated_paths,
				indexed_count: 0,
				updated_count: 0,
				removed_count,
				total_save_steps: *to_save_chunks as u64,
				total_update_steps: *to_update_chunks as u64,
				paths_and_sizes,
			},
			steps,
			errors
				.into_iter()
				.map(|e| format!("{e}"))
				.collect::<Vec<_>>()
				.into(),
		)
			.into())
	}

	/// Process each chunk of entries in the indexer job, writing to the `file_path` table
	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		CurrentStep { step, .. }: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		run_metadata: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		let init = self;
		let mut new_metadata = Self::RunMetadata::default();
		match step {
			IndexerJobStepInput::Save(step) => {
				let start_time = Instant::now();

				IndexerJobData::on_scan_progress(
					ctx,
					vec![
						ScanProgress::SavedChunks(step.chunk_idx + 1),
						ScanProgress::Message(format!(
							"Writing chunk {} of {} to database",
							step.chunk_idx, run_metadata.total_save_steps
						)),
					],
				);

				let count = execute_indexer_save_step(&init.location, step, &ctx.library).await?;

				new_metadata.indexed_count = count as u64;
				new_metadata.db_write_time = start_time.elapsed();

				Ok(new_metadata.into())
			}
			IndexerJobStepInput::Update(to_update) => {
				let start_time = Instant::now();
				IndexerJobData::on_scan_progress(
					ctx,
					vec![
						ScanProgress::UpdatedChunks(to_update.chunk_idx + 1),
						ScanProgress::Message(format!(
							"Updating chunk {} of {} to database",
							to_update.chunk_idx, run_metadata.total_save_steps
						)),
					],
				);

				let count = execute_indexer_update_step(to_update, &ctx.library).await?;

				new_metadata.updated_count = count as u64;
				new_metadata.db_write_time = start_time.elapsed();

				Ok(new_metadata.into())
			}

			IndexerJobStepInput::Walk(to_walk_entry) => {
				let location_id = init.location.id;
				let location_path =
					maybe_missing(&init.location.path, "location.path").map(Path::new)?;

				let db = Arc::clone(&ctx.library.db);

				let scan_start = Instant::now();

				let WalkResult {
					walked,
					to_update,
					to_walk,
					to_remove,
					errors,
					paths_and_sizes,
				} = keep_walking(
					to_walk_entry,
					&data.indexer_rules,
					update_notifier_fn(ctx),
					file_paths_db_fetcher_fn!(&db),
					to_remove_db_fetcher_fn!(location_id, &db),
					iso_file_path_factory(location_id, location_path),
				)
				.await?;

				new_metadata.paths_and_sizes = paths_and_sizes;

				new_metadata.scan_read_time = scan_start.elapsed();

				let db_delete_time = Instant::now();
				// TODO pass these uuids to sync system
				new_metadata.removed_count = remove_non_existing_file_paths(to_remove, &db).await?;
				new_metadata.db_write_time = db_delete_time.elapsed();

				let to_walk_count = to_walk.len();

				let more_steps = walked
					.chunks(BATCH_SIZE)
					.into_iter()
					.enumerate()
					.map(|(i, chunk)| {
						let chunk_steps = chunk.collect::<Vec<_>>();
						new_metadata.total_paths += chunk_steps.len() as u64;
						new_metadata.total_save_steps += 1;

						IndexerJobStepInput::Save(IndexerJobSaveStep {
							chunk_idx: i,
							walked: chunk_steps,
						})
					})
					.chain(to_update.chunks(BATCH_SIZE).into_iter().enumerate().map(
						|(i, chunk)| {
							let chunk_updates = chunk.collect::<Vec<_>>();
							new_metadata.total_updated_paths += chunk_updates.len() as u64;
							new_metadata.total_update_steps += 1;

							IndexerJobStepInput::Update(IndexerJobUpdateStep {
								chunk_idx: i,
								to_update: chunk_updates,
							})
						},
					))
					.chain(to_walk.into_iter().map(IndexerJobStepInput::Walk))
					.collect::<Vec<_>>();

				IndexerJobData::on_scan_progress(
					ctx,
					vec![
						ScanProgress::ChunkCount(more_steps.len() - to_walk_count),
						ScanProgress::Message(format!(
							"Scanned more {} files or directories; \
							{} more directories to scan and more {} entries to update",
							new_metadata.total_paths,
							to_walk_count,
							new_metadata.total_updated_paths
						)),
					],
				);

				Ok((
					more_steps,
					new_metadata,
					errors
						.into_iter()
						.map(|e| format!("{e}"))
						.collect::<Vec<_>>()
						.into(),
				)
					.into())
			}
		}
	}

	async fn finalize(
		&self,
		ctx: &WorkerContext,
		data: &Option<Self::Data>,
		run_metadata: &Self::RunMetadata,
	) -> JobResult {
		let init = self;
		let indexed_path_str = data
			.as_ref()
			.map(|data| Ok(data.indexed_path.to_string_lossy().to_string()))
			.unwrap_or_else(|| maybe_missing(&init.location.path, "location.path").cloned())?;

		info!(
			"Scan of {indexed_path_str} completed in {:?}. {} new files found, \
			indexed {} files in db, updated {} entries. db write completed in {:?}",
			run_metadata.scan_read_time,
			run_metadata.total_paths,
			run_metadata.indexed_count,
			run_metadata.total_updated_paths,
			run_metadata.db_write_time,
		);

		if run_metadata.indexed_count > 0 || run_metadata.removed_count > 0 {
			invalidate_query!(ctx.library, "search.paths");
		}

		if run_metadata.total_updated_paths > 0 {
			// Invoking orphan remover here as we probably have some orphans objects due to updates
			ctx.library.orphan_remover.invoke().await;
		}

		if run_metadata.indexed_count > 0
			|| run_metadata.removed_count > 0
			|| run_metadata.updated_count > 0
		{
			if let Some(data) = data {
				update_directories_sizes(
					&run_metadata.paths_and_sizes,
					init.location.id,
					&data.indexed_path,
					&ctx.library,
				)
				.await?;

				if data.indexed_path != data.location_path {
					reverse_update_directories_sizes(
						&data.indexed_path,
						init.location.id,
						&data.location_path,
						&ctx.library,
					)
					.await
					.map_err(IndexerError::from)?;
				}

				update_location_size(init.location.id, &ctx.library)
					.await
					.map_err(IndexerError::from)?;
			}
		}

		Ok(Some(json!({"init: ": init, "run_metadata": run_metadata})))
	}
}

fn update_notifier_fn(ctx: &WorkerContext) -> impl FnMut(&Path, usize) + '_ {
	move |path, total_entries| {
		IndexerJobData::on_scan_progress(
			ctx,
			vec![ScanProgress::Message(format!(
				"Found: {total_entries} entries; Scanning: {:?}",
				path.file_name().unwrap_or(path.as_os_str())
			))],
		);
	}
}

async fn update_directories_sizes(
	paths_and_sizes: &HashMap<PathBuf, u64>,
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	library: &Library,
) -> Result<(), IndexerError> {
	let location_path = location_path.as_ref();

	let Library { db, sync, .. } = library;

	let chunked_queries = paths_and_sizes
		.keys()
		.chunks(200)
		.into_iter()
		.map(|paths_chunk| {
			paths_chunk
				.into_iter()
				.map(|path| {
					IsolatedFilePathData::new(location_id, location_path, path, true)
						.map(file_path::WhereParam::from)
				})
				.collect::<Result<Vec<_>, _>>()
				.map(|params| {
					db.file_path()
						.find_many(vec![or(params)])
						.select(file_path::select!({ pub_id materialized_path name }))
				})
		})
		.collect::<Result<Vec<_>, _>>()?;

	let to_sync_and_update = db
		._batch(chunked_queries)
		.await?
		.into_iter()
		.flatten()
		.filter_map(
			|file_path| match (file_path.materialized_path, file_path.name) {
				(Some(materialized_path), Some(name)) => {
					let mut directory_full_path = location_path.join(&materialized_path[1..]);
					directory_full_path.push(name);

					if let Some(size) = paths_and_sizes.get(&directory_full_path) {
						let size_bytes = size.to_be_bytes().to_vec();

						Some((
							sync.shared_update(
								prisma_sync::file_path::SyncId {
									pub_id: file_path.pub_id.clone(),
								},
								file_path::size_in_bytes_bytes::NAME,
								json!(size_bytes.clone()),
							),
							db.file_path().update(
								file_path::pub_id::equals(file_path.pub_id),
								vec![file_path::size_in_bytes_bytes::set(Some(size_bytes))],
							),
						))
					} else {
						warn!("Found a file_path without ancestor in the database, possible corruption");
						None
					}
				}
				_ => {
					warn!(
						"Found a file_path missing its materialized_path or name: <pub_id='{:#?}'>",
						from_bytes_to_uuid(&file_path.pub_id)
					);
					None
				}
			},
		)
		.unzip::<_, _, Vec<_>, Vec<_>>();

	sync.write_ops(db, to_sync_and_update).await?;

	Ok(())
}



File: ./src/location/indexer/mod.rs
-------------------------------------------------
use crate::{
	library::Library,
	util::{db::inode_to_db, error::FileIOError},
};

use sd_prisma::{
	prisma::{file_path, location, object as prisma_object, PrismaClient},
	prisma_sync,
};
use sd_sync::*;
use sd_utils::from_bytes_to_uuid;

use std::{collections::HashMap, path::Path};

use chrono::Utc;
use futures_concurrency::future::TryJoin;
use itertools::Itertools;
use prisma_client_rust::operator::or;
use rspc::ErrorCode;
use serde::{Deserialize, Serialize};
use serde_json::json;
use thiserror::Error;
use tracing::{trace, warn};

use super::{
	file_path_helper::{file_path_pub_and_cas_ids, FilePathError, IsolatedFilePathData},
	location_with_indexer_rules,
};

pub mod indexer_job;
pub mod rules;
mod shallow;
mod walk;

use rules::IndexerRuleError;
use walk::WalkedEntry;

pub use indexer_job::IndexerJobInit;
pub use shallow::*;

#[derive(Serialize, Deserialize, Debug)]
pub struct IndexerJobSaveStep {
	chunk_idx: usize,
	walked: Vec<WalkedEntry>,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct IndexerJobUpdateStep {
	chunk_idx: usize,
	to_update: Vec<WalkedEntry>,
}

/// Error type for the indexer module
#[derive(Error, Debug)]
pub enum IndexerError {
	// Not Found errors
	#[error("indexer rule not found: <id='{0}'>")]
	IndexerRuleNotFound(i32),
	#[error("received sub path not in database: <path='{}'>", .0.display())]
	SubPathNotFound(Box<Path>),

	// Internal Errors
	#[error("Database Error: {}", .0.to_string())]
	Database(#[from] prisma_client_rust::QueryError),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	FilePath(#[from] FilePathError),

	// Mixed errors
	#[error(transparent)]
	IndexerRules(#[from] IndexerRuleError),
}

impl From<IndexerError> for rspc::Error {
	fn from(err: IndexerError) -> Self {
		match err {
			IndexerError::IndexerRuleNotFound(_) | IndexerError::SubPathNotFound(_) => {
				rspc::Error::with_cause(ErrorCode::NotFound, err.to_string(), err)
			}

			IndexerError::IndexerRules(rule_err) => rule_err.into(),

			_ => rspc::Error::with_cause(ErrorCode::InternalServerError, err.to_string(), err),
		}
	}
}

async fn execute_indexer_save_step(
	location: &location_with_indexer_rules::Data,
	save_step: &IndexerJobSaveStep,
	library: &Library,
) -> Result<i64, IndexerError> {
	let Library { sync, db, .. } = library;

	let (sync_stuff, paths): (Vec<_>, Vec<_>) = save_step
		.walked
		.iter()
		.map(|entry| {
			let IsolatedFilePathData {
				materialized_path,
				is_dir,
				name,
				extension,
				..
			} = &entry.iso_file_path;

			use file_path::*;

			let pub_id = sd_utils::uuid_to_bytes(entry.pub_id);

			let (sync_params, db_params): (Vec<_>, Vec<_>) = [
				(
					(
						location::NAME,
						json!(prisma_sync::location::SyncId {
							pub_id: location.pub_id.clone()
						}),
					),
					location_id::set(Some(location.id)),
				),
				(
					(materialized_path::NAME, json!(materialized_path)),
					materialized_path::set(Some(materialized_path.to_string())),
				),
				((name::NAME, json!(name)), name::set(Some(name.to_string()))),
				((is_dir::NAME, json!(*is_dir)), is_dir::set(Some(*is_dir))),
				(
					(extension::NAME, json!(extension)),
					extension::set(Some(extension.to_string())),
				),
				(
					(
						size_in_bytes_bytes::NAME,
						json!(entry.metadata.size_in_bytes.to_be_bytes().to_vec()),
					),
					size_in_bytes_bytes::set(Some(
						entry.metadata.size_in_bytes.to_be_bytes().to_vec(),
					)),
				),
				(
					(inode::NAME, json!(entry.metadata.inode.to_le_bytes())),
					inode::set(Some(inode_to_db(entry.metadata.inode))),
				),
				(
					(date_created::NAME, json!(entry.metadata.created_at)),
					date_created::set(Some(entry.metadata.created_at.into())),
				),
				(
					(date_modified::NAME, json!(entry.metadata.modified_at)),
					date_modified::set(Some(entry.metadata.modified_at.into())),
				),
				(
					(date_indexed::NAME, json!(Utc::now())),
					date_indexed::set(Some(Utc::now().into())),
				),
				(
					(hidden::NAME, json!(entry.metadata.hidden)),
					hidden::set(Some(entry.metadata.hidden)),
				),
			]
			.into_iter()
			.unzip();

			(
				sync.shared_create(
					prisma_sync::file_path::SyncId {
						pub_id: sd_utils::uuid_to_bytes(entry.pub_id),
					},
					sync_params,
				),
				file_path::create_unchecked(pub_id, db_params),
			)
		})
		.unzip();

	let count = sync
		.write_ops(
			db,
			(
				sync_stuff.into_iter().flatten().collect(),
				db.file_path().create_many(paths).skip_duplicates(),
			),
		)
		.await?;

	trace!("Inserted {count} records");

	Ok(count)
}

async fn execute_indexer_update_step(
	update_step: &IndexerJobUpdateStep,
	Library { sync, db, .. }: &Library,
) -> Result<i64, IndexerError> {
	let (sync_stuff, paths_to_update): (Vec<_>, Vec<_>) = update_step
		.to_update
		.iter()
		.map(|entry| async move {
			let IsolatedFilePathData { is_dir, .. } = &entry.iso_file_path;

			let pub_id = sd_utils::uuid_to_bytes(entry.pub_id);

			let should_unlink_object = if let Some(object_id) = entry.maybe_object_id {
				db.object()
					.count(vec![prisma_object::id::equals(object_id)])
					.exec()
					.await? > 1
			} else {
				false
			};

			use file_path::*;

			let (sync_params, db_params): (Vec<_>, Vec<_>) = [
				// As this file was updated while Spacedrive was offline, we mark the object_id and cas_id as null
				// So this file_path will be updated at file identifier job
				(
					(object_id::NAME, serde_json::Value::Null),
					should_unlink_object.then_some(object::disconnect()),
				),
				(
					(cas_id::NAME, serde_json::Value::Null),
					Some(cas_id::set(None)),
				),
				(
					(is_dir::NAME, json!(*is_dir)),
					Some(is_dir::set(Some(*is_dir))),
				),
				(
					(
						size_in_bytes_bytes::NAME,
						json!(entry.metadata.size_in_bytes.to_be_bytes().to_vec()),
					),
					Some(size_in_bytes_bytes::set(Some(
						entry.metadata.size_in_bytes.to_be_bytes().to_vec(),
					))),
				),
				(
					(inode::NAME, json!(entry.metadata.inode.to_le_bytes())),
					Some(inode::set(Some(inode_to_db(entry.metadata.inode)))),
				),
				(
					(date_created::NAME, json!(entry.metadata.created_at)),
					Some(date_created::set(Some(entry.metadata.created_at.into()))),
				),
				(
					(date_modified::NAME, json!(entry.metadata.modified_at)),
					Some(date_modified::set(Some(entry.metadata.modified_at.into()))),
				),
				(
					(hidden::NAME, json!(entry.metadata.hidden)),
					Some(hidden::set(Some(entry.metadata.hidden))),
				),
			]
			.into_iter()
			.filter_map(|(sync_param, maybe_db_param)| {
				maybe_db_param.map(|db_param| (sync_param, db_param))
			})
			.unzip();

			Ok::<_, IndexerError>((
				sync_params
					.into_iter()
					.map(|(field, value)| {
						sync.shared_update(
							prisma_sync::file_path::SyncId {
								pub_id: pub_id.clone(),
							},
							field,
							value,
						)
					})
					.collect::<Vec<_>>(),
				db.file_path()
					.update(file_path::pub_id::equals(pub_id), db_params)
					.select(file_path::select!({ id })),
			))
		})
		.collect::<Vec<_>>()
		.try_join()
		.await?
		.into_iter()
		.unzip();

	let updated = sync
		.write_ops(
			db,
			(sync_stuff.into_iter().flatten().collect(), paths_to_update),
		)
		.await?;

	trace!("Updated {updated:?} records");

	Ok(updated.len() as i64)
}

fn iso_file_path_factory(
	location_id: location::id::Type,
	location_path: &Path,
) -> impl Fn(&Path, bool) -> Result<IsolatedFilePathData<'static>, IndexerError> + '_ {
	move |path, is_dir| {
		IsolatedFilePathData::new(location_id, location_path, path, is_dir).map_err(Into::into)
	}
}

async fn remove_non_existing_file_paths(
	to_remove: impl IntoIterator<Item = file_path_pub_and_cas_ids::Data>,
	db: &PrismaClient,
) -> Result<u64, IndexerError> {
	db.file_path()
		.delete_many(vec![file_path::pub_id::in_vec(
			to_remove.into_iter().map(|data| data.pub_id).collect(),
		)])
		.exec()
		.await
		.map(|count| count as u64)
		.map_err(Into::into)
}

// TODO: Change this macro to a fn when we're able to return
// `impl Fn(Vec<file_path::WhereParam>) -> impl Future<Output = Result<Vec<file_path_walker::Data>, IndexerError>>`
// Maybe when TAITs arrive
#[macro_export]
macro_rules! file_paths_db_fetcher_fn {
	($db:expr) => {{
		|found_paths| async {
			// Each found path is a AND with 4 terms, and SQLite has a expression tree limit of 1000 terms
			// so we will use chunks of 200 just to be safe

			// FIXME: Can't pass this chunks variable direct to _batch because of lifetime issues
			let chunks = found_paths
				.into_iter()
				.chunks(200)
				.into_iter()
				.map(|founds| {
					$db.file_path()
						.find_many(vec![::prisma_client_rust::operator::or(
							founds.collect::<Vec<_>>(),
						)])
						.select($crate::location::file_path_helper::file_path_walker::select())
				})
				.collect::<Vec<_>>();

			$db._batch(chunks)
				.await
				.map(|fetched| fetched.into_iter().flatten().collect::<Vec<_>>())
				.map_err(Into::into)
		}
	}};
}

// TODO: Change this macro to a fn when we're able to return
// `impl Fn(&Path, Vec<file_path::WhereParam>) -> impl Future<Output = Result<Vec<file_path_just_pub_id::Data>, IndexerError>>`
// Maybe when TAITs arrive
// FIXME: (fogodev) I was receiving this error here https://github.com/rust-lang/rust/issues/74497
#[macro_export]
macro_rules! to_remove_db_fetcher_fn {
	($location_id:expr, $db:expr) => {{
		|parent_iso_file_path, unique_location_id_materialized_path_name_extension_params| async {
			let location_id: $crate::prisma::location::id::Type = $location_id;
			let db: &$crate::prisma::PrismaClient = $db;
			let parent_iso_file_path: $crate::location::file_path_helper::IsolatedFilePathData<
				'static,
			> = parent_iso_file_path;
			let unique_location_id_materialized_path_name_extension_params: ::std::vec::Vec<
				$crate::prisma::file_path::WhereParam,
			> = unique_location_id_materialized_path_name_extension_params;

			// FIXME: Can't pass this chunks variable direct to _batch because of lifetime issues
			let chunks = unique_location_id_materialized_path_name_extension_params
				.into_iter()
				.chunks(200)
				.into_iter()
				.map(|unique_params| {
					db.file_path()
						.find_many(vec![::prisma_client_rust::operator::or(
							unique_params.collect(),
						)])
						.select($crate::prisma::file_path::select!({ id }))
				})
				.collect::<::std::vec::Vec<_>>();

			let founds_ids = db._batch(chunks).await.map(|founds_chunk| {
				founds_chunk
					.into_iter()
					.map(|file_paths| file_paths.into_iter().map(|file_path| file_path.id))
					.flatten()
					.collect::<::std::collections::HashSet<_>>()
			})?;

			// NOTE: This batch size can be increased if we wish to trade memory for more performance
			const BATCH_SIZE: i64 = 1000;

			let mut to_remove = vec![];
			let mut cursor = 1;

			loop {
				let found = $db.file_path()
					.find_many(vec![
						$crate::prisma::file_path::location_id::equals(Some(location_id)),
						$crate::prisma::file_path::materialized_path::equals(Some(
							parent_iso_file_path
								.materialized_path_for_children()
								.expect("the received isolated file path must be from a directory"),
						)),
					])
					.order_by($crate::prisma::file_path::id::order($crate::prisma::SortOrder::Asc))
					.take(BATCH_SIZE)
					.cursor($crate::prisma::file_path::id::equals(cursor))
					.select($crate::prisma::file_path::select!({ id pub_id cas_id }))
					.exec()
					.await?;

				let should_stop = (found.len() as i64) < BATCH_SIZE;

				if let Some(last) = found.last() {
					cursor = last.id;
				} else {
					break;
				}

				to_remove.extend(
					found
						.into_iter()
						.filter(|file_path| !founds_ids.contains(&file_path.id))
						.map(|file_path| $crate::location::file_path_helper::file_path_pub_and_cas_ids::Data {
							pub_id: file_path.pub_id,
							cas_id: file_path.cas_id,
						}),
				);

				if should_stop {
					break;
				}
			}

			Ok(to_remove)
		}
	}};
}

pub async fn reverse_update_directories_sizes(
	base_path: impl AsRef<Path>,
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	library: &Library,
) -> Result<(), FilePathError> {
	let base_path = base_path.as_ref();
	let location_path = location_path.as_ref();

	let Library { sync, db, .. } = library;

	let ancestors = base_path
		.ancestors()
		.take_while(|&ancestor| ancestor != location_path)
		.map(|ancestor| IsolatedFilePathData::new(location_id, location_path, ancestor, true))
		.collect::<Result<Vec<_>, _>>()?;

	let chunked_queries = ancestors
		.iter()
		.chunks(200)
		.into_iter()
		.map(|ancestors_iso_file_paths_chunk| {
			db.file_path()
				.find_many(vec![or(ancestors_iso_file_paths_chunk
					.into_iter()
					.map(file_path::WhereParam::from)
					.collect::<Vec<_>>())])
				.select(file_path::select!({ pub_id materialized_path name }))
		})
		.collect::<Vec<_>>();

	let mut pub_id_by_ancestor_materialized_path = db
		._batch(chunked_queries)
		.await?
		.into_iter()
		.flatten()
		.filter_map(
			|file_path| match (file_path.materialized_path, file_path.name) {
				(Some(materialized_path), Some(name)) => {
					Some((format!("{materialized_path}{name}/"), (file_path.pub_id, 0)))
				}
				_ => {
					warn!(
						"Found a file_path missing its materialized_path or name: <pub_id='{:#?}'>",
						from_bytes_to_uuid(&file_path.pub_id)
					);
					None
				}
			},
		)
		.collect::<HashMap<_, _>>();

	db.file_path()
		.find_many(vec![
			file_path::location_id::equals(Some(location_id)),
			file_path::materialized_path::in_vec(
				ancestors
					.iter()
					.map(|ancestor_iso_file_path| {
						ancestor_iso_file_path
							.materialized_path_for_children()
							.expect("each ancestor is a directory")
					})
					.collect(),
			),
		])
		.select(file_path::select!({ materialized_path size_in_bytes_bytes }))
		.exec()
		.await?
		.into_iter()
		.for_each(|file_path| {
			if let Some(materialized_path) = file_path.materialized_path {
				if let Some((_, size)) =
					pub_id_by_ancestor_materialized_path.get_mut(&materialized_path)
				{
					*size += file_path
						.size_in_bytes_bytes
						.map(|size_in_bytes_bytes| {
							u64::from_be_bytes([
								size_in_bytes_bytes[0],
								size_in_bytes_bytes[1],
								size_in_bytes_bytes[2],
								size_in_bytes_bytes[3],
								size_in_bytes_bytes[4],
								size_in_bytes_bytes[5],
								size_in_bytes_bytes[6],
								size_in_bytes_bytes[7],
							])
						})
						.unwrap_or_else(|| {
							warn!("Got a directory missing its size in bytes");
							0
						});
				}
			} else {
				warn!("Corrupt database possesing a file_path entry without materialized_path");
			}
		});

	let to_sync_and_update = ancestors
		.into_iter()
		.filter_map(|ancestor_iso_file_path| {
			if let Some((pub_id, size)) = pub_id_by_ancestor_materialized_path.remove(
				&ancestor_iso_file_path
					.materialized_path_for_children()
					.expect("each ancestor is a directory"),
			) {
				let size_bytes = size.to_be_bytes().to_vec();

				Some((
					sync.shared_update(
						prisma_sync::file_path::SyncId {
							pub_id: pub_id.clone(),
						},
						file_path::size_in_bytes_bytes::NAME,
						json!(size_bytes.clone()),
					),
					db.file_path().update(
						file_path::pub_id::equals(pub_id),
						vec![file_path::size_in_bytes_bytes::set(Some(size_bytes))],
					),
				))
			} else {
				warn!("Got a missing ancestor for a file_path in the database, maybe we have a corruption");
				None
			}
		})
		.unzip::<_, _, Vec<_>, Vec<_>>();

	sync.write_ops(db, to_sync_and_update).await?;

	Ok(())
}



File: ./src/location/indexer/rules/mod.rs
-------------------------------------------------
pub mod seed;

use crate::{
	library::Library,
	prisma::indexer_rule,
	util::{
		db::{maybe_missing, MissingFieldError},
		error::{FileIOError, NonUtf8PathError},
	},
};

use std::{
	collections::{HashMap, HashSet},
	marker::PhantomData,
	path::Path,
};

use chrono::{DateTime, Utc};
use futures::future::try_join_all;
use globset::{Glob, GlobSet, GlobSetBuilder};
use rmp_serde::{self, decode, encode};
use rspc::ErrorCode;
use serde::{de, ser, Deserialize, Serialize};
use specta::Type;
use thiserror::Error;
use tokio::fs;
use tracing::debug;
use uuid::Uuid;

#[derive(Error, Debug)]
pub enum IndexerRuleError {
	// User errors
	#[error("invalid indexer rule kind integer: {0}")]
	InvalidRuleKindInt(i32),
	#[error("glob builder error: {0}")]
	Glob(#[from] globset::Error),
	#[error(transparent)]
	NonUtf8Path(#[from] NonUtf8PathError),

	// Internal Errors
	#[error("indexer rule parameters encode error: {0}")]
	RuleParametersRMPEncode(#[from] encode::Error),
	#[error("indexer rule parameters decode error: {0}")]
	RuleParametersRMPDecode(#[from] decode::Error),
	#[error("accept by its children file I/O error: {0}")]
	AcceptByItsChildrenFileIO(FileIOError),
	#[error("reject by its children file I/O error: {0}")]
	RejectByItsChildrenFileIO(FileIOError),
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error("missing-field: {0}")]
	MissingField(#[from] MissingFieldError),
}

impl From<IndexerRuleError> for rspc::Error {
	fn from(err: IndexerRuleError) -> Self {
		match err {
			IndexerRuleError::InvalidRuleKindInt(_)
			| IndexerRuleError::Glob(_)
			| IndexerRuleError::NonUtf8Path(_) => {
				rspc::Error::with_cause(ErrorCode::BadRequest, err.to_string(), err)
			}

			_ => rspc::Error::with_cause(ErrorCode::InternalServerError, err.to_string(), err),
		}
	}
}

/// `IndexerRuleCreateArgs` is the argument received from the client using rspc to create a new indexer rule.
/// Note that `rules` field is a vector of tuples of `RuleKind` and `parameters`.
///
/// In case of  `RuleKind::AcceptFilesByGlob` or `RuleKind::RejectFilesByGlob`, it will be a
/// vector of strings containing a glob patterns.
///
/// In case of `RuleKind::AcceptIfChildrenDirectoriesArePresent` or `RuleKind::RejectIfChildrenDirectoriesArePresent` the
/// `parameters` field must be a vector of strings containing the names of the directories.
#[derive(Type, Deserialize)]
pub struct IndexerRuleCreateArgs {
	pub name: String,
	pub dry_run: bool,
	pub rules: Vec<(RuleKind, Vec<String>)>,
}

impl IndexerRuleCreateArgs {
	pub async fn create(
		self,
		library: &Library,
	) -> Result<Option<indexer_rule::Data>, IndexerRuleError> {
		debug!(
			"{} a new indexer rule (name = {}, params = {:?})",
			if self.dry_run {
				"Dry run: Would create"
			} else {
				"Trying to create"
			},
			self.name,
			self.rules
		);

		let rules_data = rmp_serde::to_vec_named(
			&self
				.rules
				.into_iter()
				.map(|(kind, parameters)| match kind {
					RuleKind::AcceptFilesByGlob => {
						RulePerKind::new_accept_files_by_globs_str(parameters)
					}
					RuleKind::RejectFilesByGlob => {
						RulePerKind::new_reject_files_by_globs_str(parameters)
					}
					RuleKind::AcceptIfChildrenDirectoriesArePresent => {
						Ok(RulePerKind::AcceptIfChildrenDirectoriesArePresent(
							parameters.into_iter().collect(),
						))
					}
					RuleKind::RejectIfChildrenDirectoriesArePresent => {
						Ok(RulePerKind::RejectIfChildrenDirectoriesArePresent(
							parameters.into_iter().collect(),
						))
					}
				})
				.collect::<Result<Vec<_>, _>>()?,
		)?;

		if self.dry_run {
			return Ok(None);
		}

		let date_created = Utc::now();

		use indexer_rule::*;

		Ok(Some(
			library
				.db
				.indexer_rule()
				.create(
					sd_utils::uuid_to_bytes(generate_pub_id()),
					vec![
						name::set(Some(self.name)),
						rules_per_kind::set(Some(rules_data)),
						date_created::set(Some(date_created.into())),
						date_modified::set(Some(date_created.into())),
					],
				)
				.exec()
				.await?,
		))
	}
}

#[repr(i32)]
#[non_exhaustive]
#[derive(Debug, Clone, Copy, Serialize, Deserialize, Type, Eq, PartialEq, Hash)]
pub enum RuleKind {
	AcceptFilesByGlob = 0,
	RejectFilesByGlob = 1,
	AcceptIfChildrenDirectoriesArePresent = 2,
	RejectIfChildrenDirectoriesArePresent = 3,
}

impl RuleKind {
	pub const fn variant_count() -> usize {
		// TODO: Use https://doc.rust-lang.org/std/mem/fn.variant_count.html if it ever gets stabilized
		4
	}
}

/// `ParametersPerKind` is a mapping from `RuleKind` to the parameters required for each kind of rule.
/// In case of doubt about globs, consult <https://docs.rs/globset/latest/globset/#syntax>
///
/// We store directly globs in the database, serialized using rmp_serde.
///
/// In case of `ParametersPerKind::AcceptIfChildrenDirectoriesArePresent` or `ParametersPerKind::RejectIfChildrenDirectoriesArePresent`
/// first we change the data structure to a vector, then we serialize it.
#[derive(Debug)]
pub enum RulePerKind {
	// TODO: Add an indexer rule that filter files based on their extended attributes
	// https://learn.microsoft.com/en-us/windows/win32/fileio/file-attribute-constants
	// https://en.wikipedia.org/wiki/Extended_file_attributes
	AcceptFilesByGlob(Vec<Glob>, GlobSet),
	RejectFilesByGlob(Vec<Glob>, GlobSet),
	AcceptIfChildrenDirectoriesArePresent(HashSet<String>),
	RejectIfChildrenDirectoriesArePresent(HashSet<String>),
}

impl RulePerKind {
	fn new_files_by_globs_str_and_kind(
		globs_str: impl IntoIterator<Item = impl AsRef<str>>,
		kind_fn: impl Fn(Vec<Glob>, GlobSet) -> Self,
	) -> Result<Self, IndexerRuleError> {
		globs_str
			.into_iter()
			.map(|s| s.as_ref().parse::<Glob>())
			.collect::<Result<Vec<_>, _>>()
			.and_then(|globs| {
				globs
					.iter()
					.cloned()
					.fold(&mut GlobSetBuilder::new(), |builder, glob| {
						builder.add(glob)
					})
					.build()
					.map(move |glob_set| kind_fn(globs, glob_set))
					.map_err(Into::into)
			})
			.map_err(Into::into)
	}

	pub fn new_accept_files_by_globs_str(
		globs_str: impl IntoIterator<Item = impl AsRef<str>>,
	) -> Result<Self, IndexerRuleError> {
		Self::new_files_by_globs_str_and_kind(globs_str, Self::AcceptFilesByGlob)
	}

	pub fn new_reject_files_by_globs_str(
		globs_str: impl IntoIterator<Item = impl AsRef<str>>,
	) -> Result<Self, IndexerRuleError> {
		Self::new_files_by_globs_str_and_kind(globs_str, Self::RejectFilesByGlob)
	}
}

/// We're implementing `Serialize` by hand as `GlobSet`s aren't serializable, so we ignore them on
/// serialization
impl Serialize for RulePerKind {
	fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
	where
		S: ser::Serializer,
	{
		match *self {
			RulePerKind::AcceptFilesByGlob(ref globs, ref _glob_set) => serializer
				.serialize_newtype_variant("ParametersPerKind", 0, "AcceptFilesByGlob", globs),
			RulePerKind::RejectFilesByGlob(ref globs, ref _glob_set) => serializer
				.serialize_newtype_variant("ParametersPerKind", 1, "RejectFilesByGlob", globs),
			RulePerKind::AcceptIfChildrenDirectoriesArePresent(ref children) => serializer
				.serialize_newtype_variant(
					"ParametersPerKind",
					2,
					"AcceptIfChildrenDirectoriesArePresent",
					children,
				),
			RulePerKind::RejectIfChildrenDirectoriesArePresent(ref children) => serializer
				.serialize_newtype_variant(
					"ParametersPerKind",
					3,
					"RejectIfChildrenDirectoriesArePresent",
					children,
				),
		}
	}
}

impl<'de> Deserialize<'de> for RulePerKind {
	fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
	where
		D: de::Deserializer<'de>,
	{
		const VARIANTS: &[&str] = &[
			"AcceptFilesByGlob",
			"RejectFilesByGlob",
			"AcceptIfChildrenDirectoriesArePresent",
			"RejectIfChildrenDirectoriesArePresent",
		];

		enum Fields {
			AcceptFilesByGlob,
			RejectFilesByGlob,
			AcceptIfChildrenDirectoriesArePresent,
			RejectIfChildrenDirectoriesArePresent,
		}

		struct FieldsVisitor;

		impl<'de> de::Visitor<'de> for FieldsVisitor {
			type Value = Fields;

			fn expecting(&self, formatter: &mut std::fmt::Formatter) -> std::fmt::Result {
				formatter.write_str(
					"`AcceptFilesByGlob` \
				or `RejectFilesByGlob` \
				or `AcceptIfChildrenDirectoriesArePresent` \
				or `RejectIfChildrenDirectoriesArePresent`",
				)
			}

			fn visit_u64<E>(self, value: u64) -> Result<Self::Value, E>
			where
				E: de::Error,
			{
				match value {
					0 => Ok(Fields::AcceptFilesByGlob),
					1 => Ok(Fields::RejectFilesByGlob),
					2 => Ok(Fields::AcceptIfChildrenDirectoriesArePresent),
					3 => Ok(Fields::RejectIfChildrenDirectoriesArePresent),
					_ => Err(de::Error::invalid_value(
						de::Unexpected::Unsigned(value),
						&"variant index 0 <= i < 3",
					)),
				}
			}
			fn visit_str<E>(self, value: &str) -> Result<Self::Value, E>
			where
				E: de::Error,
			{
				match value {
					"AcceptFilesByGlob" => Ok(Fields::AcceptFilesByGlob),
					"RejectFilesByGlob" => Ok(Fields::RejectFilesByGlob),
					"AcceptIfChildrenDirectoriesArePresent" => {
						Ok(Fields::AcceptIfChildrenDirectoriesArePresent)
					}
					"RejectIfChildrenDirectoriesArePresent" => {
						Ok(Fields::RejectIfChildrenDirectoriesArePresent)
					}
					_ => Err(de::Error::unknown_variant(value, VARIANTS)),
				}
			}
			fn visit_bytes<E>(self, bytes: &[u8]) -> Result<Self::Value, E>
			where
				E: de::Error,
			{
				match bytes {
					b"AcceptFilesByGlob" => Ok(Fields::AcceptFilesByGlob),
					b"RejectFilesByGlob" => Ok(Fields::RejectFilesByGlob),
					b"AcceptIfChildrenDirectoriesArePresent" => {
						Ok(Fields::AcceptIfChildrenDirectoriesArePresent)
					}
					b"RejectIfChildrenDirectoriesArePresent" => {
						Ok(Fields::RejectIfChildrenDirectoriesArePresent)
					}
					_ => Err(de::Error::unknown_variant(
						&String::from_utf8_lossy(bytes),
						VARIANTS,
					)),
				}
			}
		}

		impl<'de> Deserialize<'de> for Fields {
			#[inline]
			fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
			where
				D: de::Deserializer<'de>,
			{
				deserializer.deserialize_identifier(FieldsVisitor)
			}
		}

		struct ParametersPerKindVisitor<'de> {
			marker: PhantomData<RulePerKind>,
			lifetime: PhantomData<&'de ()>,
		}

		impl<'de> de::Visitor<'de> for ParametersPerKindVisitor<'de> {
			type Value = RulePerKind;

			fn expecting(&self, formatter: &mut std::fmt::Formatter) -> std::fmt::Result {
				formatter.write_str("enum ParametersPerKind")
			}

			fn visit_enum<PPK>(self, data: PPK) -> Result<Self::Value, PPK::Error>
			where
				PPK: de::EnumAccess<'de>,
			{
				use de::Error;

				de::EnumAccess::variant(data).and_then(|value| match value {
					(Fields::AcceptFilesByGlob, accept_files_by_glob) => {
						de::VariantAccess::newtype_variant::<Vec<Glob>>(accept_files_by_glob)
							.and_then(|globs| {
								globs
									.iter()
									.fold(&mut GlobSetBuilder::new(), |builder, glob| {
										builder.add(glob.to_owned())
									})
									.build()
									.map_or_else(
										|e| Err(PPK::Error::custom(e)),
										|glob_set| {
											Ok(Self::Value::AcceptFilesByGlob(globs, glob_set))
										},
									)
							})
					}
					(Fields::RejectFilesByGlob, reject_files_by_glob) => {
						de::VariantAccess::newtype_variant::<Vec<Glob>>(reject_files_by_glob)
							.and_then(|globs| {
								globs
									.iter()
									.fold(&mut GlobSetBuilder::new(), |builder, glob| {
										builder.add(glob.to_owned())
									})
									.build()
									.map_or_else(
										|e| Err(PPK::Error::custom(e)),
										|glob_set| {
											Ok(Self::Value::RejectFilesByGlob(globs, glob_set))
										},
									)
							})
					}
					(
						Fields::AcceptIfChildrenDirectoriesArePresent,
						accept_if_children_directories_are_present,
					) => de::VariantAccess::newtype_variant::<HashSet<String>>(
						accept_if_children_directories_are_present,
					)
					.map(Self::Value::AcceptIfChildrenDirectoriesArePresent),
					(
						Fields::RejectIfChildrenDirectoriesArePresent,
						reject_if_children_directories_are_present,
					) => de::VariantAccess::newtype_variant::<HashSet<String>>(
						reject_if_children_directories_are_present,
					)
					.map(Self::Value::RejectIfChildrenDirectoriesArePresent),
				})
			}
		}

		deserializer.deserialize_enum(
			"ParametersPerKind",
			VARIANTS,
			ParametersPerKindVisitor {
				marker: PhantomData::<RulePerKind>,
				lifetime: PhantomData,
			},
		)
	}
}

impl RulePerKind {
	async fn apply(&self, source: impl AsRef<Path>) -> Result<(RuleKind, bool), IndexerRuleError> {
		match self {
			RulePerKind::AcceptIfChildrenDirectoriesArePresent(children) => {
				accept_dir_for_its_children(source, children)
					.await
					.map(|accepted| (RuleKind::AcceptIfChildrenDirectoriesArePresent, accepted))
			}
			RulePerKind::RejectIfChildrenDirectoriesArePresent(children) => {
				reject_dir_for_its_children(source, children)
					.await
					.map(|rejected| (RuleKind::RejectIfChildrenDirectoriesArePresent, rejected))
			}

			RulePerKind::AcceptFilesByGlob(_globs, accept_glob_set) => Ok((
				RuleKind::AcceptFilesByGlob,
				accept_by_glob(source, accept_glob_set),
			)),
			RulePerKind::RejectFilesByGlob(_globs, reject_glob_set) => Ok((
				RuleKind::RejectFilesByGlob,
				reject_by_glob(source, reject_glob_set),
			)),
		}
	}
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IndexerRule {
	pub id: Option<i32>,
	pub name: String,
	pub default: bool,
	pub rules: Vec<RulePerKind>,
	pub date_created: DateTime<Utc>,
	pub date_modified: DateTime<Utc>,
}

impl IndexerRule {
	pub async fn apply(
		&self,
		source: impl AsRef<Path>,
	) -> Result<Vec<(RuleKind, bool)>, IndexerRuleError> {
		try_join_all(self.rules.iter().map(|rule| rule.apply(source.as_ref()))).await
	}

	pub async fn apply_all(
		rules: &[IndexerRule],
		source: impl AsRef<Path>,
	) -> Result<HashMap<RuleKind, Vec<bool>>, IndexerRuleError> {
		try_join_all(rules.iter().map(|rule| rule.apply(source.as_ref())))
			.await
			.map(|results| {
				results.into_iter().flatten().fold(
					HashMap::<_, Vec<_>>::with_capacity(RuleKind::variant_count()),
					|mut map, (kind, result)| {
						map.entry(kind).or_default().push(result);
						map
					},
				)
			})
	}
}

impl TryFrom<&indexer_rule::Data> for IndexerRule {
	type Error = IndexerRuleError;

	fn try_from(data: &indexer_rule::Data) -> Result<Self, Self::Error> {
		Ok(Self {
			id: Some(data.id),
			name: maybe_missing(data.name.clone(), "indexer_rule.name")?,
			default: data.default.unwrap_or_default(),
			rules: rmp_serde::from_slice(maybe_missing(
				&data.rules_per_kind,
				"indexer_rule.rules_per_kind",
			)?)?,
			date_created: maybe_missing(data.date_created, "indexer_rule.date_created")?.into(),
			date_modified: maybe_missing(data.date_modified, "indexer_rule.date_modified")?.into(),
		})
	}
}

impl TryFrom<indexer_rule::Data> for IndexerRule {
	type Error = IndexerRuleError;

	fn try_from(data: indexer_rule::Data) -> Result<Self, Self::Error> {
		Self::try_from(&data)
	}
}

fn accept_by_glob(source: impl AsRef<Path>, accept_glob_set: &GlobSet) -> bool {
	accept_glob_set.is_match(source.as_ref())
}

fn reject_by_glob(source: impl AsRef<Path>, reject_glob_set: &GlobSet) -> bool {
	!accept_by_glob(source.as_ref(), reject_glob_set)
}

async fn accept_dir_for_its_children(
	source: impl AsRef<Path>,
	children: &HashSet<String>,
) -> Result<bool, IndexerRuleError> {
	let source = source.as_ref();

	// FIXME(fogodev): Just check for io::ErrorKind::NotADirectory error instead (feature = "io_error_more", issue = "86442")
	if !fs::metadata(source)
		.await
		.map_err(|e| IndexerRuleError::AcceptByItsChildrenFileIO(FileIOError::from((source, e))))?
		.is_dir()
	{
		return Ok(false);
	}

	let mut read_dir = fs::read_dir(source)
		.await // TODO: Check NotADirectory error here when available
		.map_err(|e| IndexerRuleError::AcceptByItsChildrenFileIO(FileIOError::from((source, e))))?;
	while let Some(entry) = read_dir
		.next_entry()
		.await
		.map_err(|e| IndexerRuleError::AcceptByItsChildrenFileIO(FileIOError::from((source, e))))?
	{
		let entry_name = entry
			.file_name()
			.to_str()
			.ok_or_else(|| NonUtf8PathError(entry.path().into()))?
			.to_string();

		if entry
			.metadata()
			.await
			.map_err(|e| {
				IndexerRuleError::AcceptByItsChildrenFileIO(FileIOError::from((source, e)))
			})?
			.is_dir() && children.contains(&entry_name)
		{
			return Ok(true);
		}
	}

	Ok(false)
}

async fn reject_dir_for_its_children(
	source: impl AsRef<Path>,
	children: &HashSet<String>,
) -> Result<bool, IndexerRuleError> {
	let source = source.as_ref();

	// FIXME(fogodev): Just check for io::ErrorKind::NotADirectory error instead (feature = "io_error_more", issue = "86442")
	if !fs::metadata(source)
		.await
		.map_err(|e| IndexerRuleError::AcceptByItsChildrenFileIO(FileIOError::from((source, e))))?
		.is_dir()
	{
		return Ok(true);
	}

	let mut read_dir = fs::read_dir(source)
		.await // TODO: Check NotADirectory error here when available
		.map_err(|e| IndexerRuleError::RejectByItsChildrenFileIO(FileIOError::from((source, e))))?;
	while let Some(entry) = read_dir
		.next_entry()
		.await
		.map_err(|e| IndexerRuleError::RejectByItsChildrenFileIO(FileIOError::from((source, e))))?
	{
		if entry
			.metadata()
			.await
			.map_err(|e| {
				IndexerRuleError::RejectByItsChildrenFileIO(FileIOError::from((source, e)))
			})?
			.is_dir() && children.contains(
			entry
				.file_name()
				.to_str()
				.ok_or_else(|| NonUtf8PathError(entry.path().into()))?,
		) {
			return Ok(false);
		}
	}

	Ok(true)
}

pub fn generate_pub_id() -> Uuid {
	loop {
		let pub_id = Uuid::new_v4();
		if pub_id.as_u128() >= 0xFFF {
			return pub_id;
		}
	}
}

#[cfg(test)]
#[allow(clippy::unwrap_used)]
mod tests {
	use super::*;
	use tempfile::tempdir;
	use tokio::fs;

	impl IndexerRule {
		pub fn new(name: String, default: bool, rules: Vec<RulePerKind>) -> Self {
			Self {
				id: None,
				name,
				default,
				rules,
				date_created: Utc::now(),
				date_modified: Utc::now(),
			}
		}
	}

	async fn check_rule(indexer_rule: &IndexerRule, path: impl AsRef<Path>) -> bool {
		indexer_rule
			.apply(path)
			.await
			.unwrap()
			.into_iter()
			.all(|(_kind, res)| res)
	}

	#[tokio::test]
	async fn test_reject_hidden_file() {
		let hidden = Path::new(".hidden.txt");
		let normal = Path::new("normal.txt");
		let hidden_inner_dir = Path::new("/test/.hidden/");
		let hidden_inner_file = Path::new("/test/.hidden/file.txt");
		let normal_inner_dir = Path::new("/test/normal/");
		let normal_inner_file = Path::new("/test/normal/inner.txt");
		let rule = IndexerRule::new(
			"ignore hidden files".to_string(),
			false,
			vec![RulePerKind::RejectFilesByGlob(
				vec![],
				GlobSetBuilder::new()
					.add(Glob::new("**/.*").unwrap())
					.build()
					.unwrap(),
			)],
		);

		assert!(!check_rule(&rule, hidden).await);
		assert!(check_rule(&rule, normal).await);
		assert!(!check_rule(&rule, hidden_inner_dir).await);
		assert!(!check_rule(&rule, hidden_inner_file).await);
		assert!(check_rule(&rule, normal_inner_dir).await);
		assert!(check_rule(&rule, normal_inner_file).await);
	}

	#[tokio::test]
	async fn test_reject_specific_dir() {
		let project_file = Path::new("/test/project/src/main.rs");
		let project_build_dir = Path::new("/test/project/target");
		let project_build_dir_inner = Path::new("/test/project/target/debug/");

		let rule = IndexerRule::new(
			"ignore build directory".to_string(),
			false,
			vec![RulePerKind::RejectFilesByGlob(
				vec![],
				GlobSetBuilder::new()
					.add(Glob::new("{**/target/*,**/target}").unwrap())
					.build()
					.unwrap(),
			)],
		);

		assert!(check_rule(&rule, project_file).await);
		assert!(!check_rule(&rule, project_build_dir).await);
		assert!(!check_rule(&rule, project_build_dir_inner).await);
	}

	#[tokio::test]
	async fn test_only_photos() {
		let text = Path::new("file.txt");
		let png = Path::new("photo1.png");
		let jpg = Path::new("photo1.png");
		let jpeg = Path::new("photo3.jpeg");
		let inner_text = Path::new("/test/file.txt");
		let inner_png = Path::new("/test/photo1.png");
		let inner_jpg = Path::new("/test/photo2.jpg");
		let inner_jpeg = Path::new("/test/photo3.jpeg");
		let many_inner_dirs_text = Path::new("/test/1/2/3/4/4/5/6/file.txt");
		let many_inner_dirs_png = Path::new("/test/1/2/3/4/4/5/6/photo1.png");
		let rule = IndexerRule::new(
			"only photos".to_string(),
			false,
			vec![RulePerKind::AcceptFilesByGlob(
				vec![],
				GlobSetBuilder::new()
					.add(Glob::new("*.{jpg,png,jpeg}").unwrap())
					.build()
					.unwrap(),
			)],
		);

		assert!(!check_rule(&rule, text).await);
		assert!(check_rule(&rule, png).await);
		assert!(check_rule(&rule, jpg).await);
		assert!(check_rule(&rule, jpeg).await);
		assert!(!check_rule(&rule, inner_text).await);
		assert!(check_rule(&rule, inner_png).await);
		assert!(check_rule(&rule, inner_jpg).await);
		assert!(check_rule(&rule, inner_jpeg).await);
		assert!(!check_rule(&rule, many_inner_dirs_text).await);
		assert!(check_rule(&rule, many_inner_dirs_png).await);
	}

	#[tokio::test]
	async fn test_directory_has_children() {
		let root = tempdir().unwrap();

		let project1 = root.path().join("project1");
		let project2 = root.path().join("project2");
		let not_project = root.path().join("not_project");

		fs::create_dir(&project1).await.unwrap();
		fs::create_dir(&project2).await.unwrap();
		fs::create_dir(&not_project).await.unwrap();

		fs::create_dir(project1.join(".git")).await.unwrap();
		fs::create_dir(project2.join(".git")).await.unwrap();
		fs::create_dir(project2.join("books")).await.unwrap();

		let childrens = [".git".to_string()].into_iter().collect::<HashSet<_>>();

		let rule = IndexerRule::new(
			"git projects".to_string(),
			false,
			vec![RulePerKind::AcceptIfChildrenDirectoriesArePresent(
				childrens,
			)],
		);

		assert!(check_rule(&rule, project1).await);
		assert!(check_rule(&rule, project2).await);
		assert!(!check_rule(&rule, not_project).await);
	}

	#[tokio::test]
	async fn test_reject_directory_by_its_children() {
		let root = tempdir().unwrap();

		let project1 = root.path().join("project1");
		let project2 = root.path().join("project2");
		let not_project = root.path().join("not_project");

		fs::create_dir(&project1).await.unwrap();
		fs::create_dir(&project2).await.unwrap();
		fs::create_dir(&not_project).await.unwrap();

		fs::create_dir(project1.join(".git")).await.unwrap();
		fs::create_dir(project2.join(".git")).await.unwrap();
		fs::create_dir(project2.join("books")).await.unwrap();

		let childrens = [".git".to_string()].into_iter().collect::<HashSet<_>>();

		let rule = IndexerRule::new(
			"git projects".to_string(),
			false,
			vec![RulePerKind::RejectIfChildrenDirectoriesArePresent(
				childrens,
			)],
		);

		assert!(!check_rule(&rule, project1).await);
		assert!(!check_rule(&rule, project2).await);
		assert!(check_rule(&rule, not_project).await);
	}

	impl PartialEq for RulePerKind {
		fn eq(&self, other: &Self) -> bool {
			match (self, other) {
				(
					RulePerKind::AcceptFilesByGlob(self_globs, _),
					RulePerKind::AcceptFilesByGlob(other_globs, _),
				) => self_globs == other_globs,
				(
					RulePerKind::RejectFilesByGlob(self_globs, _),
					RulePerKind::RejectFilesByGlob(other_globs, _),
				) => self_globs == other_globs,
				(
					RulePerKind::AcceptIfChildrenDirectoriesArePresent(self_childrens),
					RulePerKind::AcceptIfChildrenDirectoriesArePresent(other_childrens),
				) => self_childrens == other_childrens,
				(
					RulePerKind::RejectIfChildrenDirectoriesArePresent(self_childrens),
					RulePerKind::RejectIfChildrenDirectoriesArePresent(other_childrens),
				) => self_childrens == other_childrens,
				_ => false,
			}
		}
	}

	impl Eq for RulePerKind {}

	impl PartialEq for IndexerRule {
		fn eq(&self, other: &Self) -> bool {
			self.id == other.id
				&& self.name == other.name
				&& self.default == other.default
				&& self.rules == other.rules
				&& self.date_created == other.date_created
				&& self.date_modified == other.date_modified
		}
	}

	impl Eq for IndexerRule {}

	#[test]
	fn serde_smoke_test() {
		let actual = IndexerRule::new(
			"No Hidden".to_string(),
			true,
			vec![RulePerKind::RejectFilesByGlob(
				vec![Glob::new("**/.*").unwrap()],
				Glob::new("**/.*")
					.and_then(|glob| GlobSetBuilder::new().add(glob).build())
					.unwrap(),
			)],
		);

		let expected =
			rmp_serde::from_slice::<IndexerRule>(&rmp_serde::to_vec_named(&actual).unwrap())
				.unwrap();

		assert_eq!(actual, expected);
	}
}



File: ./src/location/indexer/rules/seed.rs
-------------------------------------------------
use crate::{
	library::Library,
	location::indexer::rules::{IndexerRule, IndexerRuleError, RulePerKind},
};
use chrono::Utc;
use sd_prisma::prisma::indexer_rule;
use thiserror::Error;
use uuid::Uuid;

#[derive(Error, Debug)]
pub enum SeederError {
	#[error("Failed to run indexer rules seeder: {0}")]
	IndexerRules(#[from] IndexerRuleError),
	#[error("An error occurred with the database while applying migrations: {0}")]
	DatabaseError(#[from] prisma_client_rust::QueryError),
}

pub struct SystemIndexerRule {
	name: &'static str,
	rules: Vec<RulePerKind>,
	default: bool,
}

impl From<SystemIndexerRule> for IndexerRule {
	fn from(rule: SystemIndexerRule) -> Self {
		Self {
			id: None,
			name: rule.name.to_string(),
			default: rule.default,
			rules: rule.rules,
			date_created: Utc::now(),
			date_modified: Utc::now(),
		}
	}
}

/// Seeds system indexer rules into a new or existing library,
pub async fn new_or_existing_library(library: &Library) -> Result<(), SeederError> {
	// DO NOT REORDER THIS ARRAY!
	for (i, rule) in [no_os_protected(), no_hidden(), no_git(), only_images()]
		.into_iter()
		.enumerate()
	{
		let pub_id = sd_utils::uuid_to_bytes(Uuid::from_u128(i as u128));
		let rules = rmp_serde::to_vec_named(&rule.rules).map_err(IndexerRuleError::from)?;

		use indexer_rule::*;

		let data = vec![
			name::set(Some(rule.name.to_string())),
			rules_per_kind::set(Some(rules.clone())),
			default::set(Some(rule.default)),
			date_created::set(Some(Utc::now().into())),
			date_modified::set(Some(Utc::now().into())),
		];

		library
			.db
			.indexer_rule()
			.upsert(
				indexer_rule::pub_id::equals(pub_id.clone()),
				indexer_rule::create(pub_id.clone(), data.clone()),
				data,
			)
			.exec()
			.await?;
	}

	Ok(())
}

pub fn no_os_protected() -> SystemIndexerRule {
	SystemIndexerRule {
        // TODO: On windows, beside the listed files, any file with the FILE_ATTRIBUTE_SYSTEM should be considered a system file
        // https://learn.microsoft.com/en-us/windows/win32/fileio/file-attribute-constants#FILE_ATTRIBUTE_SYSTEM
        name: "No OS protected",
        default: true,
        rules: vec![
            RulePerKind::new_reject_files_by_globs_str(
                [
                    vec![
                        "**/.spacedrive",
                    ],
                    // Globset, even on Windows, requires the use of / as a separator
                    // https://github.com/github/gitignore/blob/main/Global/Windows.gitignore
                    // https://learn.microsoft.com/en-us/windows/win32/fileio/naming-a-file
                    #[cfg(target_os = "windows")]
                    vec![
                        // Windows thumbnail cache files
                        "**/{Thumbs.db,Thumbs.db:encryptable,ehthumbs.db,ehthumbs_vista.db}",
                        // Dump file
                        "**/*.stackdump",
                        // Folder config file
                        "**/[Dd]esktop.ini",
                        // Recycle Bin used on file shares
                        "**/$RECYCLE.BIN",
                        // Chkdsk recovery directory
                        "**/FOUND.[0-9][0-9][0-9]",
                        // Reserved names
                        "**/{CON,PRN,AUX,NUL,COM0,COM1,COM2,COM3,COM4,COM5,COM6,COM7,COM8,COM9,LPT0,LPT1,LPT2,LPT3,LPT4,LPT5,LPT6,LPT7,LPT8,LPT9}",
                        "**/{CON,PRN,AUX,NUL,COM0,COM1,COM2,COM3,COM4,COM5,COM6,COM7,COM8,COM9,LPT0,LPT1,LPT2,LPT3,LPT4,LPT5,LPT6,LPT7,LPT8,LPT9}.*",
                        // User special files
                        "C:/Users/*/NTUSER.DAT*",
                        "C:/Users/*/ntuser.dat*",
                        "C:/Users/*/{ntuser.ini,ntuser.dat,NTUSER.DAT}",
                        // User special folders (most of these the user dont even have permission to access)
                        "C:/Users/*/{Cookies,AppData,NetHood,Recent,PrintHood,SendTo,Templates,Start Menu,Application Data,Local Settings,My Documents}",
                        // System special folders
                        "C:/{$Recycle.Bin,$WinREAgent,Documents and Settings,Program Files,Program Files (x86),ProgramData,Recovery,PerfLogs,Windows,Windows.old}",
                        // NTFS internal dir, can exists on any drive
                        "[A-Z]:/System Volume Information",
                        // System special files
                        "C:/{config,pagefile,hiberfil}.sys",
                        // Windows can create a swapfile on any drive
                        "[A-Z]:/swapfile.sys",
                        "C:/DumpStack.log.tmp",
                    ],
                    // https://github.com/github/gitignore/blob/main/Global/macOS.gitignore
                    // https://developer.apple.com/library/archive/documentation/FileManagement/Conceptual/FileSystemProgrammingGuide/FileSystemOverview/FileSystemOverview.html#//apple_ref/doc/uid/TP40010672-CH2-SW14
                    #[cfg(any(target_os = "ios", target_os = "macos"))]
                    vec![
                        "**/.{DS_Store,AppleDouble,LSOverride}",
                        // Icon must end with two \r
                        "**/Icon\r\r",
                        // Thumbnails
                        "**/._*",
                    ],
                    #[cfg(target_os = "macos")]
                    vec![
                        "/{System,Network,Library,Applications,.PreviousSystemInformation,.com.apple.templatemigration.boot-install}",
						"/System/Volumes/Data/{System,Network,Library,Applications,.PreviousSystemInformation,.com.apple.templatemigration.boot-install}",
                        "/Users/*/{Library,Applications}",
                        "/System/Volumes/Data/Users/*/{Library,Applications}",
                        "**/*.photoslibrary/{database,external,private,resources,scope}",
                        // Files that might appear in the root of a volume
                        "**/.{DocumentRevisions-V100,fseventsd,Spotlight-V100,TemporaryItems,Trashes,VolumeIcon.icns,com.apple.timemachine.donotpresent}",
                        // Directories potentially created on remote AFP share
                        "**/.{AppleDB,AppleDesktop,apdisk}",
                        "**/{Network Trash Folder,Temporary Items}",
                    ],
                    // https://github.com/github/gitignore/blob/main/Global/Linux.gitignore
                    #[cfg(target_os = "linux")]
                    vec![
                        "**/*~",
                        // temporary files which can be created if a process still has a handle open of a deleted file
                        "**/.fuse_hidden*",
                        // KDE directory preferences
                        "**/.directory",
                        // Linux trash folder which might appear on any partition or disk
                        "**/.Trash-*",
                        // .nfs files are created when an open file is removed but is still being accessed
                        "**/.nfs*",
                    ],
                    #[cfg(target_os = "android")]
                    vec![
                        "**/.nomedia",
                        "**/.thumbnails",
                    ],
                    // https://en.wikipedia.org/wiki/Unix_filesystem#Conventional_directory_layout
                    // https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard
                    #[cfg(target_family = "unix")]
                    vec![
                        // Directories containing unix memory/device mapped files/dirs
                        "/{dev,sys,proc}",
                        // Directories containing special files for current running programs
                        "/{run,var,boot}",
                        // ext2-4 recovery directory
                        "**/lost+found",
                    ],
                ]
                .into_iter()
                .flatten()
            ).expect("this is hardcoded and should always work"),
        ],
    }
}

pub fn no_hidden() -> SystemIndexerRule {
	SystemIndexerRule {
		name: "No Hidden",
		default: false,
		rules: vec![RulePerKind::new_reject_files_by_globs_str(["**/.*"])
			.expect("this is hardcoded and should always work")],
	}
}

fn no_git() -> SystemIndexerRule {
	SystemIndexerRule {
		name: "No Git",
		default: false,
		rules: vec![RulePerKind::new_reject_files_by_globs_str([
			"**/{.git,.gitignore,.gitattributes,.gitkeep,.gitconfig,.gitmodules}",
		])
		.expect("this is hardcoded and should always work")],
	}
}

fn only_images() -> SystemIndexerRule {
	SystemIndexerRule {
		name: "Only Images",
		default: false,
		rules: vec![RulePerKind::new_accept_files_by_globs_str([
			"*.{avif,bmp,gif,ico,jpeg,jpg,png,svg,tif,tiff,webp}",
		])
		.expect("this is hardcoded and should always work")],
	}
}



File: ./src/location/indexer/walk.rs
-------------------------------------------------
use crate::{
	location::file_path_helper::{
		file_path_pub_and_cas_ids, file_path_walker, FilePathMetadata, IsolatedFilePathData,
	},
	prisma::file_path,
	util::{db::inode_from_db, error::FileIOError},
};

use std::{
	collections::{HashMap, HashSet, VecDeque},
	future::Future,
	hash::{Hash, Hasher},
	path::{Path, PathBuf},
};

use chrono::{DateTime, Duration, FixedOffset};
use serde::{Deserialize, Serialize};
use tokio::fs;
use tracing::trace;
use uuid::Uuid;

use super::{
	rules::{IndexerRule, RuleKind},
	IndexerError,
};

const TO_WALK_QUEUE_INITIAL_CAPACITY: usize = 32;
const WALKER_PATHS_BUFFER_INITIAL_CAPACITY: usize = 256;
const WALK_SINGLE_DIR_PATHS_BUFFER_INITIAL_CAPACITY: usize = 32;

/// `WalkEntry` represents a single path in the filesystem, for any comparison purposes, we only
/// consider the path itself, not the metadata.
#[derive(Debug, Serialize, Deserialize)]
pub struct WalkedEntry {
	pub pub_id: Uuid,
	pub maybe_object_id: file_path::object_id::Type,
	pub iso_file_path: IsolatedFilePathData<'static>,
	pub metadata: FilePathMetadata,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ToWalkEntry {
	path: PathBuf,
	parent_dir_accepted_by_its_children: Option<bool>,
	maybe_parent: Option<PathBuf>,
}

#[derive(Debug)]
struct WalkingEntry {
	iso_file_path: IsolatedFilePathData<'static>,
	maybe_metadata: Option<FilePathMetadata>,
}

impl From<WalkingEntry> for WalkedEntry {
	fn from(walking_entry: WalkingEntry) -> Self {
		let WalkingEntry {
			iso_file_path,
			maybe_metadata,
		} = walking_entry;

		Self {
			pub_id: Uuid::new_v4(),
			maybe_object_id: None,
			iso_file_path,
			metadata: maybe_metadata
				.expect("we always use Some in `the inner_walk_single_dir` function"),
		}
	}
}

impl From<(Uuid, file_path::object_id::Type, WalkingEntry)> for WalkedEntry {
	fn from(
		(pub_id, maybe_object_id, walking_entry): (Uuid, file_path::object_id::Type, WalkingEntry),
	) -> Self {
		let WalkingEntry {
			iso_file_path,
			maybe_metadata,
		} = walking_entry;

		Self {
			pub_id,
			maybe_object_id,
			iso_file_path,
			metadata: maybe_metadata
				.expect("we always use Some in `the inner_walk_single_dir` function"),
		}
	}
}

impl PartialEq for WalkingEntry {
	fn eq(&self, other: &Self) -> bool {
		self.iso_file_path == other.iso_file_path
	}
}

impl Eq for WalkingEntry {}

impl Hash for WalkingEntry {
	fn hash<H: Hasher>(&self, state: &mut H) {
		self.iso_file_path.hash(state);
	}
}

pub struct WalkResult<Walked, ToUpdate, ToRemove>
where
	Walked: Iterator<Item = WalkedEntry>,
	ToUpdate: Iterator<Item = WalkedEntry>,
	ToRemove: Iterator<Item = file_path_pub_and_cas_ids::Data>,
{
	pub walked: Walked,
	pub to_update: ToUpdate,
	pub to_walk: VecDeque<ToWalkEntry>,
	pub to_remove: ToRemove,
	pub errors: Vec<IndexerError>,
	pub paths_and_sizes: HashMap<PathBuf, u64>,
}

/// This function walks through the filesystem, applying the rules to each entry and then returning
/// a list of accepted entries. There are some useful comments in the implementation of this function
/// in case of doubts.
pub(super) async fn walk<FilePathDBFetcherFut, ToRemoveDbFetcherFut>(
	root: impl AsRef<Path>,
	indexer_rules: &[IndexerRule],
	mut update_notifier: impl FnMut(&Path, usize),
	file_paths_db_fetcher: impl Fn(Vec<file_path::WhereParam>) -> FilePathDBFetcherFut,
	to_remove_db_fetcher: impl Fn(
		IsolatedFilePathData<'static>,
		Vec<file_path::WhereParam>,
	) -> ToRemoveDbFetcherFut,
	iso_file_path_factory: impl Fn(&Path, bool) -> Result<IsolatedFilePathData<'static>, IndexerError>,
	limit: u64,
) -> Result<
	WalkResult<
		impl Iterator<Item = WalkedEntry>,
		impl Iterator<Item = WalkedEntry>,
		impl Iterator<Item = file_path_pub_and_cas_ids::Data>,
	>,
	IndexerError,
>
where
	FilePathDBFetcherFut: Future<Output = Result<Vec<file_path_walker::Data>, IndexerError>>,
	ToRemoveDbFetcherFut:
		Future<Output = Result<Vec<file_path_pub_and_cas_ids::Data>, IndexerError>>,
{
	let root = root.as_ref();

	let mut to_walk = VecDeque::with_capacity(TO_WALK_QUEUE_INITIAL_CAPACITY);
	to_walk.push_back(ToWalkEntry {
		path: root.to_path_buf(),
		parent_dir_accepted_by_its_children: None,
		maybe_parent: None,
	});
	let mut indexed_paths = HashSet::with_capacity(WALKER_PATHS_BUFFER_INITIAL_CAPACITY);
	let mut errors = vec![];
	let mut paths_buffer = HashSet::with_capacity(WALKER_PATHS_BUFFER_INITIAL_CAPACITY);
	let mut paths_and_sizes = HashMap::with_capacity(TO_WALK_QUEUE_INITIAL_CAPACITY);
	let mut to_remove = vec![];

	while let Some(entry) = to_walk.pop_front() {
		let (entry_size, current_to_remove) = inner_walk_single_dir(
			root,
			&entry,
			indexer_rules,
			&mut update_notifier,
			&to_remove_db_fetcher,
			&iso_file_path_factory,
			WorkingTable {
				indexed_paths: &mut indexed_paths,
				paths_buffer: &mut paths_buffer,
				maybe_to_walk: Some(&mut to_walk),
				errors: &mut errors,
			},
		)
		.await;
		to_remove.push(current_to_remove);

		// Saving the size of current entry
		paths_and_sizes.insert(entry.path, entry_size);

		// Adding the size of current entry to its parent
		if let Some(parent) = entry.maybe_parent {
			*paths_and_sizes.entry(parent).or_default() += entry_size;
		}

		if indexed_paths.len() >= limit as usize {
			break;
		}
	}

	let (walked, to_update) = filter_existing_paths(indexed_paths, file_paths_db_fetcher).await?;

	Ok(WalkResult {
		walked,
		to_update,
		to_walk,
		to_remove: to_remove.into_iter().flatten(),
		errors,
		paths_and_sizes,
	})
}

pub(super) async fn keep_walking<FilePathDBFetcherFut, ToRemoveDbFetcherFut>(
	to_walk_entry: &ToWalkEntry,
	indexer_rules: &[IndexerRule],
	mut update_notifier: impl FnMut(&Path, usize),
	file_paths_db_fetcher: impl Fn(Vec<file_path::WhereParam>) -> FilePathDBFetcherFut,
	to_remove_db_fetcher: impl Fn(
		IsolatedFilePathData<'static>,
		Vec<file_path::WhereParam>,
	) -> ToRemoveDbFetcherFut,
	iso_file_path_factory: impl Fn(&Path, bool) -> Result<IsolatedFilePathData<'static>, IndexerError>,
) -> Result<
	WalkResult<
		impl Iterator<Item = WalkedEntry>,
		impl Iterator<Item = WalkedEntry>,
		impl Iterator<Item = file_path_pub_and_cas_ids::Data>,
	>,
	IndexerError,
>
where
	FilePathDBFetcherFut: Future<Output = Result<Vec<file_path_walker::Data>, IndexerError>>,
	ToRemoveDbFetcherFut:
		Future<Output = Result<Vec<file_path_pub_and_cas_ids::Data>, IndexerError>>,
{
	let mut to_keep_walking = VecDeque::with_capacity(TO_WALK_QUEUE_INITIAL_CAPACITY);
	let mut indexed_paths = HashSet::with_capacity(WALK_SINGLE_DIR_PATHS_BUFFER_INITIAL_CAPACITY);
	let mut paths_buffer = HashSet::with_capacity(WALK_SINGLE_DIR_PATHS_BUFFER_INITIAL_CAPACITY);
	let mut errors = vec![];

	let (to_walk_entry_size, to_remove) = inner_walk_single_dir(
		to_walk_entry.path.clone(),
		to_walk_entry,
		indexer_rules,
		&mut update_notifier,
		&to_remove_db_fetcher,
		&iso_file_path_factory,
		WorkingTable {
			indexed_paths: &mut indexed_paths,
			paths_buffer: &mut paths_buffer,
			maybe_to_walk: Some(&mut to_keep_walking),
			errors: &mut errors,
		},
	)
	.await;

	let (walked, to_update) = filter_existing_paths(indexed_paths, file_paths_db_fetcher).await?;

	Ok(WalkResult {
		walked,
		to_update,
		to_walk: to_keep_walking,
		to_remove: to_remove.into_iter(),
		errors,
		paths_and_sizes: [
			Some((to_walk_entry.path.clone(), to_walk_entry_size)),
			to_walk_entry
				.maybe_parent
				.as_ref()
				.map(|parent_path| (parent_path.clone(), to_walk_entry_size)),
		]
		.into_iter()
		.flatten()
		.collect(),
	})
}

pub(super) async fn walk_single_dir<FilePathDBFetcherFut, ToRemoveDbFetcherFut>(
	root: impl AsRef<Path>,
	indexer_rules: &[IndexerRule],
	mut update_notifier: impl FnMut(&Path, usize) + '_,
	file_paths_db_fetcher: impl Fn(Vec<file_path::WhereParam>) -> FilePathDBFetcherFut,
	to_remove_db_fetcher: impl Fn(
		IsolatedFilePathData<'static>,
		Vec<file_path::WhereParam>,
	) -> ToRemoveDbFetcherFut,
	iso_file_path_factory: impl Fn(&Path, bool) -> Result<IsolatedFilePathData<'static>, IndexerError>,
	add_root: bool,
) -> Result<
	(
		impl Iterator<Item = WalkedEntry>,
		impl Iterator<Item = WalkedEntry>,
		Vec<file_path_pub_and_cas_ids::Data>,
		Vec<IndexerError>,
		u64,
	),
	IndexerError,
>
where
	FilePathDBFetcherFut: Future<Output = Result<Vec<file_path_walker::Data>, IndexerError>>,
	ToRemoveDbFetcherFut:
		Future<Output = Result<Vec<file_path_pub_and_cas_ids::Data>, IndexerError>>,
{
	let root = root.as_ref();

	let mut indexed_paths = HashSet::with_capacity(WALK_SINGLE_DIR_PATHS_BUFFER_INITIAL_CAPACITY);

	if add_root {
		let metadata = fs::metadata(root)
			.await
			.map_err(|e| FileIOError::from((root, e)))?;

		indexed_paths.insert(WalkingEntry {
			iso_file_path: iso_file_path_factory(root, true)?,
			maybe_metadata: Some(FilePathMetadata::from_path(&root, &metadata).await?),
		});
	}

	let mut paths_buffer = HashSet::with_capacity(WALK_SINGLE_DIR_PATHS_BUFFER_INITIAL_CAPACITY);
	let mut errors = vec![];

	let (root_size, to_remove) = inner_walk_single_dir(
		root,
		&ToWalkEntry {
			path: root.to_path_buf(),
			parent_dir_accepted_by_its_children: None,
			maybe_parent: None,
		},
		indexer_rules,
		&mut update_notifier,
		&to_remove_db_fetcher,
		&iso_file_path_factory,
		WorkingTable {
			indexed_paths: &mut indexed_paths,
			paths_buffer: &mut paths_buffer,
			maybe_to_walk: None,
			errors: &mut errors,
		},
	)
	.await;

	let (walked, to_update) = filter_existing_paths(indexed_paths, file_paths_db_fetcher).await?;

	Ok((walked, to_update, to_remove, errors, root_size))
}

async fn filter_existing_paths<F>(
	indexed_paths: HashSet<WalkingEntry>,
	file_paths_db_fetcher: impl Fn(Vec<file_path::WhereParam>) -> F,
) -> Result<
	(
		impl Iterator<Item = WalkedEntry>,
		impl Iterator<Item = WalkedEntry>,
	),
	IndexerError,
>
where
	F: Future<Output = Result<Vec<file_path_walker::Data>, IndexerError>>,
{
	if !indexed_paths.is_empty() {
		file_paths_db_fetcher(
			indexed_paths
				.iter()
				.map(|entry| &entry.iso_file_path)
				.map(Into::into)
				.collect(),
		)
		.await
	} else {
		Ok(vec![])
	}
	.map(move |file_paths| {
		let isolated_paths_already_in_db = file_paths
			.into_iter()
			.flat_map(|file_path| {
				IsolatedFilePathData::try_from(file_path.clone())
					.map(|iso_file_path| (iso_file_path, file_path))
			})
			.collect::<HashMap<_, _>>();

		let mut to_update = vec![];

		let to_create = indexed_paths
			.into_iter()
			.filter_map(|entry| {
				if let Some(file_path) = isolated_paths_already_in_db.get(&entry.iso_file_path) {
					if let (Some(metadata), Some(inode), Some(date_modified)) = (
						&entry.maybe_metadata,
						&file_path.inode,
						&file_path.date_modified,
					) {
						if (
								inode_from_db(&inode[0..8]) != metadata.inode
								// Datetimes stored in DB loses a bit of precision, so we need to check against a delta
								// instead of using != operator
								|| DateTime::<FixedOffset>::from(metadata.modified_at) - *date_modified
									> Duration::milliseconds(1) || file_path.hidden.is_none() || metadata.hidden != file_path.hidden.unwrap_or_default()
							)
							// We ignore the size of directories because it is not reliable, we need to
							// calculate it ourselves later
							&& !(
								entry.iso_file_path.is_dir
								&& metadata.size_in_bytes
									!= file_path
										.size_in_bytes_bytes
										.as_ref()
										.map(|size_in_bytes_bytes| {
											u64::from_be_bytes([
												size_in_bytes_bytes[0],
												size_in_bytes_bytes[1],
												size_in_bytes_bytes[2],
												size_in_bytes_bytes[3],
												size_in_bytes_bytes[4],
												size_in_bytes_bytes[5],
												size_in_bytes_bytes[6],
												size_in_bytes_bytes[7],
											])
										})
										.unwrap_or_default()
								) {
							to_update.push(
								(sd_utils::from_bytes_to_uuid(&file_path.pub_id), file_path.object_id, entry).into(),
							);
						}
					}

					None
				} else {
					Some(entry.into())
				}
			})
			.collect::<Vec<_>>();

		(to_create.into_iter(), to_update.into_iter())
	})
}

struct WorkingTable<'a> {
	indexed_paths: &'a mut HashSet<WalkingEntry>,
	paths_buffer: &'a mut HashSet<WalkingEntry>,
	maybe_to_walk: Option<&'a mut VecDeque<ToWalkEntry>>,
	errors: &'a mut Vec<IndexerError>,
}

async fn inner_walk_single_dir<ToRemoveDbFetcherFut>(
	root: impl AsRef<Path>,
	ToWalkEntry {
		path,
		parent_dir_accepted_by_its_children,
		..
	}: &ToWalkEntry,
	indexer_rules: &[IndexerRule],
	update_notifier: &mut impl FnMut(&Path, usize),
	to_remove_db_fetcher: impl Fn(
		IsolatedFilePathData<'static>,
		Vec<file_path::WhereParam>,
	) -> ToRemoveDbFetcherFut,
	iso_file_path_factory: &impl Fn(&Path, bool) -> Result<IsolatedFilePathData<'static>, IndexerError>,
	WorkingTable {
		indexed_paths,
		paths_buffer,
		mut maybe_to_walk,
		errors,
	}: WorkingTable<'_>,
) -> (u64, Vec<file_path_pub_and_cas_ids::Data>)
where
	ToRemoveDbFetcherFut:
		Future<Output = Result<Vec<file_path_pub_and_cas_ids::Data>, IndexerError>>,
{
	let Ok(iso_file_path_to_walk) = iso_file_path_factory(path, true).map_err(|e| errors.push(e))
	else {
		return (0, vec![]);
	};

	let Ok(mut read_dir) = fs::read_dir(path)
		.await
		.map_err(|e| errors.push(FileIOError::from((path.clone(), e)).into()))
	else {
		return (0, vec![]);
	};

	let root = root.as_ref();

	// Just to make sure...
	paths_buffer.clear();

	let mut found_paths_counts = 0;

	// Marking with a loop label here in case of rejection or errors, to continue with next entry
	'entries: loop {
		let entry = match read_dir.next_entry().await {
			Ok(Some(entry)) => entry,
			Ok(None) => break,
			Err(e) => {
				errors.push(FileIOError::from((path.clone(), e)).into());
				continue;
			}
		};

		// Accept by children has three states,
		// None if we don't now yet or if this check doesn't apply
		// Some(true) if this check applies and it passes
		// Some(false) if this check applies and it was rejected
		// and we pass the current parent state to its children
		let mut accept_by_children_dir = *parent_dir_accepted_by_its_children;

		let current_path = entry.path();

		// Just sending updates if we found more paths since the last loop
		let current_found_paths_count = paths_buffer.len();
		if found_paths_counts != current_found_paths_count {
			update_notifier(
				&current_path,
				indexed_paths.len() + current_found_paths_count,
			);
			found_paths_counts = current_found_paths_count;
		}

		trace!(
			"Current filesystem path: {}, accept_by_children_dir: {:#?}",
			current_path.display(),
			accept_by_children_dir
		);

		let Ok(rules_per_kind) = IndexerRule::apply_all(indexer_rules, &current_path)
			.await
			.map_err(|e| errors.push(e.into()))
		else {
			continue 'entries;
		};

		if rules_per_kind
			.get(&RuleKind::RejectFilesByGlob)
			.map_or(false, |reject_results| {
				reject_results.iter().any(|reject| !reject)
			}) {
			trace!(
				"Path {} rejected by `RuleKind::RejectFilesByGlob`",
				current_path.display()
			);
			continue 'entries;
		}

		let Ok(metadata) = entry
			.metadata()
			.await
			.map_err(|e| errors.push(FileIOError::from((&current_path, e)).into()))
		else {
			continue 'entries;
		};

		// TODO: Hard ignoring symlinks for now, but this should be configurable
		if metadata.is_symlink() {
			continue 'entries;
		}

		let is_dir = metadata.is_dir();

		if is_dir {
			// If it is a directory, first we check if we must reject it and its children entirely
			if rules_per_kind
				.get(&RuleKind::RejectIfChildrenDirectoriesArePresent)
				.map_or(false, |reject_results| {
					reject_results.iter().any(|reject| !reject)
				}) {
				trace!(
					"Path {} rejected by rule `RuleKind::RejectIfChildrenDirectoriesArePresent`",
					current_path.display(),
				);
				continue 'entries;
			}

			// Then we check if we must accept it and its children
			if let Some(accept_by_children_rules) =
				rules_per_kind.get(&RuleKind::AcceptIfChildrenDirectoriesArePresent)
			{
				if accept_by_children_rules.iter().any(|accept| *accept) {
					accept_by_children_dir = Some(true);
				}

				// If it wasn't accepted then we mark as rejected
				if accept_by_children_dir.is_none() {
					trace!(
						"Path {} rejected because it didn't passed in any AcceptIfChildrenDirectoriesArePresent rule",
						current_path.display()
					);
					accept_by_children_dir = Some(false);
				}
			}

			// Then we mark this directory the be walked in too
			if let Some(ref mut to_walk) = maybe_to_walk {
				to_walk.push_back(ToWalkEntry {
					path: current_path.clone(),
					parent_dir_accepted_by_its_children: accept_by_children_dir,
					maybe_parent: Some(path.clone()),
				});
			}
		}

		if rules_per_kind
			.get(&RuleKind::AcceptFilesByGlob)
			.map_or(false, |accept_rules| {
				accept_rules.iter().all(|accept| !accept)
			}) {
			trace!(
				"Path {} reject because it didn't passed in any AcceptFilesByGlob rules",
				current_path.display()
			);
			continue 'entries;
		}

		if accept_by_children_dir.unwrap_or(true) {
			let Ok(iso_file_path) =
				iso_file_path_factory(&current_path, is_dir).map_err(|e| errors.push(e))
			else {
				continue 'entries;
			};

			let Ok(metadata) = FilePathMetadata::from_path(&current_path, &metadata)
				.await
				.map_err(|e| errors.push(e.into()))
			else {
				continue;
			};

			paths_buffer.insert(WalkingEntry {
				iso_file_path,
				maybe_metadata: Some(metadata),
			});

			// If the ancestors directories wasn't indexed before, now we do
			for ancestor in current_path
				.ancestors()
				.skip(1) // Skip the current directory as it was already indexed
				.take_while(|&ancestor| ancestor != root)
			{
				let Ok(iso_file_path) =
					iso_file_path_factory(ancestor, true).map_err(|e| errors.push(e))
				else {
					// Checking the next ancestor, as this one we got an error
					continue;
				};

				let mut ancestor_iso_walking_entry = WalkingEntry {
					iso_file_path,
					maybe_metadata: None,
				};
				trace!("Indexing ancestor {}", ancestor.display());
				if !indexed_paths.contains(&ancestor_iso_walking_entry) {
					let Ok(metadata) = fs::metadata(ancestor)
						.await
						.map_err(|e| errors.push(FileIOError::from((&ancestor, e)).into()))
					else {
						// Checking the next ancestor, as this one we got an error
						continue;
					};

					let Ok(metadata) = FilePathMetadata::from_path(&ancestor, &metadata)
						.await
						.map_err(|e| errors.push(e.into()))
					else {
						continue;
					};

					ancestor_iso_walking_entry.maybe_metadata = Some(metadata);

					paths_buffer.insert(ancestor_iso_walking_entry);
				} else {
					// If indexed_paths contains the current ancestors, then it will contain
					// also all if its ancestors too, so we can stop here
					break;
				}
			}
		}
	}

	// We continue the function even if we fail to fetch `file_path`s to remove,
	// the DB will have old `file_path`s but at least this is better than
	// don't adding the newly indexed paths
	let to_remove = to_remove_db_fetcher(
		iso_file_path_to_walk,
		paths_buffer
			.iter()
			.map(|entry| &entry.iso_file_path)
			.map(Into::into)
			.collect(),
	)
	.await
	.unwrap_or_else(|e| {
		errors.push(e);
		vec![]
	});

	let mut to_walk_entry_size = 0;

	// Just merging the `found_paths` with `indexed_paths` here in the end to avoid possibly
	// multiple rehashes during function execution
	indexed_paths.extend(paths_buffer.drain().map(|walking_entry| {
		if let Some(metadata) = &walking_entry.maybe_metadata {
			to_walk_entry_size += metadata.size_in_bytes;
		}
		walking_entry
	}));

	(to_walk_entry_size, to_remove)
}

#[cfg(test)]
#[allow(clippy::unwrap_used, clippy::panic)]
mod tests {
	use super::super::rules::RulePerKind;
	use super::*;
	use chrono::Utc;
	use globset::{Glob, GlobSetBuilder};
	use tempfile::{tempdir, TempDir};
	use tokio::fs;
	// use tracing_test::traced_test;

	impl PartialEq for WalkedEntry {
		fn eq(&self, other: &Self) -> bool {
			self.iso_file_path == other.iso_file_path
		}
	}

	impl Eq for WalkedEntry {}

	impl Hash for WalkedEntry {
		fn hash<H: Hasher>(&self, state: &mut H) {
			self.iso_file_path.hash(state);
		}
	}

	async fn prepare_location() -> TempDir {
		let root = tempdir().unwrap();
		let root_path = root.path();
		let rust_project = root_path.join("rust_project");
		let inner_project = root_path.join("inner");
		let node_project = inner_project.join("node_project");
		let photos = root_path.join("photos");

		fs::create_dir(&rust_project).await.unwrap();
		fs::create_dir(&inner_project).await.unwrap();
		fs::create_dir(&node_project).await.unwrap();
		fs::create_dir(&photos).await.unwrap();

		// Making rust and node projects a git repository
		fs::create_dir(rust_project.join(".git")).await.unwrap();
		fs::create_dir(node_project.join(".git")).await.unwrap();

		// Populating rust project
		fs::File::create(rust_project.join("Cargo.toml"))
			.await
			.unwrap();
		let rust_src_dir = rust_project.join("src");
		fs::create_dir(&rust_src_dir).await.unwrap();
		fs::File::create(rust_src_dir.join("main.rs"))
			.await
			.unwrap();
		let rust_target_dir = rust_project.join("target");
		fs::create_dir(&rust_target_dir).await.unwrap();
		let rust_build_dir = rust_target_dir.join("debug");
		fs::create_dir(&rust_build_dir).await.unwrap();
		fs::File::create(rust_build_dir.join("main")).await.unwrap();

		// Populating node project
		fs::File::create(node_project.join("package.json"))
			.await
			.unwrap();
		let node_src_dir = node_project.join("src");
		fs::create_dir(&node_src_dir).await.unwrap();
		fs::File::create(node_src_dir.join("App.tsx"))
			.await
			.unwrap();
		let node_modules = node_project.join("node_modules");
		fs::create_dir(&node_modules).await.unwrap();
		let node_modules_dep = node_modules.join("react");
		fs::create_dir(&node_modules_dep).await.unwrap();
		fs::File::create(node_modules_dep.join("package.json"))
			.await
			.unwrap();

		// Photos directory
		for photo in ["photo1.png", "photo2.jpg", "photo3.jpeg", "text.txt"].iter() {
			fs::File::create(photos.join(photo)).await.unwrap();
		}

		root
	}

	#[tokio::test]
	async fn test_walk_without_rules() {
		let root = prepare_location().await;
		let root_path = root.path();

		let metadata = FilePathMetadata {
			inode: 0,
			size_in_bytes: 0,
			created_at: Utc::now(),
			modified_at: Utc::now(),
			hidden: false,
		};

		let f = |path, is_dir| IsolatedFilePathData::new(0, root_path, path, is_dir).unwrap();
		let pub_id = Uuid::new_v4();
		let maybe_object_id = None;

		#[rustfmt::skip]
		let expected = [
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/.git"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/Cargo.toml"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/src"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/src/main.rs"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/target"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/target/debug"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/target/debug/main"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/.git"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/package.json"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/src"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/src/App.tsx"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/node_modules"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/node_modules/react"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/node_modules/react/package.json"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos/photo1.png"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos/photo2.jpg"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos/photo3.jpeg"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos/text.txt"), false), metadata },
		]
		.into_iter()
		.collect::<HashSet<_>>();

		let walk_result = walk(
			root_path.to_path_buf(),
			&[],
			|_, _| {},
			|_| async { Ok(vec![]) },
			|_, _| async { Ok(vec![]) },
			|path, is_dir| {
				IsolatedFilePathData::new(0, root_path, path, is_dir).map_err(Into::into)
			},
			420,
		)
		.await
		.unwrap();

		if !walk_result.errors.is_empty() {
			panic!("errors: {:#?}", walk_result.errors);
		}

		let actual = walk_result.walked.collect::<HashSet<_>>();

		if actual != expected {
			panic!("difference: {:#?}", expected.difference(&actual));
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn test_only_photos() {
		let root = prepare_location().await;
		let root_path = root.path();

		let metadata = FilePathMetadata {
			inode: 0,
			size_in_bytes: 0,
			created_at: Utc::now(),
			modified_at: Utc::now(),
			hidden: false,
		};

		let f = |path, is_dir| IsolatedFilePathData::new(0, root_path, path, is_dir).unwrap();
		let pub_id = Uuid::new_v4();
		let maybe_object_id = None;

		#[rustfmt::skip]
		let expected = [
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos/photo1.png"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos/photo2.jpg"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("photos/photo3.jpeg"), false), metadata },
		]
		.into_iter()
		.collect::<HashSet<_>>();

		let only_photos_rule = &[IndexerRule::new(
			"only photos".to_string(),
			false,
			vec![RulePerKind::AcceptFilesByGlob(
				vec![],
				GlobSetBuilder::new()
					.add(Glob::new("{*.png,*.jpg,*.jpeg}").unwrap())
					.build()
					.unwrap(),
			)],
		)];

		let walk_result = walk(
			root_path.to_path_buf(),
			only_photos_rule,
			|_, _| {},
			|_| async { Ok(vec![]) },
			|_, _| async { Ok(vec![]) },
			|path, is_dir| {
				IsolatedFilePathData::new(0, root_path, path, is_dir).map_err(Into::into)
			},
			420,
		)
		.await
		.unwrap();

		if !walk_result.errors.is_empty() {
			panic!("errors: {:#?}", walk_result.errors);
		}

		let actual = walk_result.walked.collect::<HashSet<_>>();

		if actual != expected {
			panic!("difference: {:#?}", expected.difference(&actual));
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn test_git_repos() {
		let root = prepare_location().await;
		let root_path = root.path();

		let metadata = FilePathMetadata {
			inode: 0,
			size_in_bytes: 0,
			created_at: Utc::now(),
			modified_at: Utc::now(),
			hidden: false,
		};

		let f = |path, is_dir| IsolatedFilePathData::new(0, root_path, path, is_dir).unwrap();
		let pub_id = Uuid::new_v4();
		let maybe_object_id = None;

		#[rustfmt::skip]
		let expected = [
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/.git"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/Cargo.toml"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/src"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/src/main.rs"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/target"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/target/debug"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/target/debug/main"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/.git"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/package.json"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/src"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/src/App.tsx"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/node_modules"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/node_modules/react"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/node_modules/react/package.json"), false), metadata },
		]
		.into_iter()
		.collect::<HashSet<_>>();

		let git_repos = &[IndexerRule::new(
			"git repos".to_string(),
			false,
			vec![RulePerKind::AcceptIfChildrenDirectoriesArePresent(
				[".git".to_string()].into_iter().collect(),
			)],
		)];

		let walk_result = walk(
			root_path.to_path_buf(),
			git_repos,
			|_, _| {},
			|_| async { Ok(vec![]) },
			|_, _| async { Ok(vec![]) },
			|path, is_dir| {
				IsolatedFilePathData::new(0, root_path, path, is_dir).map_err(Into::into)
			},
			420,
		)
		.await
		.unwrap();

		if !walk_result.errors.is_empty() {
			panic!("errors: {:#?}", walk_result.errors);
		}

		let actual = walk_result.walked.collect::<HashSet<_>>();

		if actual != expected {
			panic!("difference: {:#?}", expected.difference(&actual));
		}
	}

	#[tokio::test]
	// #[traced_test]
	async fn git_repos_without_deps_or_build_dirs() {
		let root = prepare_location().await;
		let root_path = root.path();

		let metadata = FilePathMetadata {
			inode: 0,
			size_in_bytes: 0,
			created_at: Utc::now(),
			modified_at: Utc::now(),
			hidden: false,
		};

		let f = |path, is_dir| IsolatedFilePathData::new(0, root_path, path, is_dir).unwrap();
		let pub_id = Uuid::new_v4();
		let maybe_object_id = None;

		#[rustfmt::skip]
		let expected = [
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/.git"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/Cargo.toml"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/src"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("rust_project/src/main.rs"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/.git"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/package.json"), false), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/src"), true), metadata },
			WalkedEntry { pub_id, maybe_object_id, iso_file_path: f(root_path.join("inner/node_project/src/App.tsx"), false), metadata },
		]
		.into_iter()
		.collect::<HashSet<_>>();

		let git_repos_no_deps_no_build_dirs = &[
			IndexerRule::new(
				"git repos".to_string(),
				false,
				vec![RulePerKind::AcceptIfChildrenDirectoriesArePresent(
					[".git".to_string()].into_iter().collect(),
				)],
			),
			IndexerRule::new(
				"reject node_modules".to_string(),
				false,
				vec![RulePerKind::RejectFilesByGlob(
					vec![],
					GlobSetBuilder::new()
						.add(Glob::new("{**/node_modules/*,**/node_modules}").unwrap())
						.build()
						.unwrap(),
				)],
			),
			IndexerRule::new(
				"reject rust build dir".to_string(),
				false,
				vec![RulePerKind::RejectFilesByGlob(
					vec![],
					GlobSetBuilder::new()
						.add(Glob::new("{**/target/*,**/target}").unwrap())
						.build()
						.unwrap(),
				)],
			),
		];

		let walk_result = walk(
			root_path.to_path_buf(),
			git_repos_no_deps_no_build_dirs,
			|_, _| {},
			|_| async { Ok(vec![]) },
			|_, _| async { Ok(vec![]) },
			|path, is_dir| {
				IsolatedFilePathData::new(0, root_path, path, is_dir).map_err(Into::into)
			},
			420,
		)
		.await
		.unwrap();

		if !walk_result.errors.is_empty() {
			panic!("errors: {:#?}", walk_result.errors);
		}

		let actual = walk_result.walked.collect::<HashSet<_>>();

		if actual != expected {
			panic!("difference: {:#?}", expected.difference(&actual));
		}
	}
}



File: ./src/location/file_path_helper/isolated_file_path_data.rs
-------------------------------------------------
use crate::{
	prisma::{file_path, location},
	util::error::NonUtf8PathError,
};

use std::{
	borrow::Cow,
	fmt,
	path::{Path, PathBuf, MAIN_SEPARATOR, MAIN_SEPARATOR_STR},
	sync::OnceLock,
};

use regex::RegexSet;
use serde::{Deserialize, Serialize};

use super::{
	file_path_for_file_identifier, file_path_for_media_processor, file_path_for_object_validator,
	file_path_to_full_path, file_path_to_handle_custom_uri, file_path_to_handle_p2p_serve_file,
	file_path_to_isolate, file_path_to_isolate_with_id, file_path_walker, file_path_with_object,
	FilePathError,
};

static FORBIDDEN_FILE_NAMES: OnceLock<RegexSet> = OnceLock::new();

#[derive(Serialize, Deserialize, Debug, Hash, Eq, PartialEq)]
#[non_exhaustive]
pub struct IsolatedFilePathData<'a> {
	// WARN! These fields MUST NOT be changed outside the location module, that's why they have this visibility
	// and are not public. They have some specific logic on them and should not be writen to directly.
	// If you wanna access one of them outside from location module, write yourself an accessor method
	// to have read only access to them.
	pub(in crate::location) location_id: location::id::Type,
	pub(in crate::location) materialized_path: Cow<'a, str>,
	pub(in crate::location) is_dir: bool,
	pub(in crate::location) name: Cow<'a, str>,
	pub(in crate::location) extension: Cow<'a, str>,
	relative_path: Cow<'a, str>,
}

impl IsolatedFilePathData<'static> {
	pub fn new(
		location_id: location::id::Type,
		location_path: impl AsRef<Path>,
		full_path: impl AsRef<Path>,
		is_dir: bool,
	) -> Result<Self, FilePathError> {
		let full_path = full_path.as_ref();
		let location_path = location_path.as_ref();

		let extension = (!is_dir)
			.then(|| {
				full_path
					.extension()
					.and_then(|ext| ext.to_str().map(str::to_string))
					.unwrap_or_default()
			})
			.unwrap_or_default();

		Ok(Self {
			is_dir,
			location_id,
			materialized_path: Cow::Owned(extract_normalized_materialized_path_str(
				location_id,
				location_path,
				full_path,
			)?),
			name: Cow::Owned(
				(location_path != full_path)
					.then(|| Self::prepare_name(full_path, is_dir).to_string())
					.unwrap_or_default(),
			),
			extension: Cow::Owned(extension),
			relative_path: Cow::Owned(extract_relative_path(
				location_id,
				location_path,
				full_path,
			)?),
		})
	}
}

impl<'a> IsolatedFilePathData<'a> {
	pub fn location_id(&self) -> location::id::Type {
		self.location_id
	}

	pub fn extension(&self) -> &str {
		self.extension.as_ref()
	}

	pub fn is_root(&self) -> bool {
		self.is_dir
			&& self.materialized_path == "/"
			&& self.name.is_empty()
			&& self.relative_path.is_empty()
	}

	pub fn parent(&'a self) -> Self {
		let (parent_path_str, name, relative_path) = if self.materialized_path == "/" {
			("/", "", "")
		} else {
			let trailing_slash_idx = self.materialized_path.len() - 1;
			let last_slash_idx = self.materialized_path[..trailing_slash_idx]
				.rfind('/')
				.expect("malformed materialized path at `parent` method");

			(
				&self.materialized_path[..last_slash_idx + 1],
				&self.materialized_path[last_slash_idx + 1..trailing_slash_idx],
				&self.materialized_path[1..trailing_slash_idx],
			)
		};

		Self {
			is_dir: true,
			location_id: self.location_id,
			relative_path: Cow::Borrowed(relative_path),
			materialized_path: Cow::Borrowed(parent_path_str),
			name: Cow::Borrowed(name),
			extension: Cow::Borrowed(""),
		}
	}

	pub fn from_relative_str(
		location_id: location::id::Type,
		relative_file_path_str: &'a str,
	) -> Self {
		let is_dir = relative_file_path_str.ends_with('/');

		let (materialized_path, maybe_name, maybe_extension) =
			Self::separate_path_name_and_extension_from_str(relative_file_path_str, is_dir);

		Self {
			location_id,
			materialized_path: Cow::Borrowed(materialized_path),
			is_dir,
			name: maybe_name.map(Cow::Borrowed).unwrap_or_default(),
			extension: maybe_extension.map(Cow::Borrowed).unwrap_or_default(),
			relative_path: Cow::Borrowed(relative_file_path_str),
		}
	}

	pub fn full_name(&self) -> String {
		if self.extension.is_empty() {
			self.name.to_string()
		} else {
			format!("{}.{}", self.name, self.extension)
		}
	}

	pub fn materialized_path_for_children(&self) -> Option<String> {
		if self.materialized_path == "/" && self.name.is_empty() && self.is_dir {
			// We're at the root file_path
			Some("/".to_string())
		} else {
			self.is_dir
				.then(|| format!("{}{}/", self.materialized_path, self.name))
		}
	}

	pub fn separate_name_and_extension_from_str(
		source: &'a str,
	) -> Result<(&'a str, &'a str), FilePathError> {
		if source.contains(MAIN_SEPARATOR) {
			return Err(FilePathError::InvalidFilenameAndExtension(
				source.to_string(),
			));
		}

		if let Some(last_dot_idx) = source.rfind('.') {
			if last_dot_idx == 0 {
				// The dot is the first character, so it's a hidden file
				Ok((source, ""))
			} else {
				Ok((&source[..last_dot_idx], &source[last_dot_idx + 1..]))
			}
		} else {
			// It's a file without extension
			Ok((source, ""))
		}
	}

	pub fn accept_file_name(name: &str) -> bool {
		let reg = {
			// Maybe we should enforce windows more restrictive rules on all platforms?
			#[cfg(target_os = "windows")]
			{
				FORBIDDEN_FILE_NAMES.get_or_init(|| {
					RegexSet::new([
						r"(?i)^(CON|PRN|AUX|NUL|COM[1-9]|LPT[1-9])(\.\w+)*$",
						r#"[<>:"/\\|?*\u0000-\u0031]"#,
					])
					.expect("this regex should always be valid")
				})
			}

			#[cfg(not(target_os = "windows"))]
			{
				FORBIDDEN_FILE_NAMES.get_or_init(|| {
					RegexSet::new([r"/|\x00"]).expect("this regex should always be valid")
				})
			}
		};

		!reg.is_match(name)
	}

	pub fn separate_path_name_and_extension_from_str(
		source: &'a str,
		is_dir: bool,
	) -> (
		&'a str,         // Materialized path
		Option<&'a str>, // Maybe a name
		Option<&'a str>, // Maybe an extension
	) {
		let length = source.len();

		if length == 1 {
			// The case for the root path
			(source, None, None)
		} else if is_dir {
			let last_char_idx = if source.ends_with('/') {
				length - 1
			} else {
				length
			};

			let first_name_char_idx = source[..last_char_idx].rfind('/').unwrap_or(0) + 1;
			(
				&source[..first_name_char_idx],
				Some(&source[first_name_char_idx..last_char_idx]),
				None,
			)
		} else {
			let first_name_char_idx = source.rfind('/').unwrap_or(0) + 1;
			let end_idx = first_name_char_idx - 1;
			if let Some(last_dot_relative_idx) = source[first_name_char_idx..].rfind('.') {
				let last_dot_idx = first_name_char_idx + last_dot_relative_idx;
				(
					&source[..end_idx],
					Some(&source[first_name_char_idx..last_dot_idx]),
					Some(&source[last_dot_idx + 1..]),
				)
			} else {
				(
					&source[..end_idx],
					Some(&source[first_name_char_idx..]),
					None,
				)
			}
		}
	}

	fn prepare_name(path: &Path, is_dir: bool) -> &str {
		// Not using `impl AsRef<Path>` here because it's an private method
		if is_dir {
			path.file_name()
		} else {
			path.file_stem()
		}
		.unwrap_or_default()
		.to_str()
		.unwrap_or_default()
	}

	pub fn from_db_data(
		location_id: location::id::Type,
		is_dir: bool,
		materialized_path: Cow<'a, str>,
		name: Cow<'a, str>,
		extension: Cow<'a, str>,
	) -> Self {
		Self {
			relative_path: Cow::Owned(assemble_relative_path(
				&materialized_path,
				&name,
				&extension,
				is_dir,
			)),
			location_id,
			materialized_path,
			is_dir,
			name,
			extension,
		}
	}
}

impl AsRef<Path> for IsolatedFilePathData<'_> {
	fn as_ref(&self) -> &Path {
		Path::new(self.relative_path.as_ref())
	}
}

impl From<IsolatedFilePathData<'static>> for file_path::UniqueWhereParam {
	fn from(path: IsolatedFilePathData<'static>) -> Self {
		Self::LocationIdMaterializedPathNameExtensionEquals(
			path.location_id,
			path.materialized_path.into_owned(),
			path.name.into_owned(),
			path.extension.into_owned(),
		)
	}
}

impl From<IsolatedFilePathData<'static>> for file_path::WhereParam {
	fn from(path: IsolatedFilePathData<'static>) -> Self {
		Self::And(vec![
			file_path::location_id::equals(Some(path.location_id)),
			file_path::materialized_path::equals(Some(path.materialized_path.into_owned())),
			file_path::name::equals(Some(path.name.into_owned())),
			file_path::extension::equals(Some(path.extension.into_owned())),
		])
	}
}

impl From<&IsolatedFilePathData<'_>> for file_path::UniqueWhereParam {
	fn from(path: &IsolatedFilePathData<'_>) -> Self {
		Self::LocationIdMaterializedPathNameExtensionEquals(
			path.location_id,
			path.materialized_path.to_string(),
			path.name.to_string(),
			path.extension.to_string(),
		)
	}
}

impl From<&IsolatedFilePathData<'_>> for file_path::WhereParam {
	fn from(path: &IsolatedFilePathData<'_>) -> Self {
		Self::And(vec![
			file_path::location_id::equals(Some(path.location_id)),
			file_path::materialized_path::equals(Some(path.materialized_path.to_string())),
			file_path::name::equals(Some(path.name.to_string())),
			file_path::extension::equals(Some(path.extension.to_string())),
		])
	}
}

impl fmt::Display for IsolatedFilePathData<'_> {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		write!(f, "{}", self.relative_path)
	}
}

#[macro_use]
mod macros {
	macro_rules! impl_from_db {
		($($file_path_kind:ident),+ $(,)?) => {
			$(
				impl ::std::convert::TryFrom<$file_path_kind::Data> for $crate::
					location::
					file_path_helper::
					isolated_file_path_data::
					IsolatedFilePathData<'static>
				{
                    type Error = $crate::util::db::MissingFieldError;

					fn try_from(path: $file_path_kind::Data) -> Result<Self, Self::Error> {
                        use $crate::util::db::maybe_missing;
                        use ::std::borrow::Cow;

                        Ok(Self::from_db_data(
                            maybe_missing(path.location_id, "file_path.location_id")?,
                            maybe_missing(path.is_dir, "file_path.is_dir")?,
                            Cow::Owned(maybe_missing(path.materialized_path, "file_path.materialized_path")?),
                            Cow::Owned(maybe_missing(path.name, "file_path.name")?),
                            Cow::Owned(maybe_missing(path.extension, "file_path.extension")?)
                        ))
					}
				}

				impl<'a> ::std::convert::TryFrom<&'a $file_path_kind::Data> for $crate::
					location::
					file_path_helper::
					isolated_file_path_data::
					IsolatedFilePathData<'a>
				{
                    type Error = $crate::util::db::MissingFieldError;

					fn try_from(path: &'a $file_path_kind::Data) -> Result<Self, Self::Error> {
                        use $crate::util::db::maybe_missing;
                        use ::std::borrow::Cow;

						Ok(Self::from_db_data(
							maybe_missing(path.location_id, "file_path.location_id")?,
                            maybe_missing(path.is_dir, "file_path.is_dir")?,
							Cow::Borrowed(maybe_missing(&path.materialized_path, "file_path.materialized_path")?),
							Cow::Borrowed(maybe_missing(&path.name, "file_path.name")?),
							Cow::Borrowed(maybe_missing(&path.extension, "file_path.extension")?)
						))
					}
				}
			)+
		};
	}

	macro_rules! impl_from_db_without_location_id {
		($($file_path_kind:ident),+ $(,)?) => {
			$(
				impl ::std::convert::TryFrom<($crate::prisma::location::id::Type, $file_path_kind::Data)> for $crate::
					location::
					file_path_helper::
					isolated_file_path_data::
					IsolatedFilePathData<'static>
				{
                    type Error = $crate::util::db::MissingFieldError;

					fn try_from((location_id, path): ($crate::prisma::location::id::Type, $file_path_kind::Data)) -> Result<Self, Self::Error> {
                        use $crate::util::db::maybe_missing;
                        use ::std::borrow::Cow;

                        Ok(Self::from_db_data(
                            location_id,
                            maybe_missing(path.is_dir, "file_path.is_dir")?,
                            Cow::Owned(maybe_missing(path.materialized_path, "file_path.materialized_path")?),
                            Cow::Owned(maybe_missing(path.name, "file_path.name")?),
                            Cow::Owned(maybe_missing(path.extension, "file_path.extension")?)
                        ))
					}
				}

				impl<'a> ::std::convert::TryFrom<($crate::prisma::location::id::Type, &'a $file_path_kind::Data)> for $crate::
					location::
					file_path_helper::
					isolated_file_path_data::
					IsolatedFilePathData<'a>
				{
                    type Error = $crate::util::db::MissingFieldError;

					fn try_from((location_id, path): ($crate::prisma::location::id::Type, &'a $file_path_kind::Data)) -> Result<Self, Self::Error> {
                        use $crate::util::db::maybe_missing;
                        use ::std::borrow::Cow;

						Ok(Self::from_db_data(
							location_id,
                            maybe_missing(path.is_dir, "file_path.is_dir")?,
							Cow::Borrowed(maybe_missing(&path.materialized_path, "file_path.materialized_path")?),
							Cow::Borrowed(maybe_missing(&path.name, "file_path.name")?),
							Cow::Borrowed(maybe_missing(&path.extension, "file_path.extension")?)
						))
					}
				}
			)+
		};
	}
}

impl_from_db!(
	file_path,
	file_path_to_isolate,
	file_path_walker,
	file_path_to_isolate_with_id,
	file_path_with_object
);

impl_from_db_without_location_id!(
	file_path_for_file_identifier,
	file_path_to_full_path,
	file_path_for_media_processor,
	file_path_for_object_validator,
	file_path_to_handle_custom_uri,
	file_path_to_handle_p2p_serve_file
);

fn extract_relative_path(
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	path: impl AsRef<Path>,
) -> Result<String, FilePathError> {
	let path = path.as_ref();

	path.strip_prefix(location_path)
		.map_err(|_| FilePathError::UnableToExtractMaterializedPath {
			location_id,
			path: path.into(),
		})
		.and_then(|relative| {
			relative
				.to_str()
				.map(|relative_str| relative_str.replace('\\', "/"))
				.ok_or_else(|| NonUtf8PathError(path.into()).into())
		})
}

/// This function separates a file path from a location path, and normalizes replacing '\' with '/'
/// to be consistent between Windows and Unix like systems
pub fn extract_normalized_materialized_path_str(
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	path: impl AsRef<Path>,
) -> Result<String, FilePathError> {
	let path = path.as_ref();

	path.strip_prefix(location_path)
		.map_err(|_| FilePathError::UnableToExtractMaterializedPath {
			location_id,
			path: path.into(),
		})?
		.parent()
		.map(|materialized_path| {
			materialized_path
				.to_str()
				.map(|materialized_path_str| {
					if !materialized_path_str.is_empty() {
						format!("/{}/", materialized_path_str.replace('\\', "/"))
					} else {
						"/".to_string()
					}
				})
				.ok_or_else(|| NonUtf8PathError(path.into()))
		})
		.unwrap_or_else(|| Ok("/".to_string()))
		.map_err(Into::into)
}

fn assemble_relative_path(
	materialized_path: &str,
	name: &str,
	extension: &str,
	is_dir: bool,
) -> String {
	match (is_dir, extension) {
		(false, extension) if !extension.is_empty() => {
			format!("{}{}.{}", &materialized_path[1..], name, extension)
		}
		(_, _) => format!("{}{}", &materialized_path[1..], name),
	}
}

pub fn join_location_relative_path(
	location_path: impl AsRef<Path>,
	relative_path: impl AsRef<Path>,
) -> PathBuf {
	let relative_path = relative_path.as_ref();

	location_path
		.as_ref()
		.join(if relative_path.starts_with(MAIN_SEPARATOR_STR) {
			relative_path
				.strip_prefix(MAIN_SEPARATOR_STR)
				.expect("just checked")
		} else {
			relative_path
		})
}

pub fn push_location_relative_path(
	mut location_path: PathBuf,
	relative_path: impl AsRef<Path>,
) -> PathBuf {
	let relative_path = relative_path.as_ref();

	location_path.push(if relative_path.starts_with(MAIN_SEPARATOR_STR) {
		relative_path
			.strip_prefix(MAIN_SEPARATOR_STR)
			.expect("just checked")
	} else {
		relative_path
	});

	location_path
}

#[cfg(test)]
#[allow(clippy::unwrap_used)]
mod tests {
	use super::*;

	fn expected(
		materialized_path: &'static str,
		is_dir: bool,
		name: &'static str,
		extension: &'static str,
		relative_path: &'static str,
	) -> IsolatedFilePathData<'static> {
		IsolatedFilePathData {
			location_id: 1,
			materialized_path: materialized_path.into(),
			is_dir,
			name: name.into(),
			extension: extension.into(),
			relative_path: relative_path.into(),
		}
	}

	#[test]
	fn new_method() {
		let tester = |full_path, is_dir, expected, msg| {
			let actual =
				IsolatedFilePathData::new(1, "/spacedrive/location", full_path, is_dir).unwrap();
			assert_eq!(actual, expected, "{msg}");
		};

		tester(
			"/spacedrive/location",
			true,
			expected("/", true, "", "", ""),
			"the location root directory",
		);

		tester(
			"/spacedrive/location/file.txt",
			false,
			expected("/", false, "file", "txt", "file.txt"),
			"a file in the root directory",
		);

		tester(
			"/spacedrive/location/dir",
			true,
			expected("/", true, "dir", "", "dir"),
			"a directory in the root directory",
		);

		tester(
			"/spacedrive/location/dir/file.txt",
			false,
			expected("/dir/", false, "file", "txt", "dir/file.txt"),
			"a directory with a file inside",
		);

		tester(
			"/spacedrive/location/dir/dir2",
			true,
			expected("/dir/", true, "dir2", "", "dir/dir2"),
			"a directory in a directory",
		);

		tester(
			"/spacedrive/location/dir/dir2/dir3",
			true,
			expected("/dir/dir2/", true, "dir3", "", "dir/dir2/dir3"),
			"3 level of directories",
		);

		tester(
			"/spacedrive/location/dir/dir2/dir3/file.txt",
			false,
			expected(
				"/dir/dir2/dir3/",
				false,
				"file",
				"txt",
				"dir/dir2/dir3/file.txt",
			),
			"a file inside a third level directory",
		);
	}

	#[test]
	fn parent_method() {
		let tester = |full_path, is_dir, expected, msg| {
			let child =
				IsolatedFilePathData::new(1, "/spacedrive/location", full_path, is_dir).unwrap();

			let actual = child.parent();
			assert_eq!(actual, expected, "{msg}");
		};

		tester(
			"/spacedrive/location",
			true,
			expected("/", true, "", "", ""),
			"the location root directory",
		);

		tester(
			"/spacedrive/location/file.txt",
			false,
			expected("/", true, "", "", ""),
			"a file in the root directory",
		);

		tester(
			"/spacedrive/location/dir",
			true,
			expected("/", true, "", "", ""),
			"a directory in the root directory",
		);

		tester(
			"/spacedrive/location/dir/file.txt",
			false,
			expected("/", true, "dir", "", "dir"),
			"a directory with a file inside",
		);

		tester(
			"/spacedrive/location/dir/dir2",
			true,
			expected("/", true, "dir", "", "dir"),
			"a directory in a directory",
		);

		tester(
			"/spacedrive/location/dir/dir2/dir3",
			true,
			expected("/dir/", true, "dir2", "", "dir/dir2"),
			"3 level of directories",
		);

		tester(
			"/spacedrive/location/dir/dir2/dir3/file.txt",
			false,
			expected("/dir/dir2/", true, "dir3", "", "dir/dir2/dir3"),
			"a file inside a third level directory",
		);
	}

	#[test]
	fn extract_normalized_materialized_path() {
		let tester = |path, expected, msg| {
			let actual =
				extract_normalized_materialized_path_str(1, "/spacedrive/location", path).unwrap();
			assert_eq!(actual, expected, "{msg}");
		};

		tester("/spacedrive/location", "/", "the location root directory");
		tester(
			"/spacedrive/location/file.txt",
			"/",
			"a file in the root directory",
		);
		tester(
			"/spacedrive/location/dir",
			"/",
			"a directory in the root directory",
		);
		tester(
			"/spacedrive/location/dir/file.txt",
			"/dir/",
			"a directory with a file inside",
		);
		tester(
			"/spacedrive/location/dir/dir2",
			"/dir/",
			"a directory in a directory",
		);
		tester(
			"/spacedrive/location/dir/dir2/dir3",
			"/dir/dir2/",
			"3 level of directories",
		);
		tester(
			"/spacedrive/location/dir/dir2/dir3/file.txt",
			"/dir/dir2/dir3/",
			"a file inside a third level directory",
		);
	}
}



File: ./src/location/file_path_helper/mod.rs
-------------------------------------------------
use crate::{
	prisma::{file_path, location, PrismaClient},
	util::error::{FileIOError, NonUtf8PathError},
};

use std::{
	fs::Metadata,
	path::{Path, PathBuf, MAIN_SEPARATOR_STR},
	time::SystemTime,
};

use chrono::{DateTime, Utc};
use prisma_client_rust::QueryError;
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::{fs, io};
use tracing::error;

pub mod isolated_file_path_data;

pub use isolated_file_path_data::{
	join_location_relative_path, push_location_relative_path, IsolatedFilePathData,
};

// File Path selectables!
file_path::select!(file_path_pub_and_cas_ids { pub_id cas_id });
file_path::select!(file_path_just_pub_id_materialized_path {
	pub_id
	materialized_path
});
file_path::select!(file_path_for_file_identifier {
	id
	pub_id
	materialized_path
	date_created
	is_dir
	name
	extension
	object_id
});
file_path::select!(file_path_for_object_validator {
	pub_id
	materialized_path
	is_dir
	name
	extension
	integrity_checksum
});
file_path::select!(file_path_for_media_processor {
	id
	materialized_path
	is_dir
	name
	extension
	cas_id
	object_id
});
file_path::select!(file_path_to_isolate {
	location_id
	materialized_path
	is_dir
	name
	extension
});
file_path::select!(file_path_to_isolate_with_id {
	id
	location_id
	materialized_path
	is_dir
	name
	extension
});
file_path::select!(file_path_walker {
	pub_id
	location_id
	object_id
	materialized_path
	is_dir
	name
	extension
	date_modified
	inode
	size_in_bytes_bytes
	hidden
});
file_path::select!(file_path_to_handle_custom_uri {
	pub_id
	materialized_path
	is_dir
	name
	extension
	location: select {
		id
		path
		instance: select {
			identity
		}
	}
});
file_path::select!(file_path_to_handle_p2p_serve_file {
	materialized_path
	name
	extension
	is_dir // For isolated file path
	location: select {
		id
		path
	}
});
file_path::select!(file_path_to_full_path {
	id
	materialized_path
	is_dir
	name
	extension
	location: select {
		id
		path
	}
});

// File Path includes!
file_path::include!(file_path_with_object { object });

#[derive(Clone, Copy, Debug, Serialize, Deserialize)]
pub struct FilePathMetadata {
	pub inode: u64,
	pub size_in_bytes: u64,
	pub created_at: DateTime<Utc>,
	pub modified_at: DateTime<Utc>,
	pub hidden: bool,
}

pub fn path_is_hidden(path: impl AsRef<Path>, metadata: &Metadata) -> bool {
	#[cfg(target_family = "unix")]
	{
		use std::ffi::OsStr;
		let _ = metadata; // just to avoid warnings on Linux
		if path
			.as_ref()
			.file_name()
			.and_then(OsStr::to_str)
			.map(|s| s.starts_with('.'))
			.unwrap_or_default()
		{
			return true;
		}
	}

	#[cfg(target_os = "macos")]
	{
		use std::os::macos::fs::MetadataExt;

		// https://developer.apple.com/library/archive/documentation/FileManagement/Conceptual/FileSystemProgrammingGuide/FileSystemDetails/FileSystemDetails.html#:~:text=UF_HIDDEN
		const UF_HIDDEN: u32 = 0x8000;

		if (metadata.st_flags() & UF_HIDDEN) == UF_HIDDEN {
			return true;
		}
	}

	#[cfg(target_family = "windows")]
	{
		use std::os::windows::fs::MetadataExt;

		const FILE_ATTRIBUTE_HIDDEN: u32 = 0x2;

		let _ = path; // just to avoid warnings on Windows

		if (metadata.file_attributes() & FILE_ATTRIBUTE_HIDDEN) == FILE_ATTRIBUTE_HIDDEN {
			return true;
		}
	}

	false
}

impl FilePathMetadata {
	pub async fn from_path(
		path: impl AsRef<Path>,
		metadata: &Metadata,
	) -> Result<Self, FilePathError> {
		let inode = {
			#[cfg(target_family = "unix")]
			{
				get_inode(metadata)
			}

			#[cfg(target_family = "windows")]
			{
				get_inode_from_path(path.as_ref()).await?
			}
		};

		Ok(Self {
			inode,
			hidden: path_is_hidden(path.as_ref(), metadata),
			size_in_bytes: metadata.len(),
			created_at: metadata.created_or_now().into(),
			modified_at: metadata.modified_or_now().into(),
		})
	}
}

#[derive(Error, Debug)]
pub enum FilePathError {
	#[error("file path not found: <id='{0}'>")]
	IdNotFound(file_path::id::Type),
	#[error("file Path not found: <path='{}'>", .0.display())]
	NotFound(Box<Path>),
	#[error("location '{0}' not found")]
	LocationNotFound(location::id::Type),
	#[error("received an invalid sub path: <location_path='{}', sub_path='{}'>", .location_path.display(), .sub_path.display())]
	InvalidSubPath {
		location_path: Box<Path>,
		sub_path: Box<Path>,
	},
	#[error("sub path is not a directory: <path='{}'>", .0.display())]
	SubPathNotDirectory(Box<Path>),
	#[error(
		"the parent directory of the received sub path isn't indexed in the location: <id='{}', sub_path='{}'>",
		.location_id,
		.sub_path.display()
	)]
	SubPathParentNotInLocation {
		location_id: location::id::Type,
		sub_path: Box<Path>,
	},
	#[error("unable to extract materialized path from location: <id='{}', path='{}'>", .location_id, .path.display())]
	UnableToExtractMaterializedPath {
		location_id: location::id::Type,
		path: Box<Path>,
	},
	#[error("database error: {0}")]
	Database(#[from] QueryError),

	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	NonUtf8Path(#[from] NonUtf8PathError),
	#[error("received an invalid filename and extension: <filename_and_extension='{0}'>")]
	InvalidFilenameAndExtension(String),
}

#[cfg(feature = "location-watcher")]
pub async fn create_file_path(
	crate::location::Library { db, sync, .. }: &crate::location::Library,
	IsolatedFilePathData {
		materialized_path,
		is_dir,
		location_id,
		name,
		extension,
		..
	}: IsolatedFilePathData<'_>,
	cas_id: Option<String>,
	metadata: FilePathMetadata,
) -> Result<file_path::Data, FilePathError> {
	use crate::util::db::inode_to_db;

	use sd_prisma::{prisma, prisma_sync};
	use sd_sync::OperationFactory;
	use serde_json::json;
	use uuid::Uuid;

	let indexed_at = Utc::now();

	let location = db
		.location()
		.find_unique(location::id::equals(location_id))
		.select(location::select!({ id pub_id }))
		.exec()
		.await?
		.ok_or(FilePathError::LocationNotFound(location_id))?;

	let params = {
		use file_path::*;

		vec![
			(
				location::NAME,
				json!(prisma_sync::location::SyncId {
					pub_id: location.pub_id
				}),
			),
			(cas_id::NAME, json!(cas_id)),
			(materialized_path::NAME, json!(materialized_path)),
			(name::NAME, json!(name)),
			(extension::NAME, json!(extension)),
			(
				size_in_bytes_bytes::NAME,
				json!(metadata.size_in_bytes.to_be_bytes().to_vec()),
			),
			(inode::NAME, json!(metadata.inode.to_le_bytes())),
			(is_dir::NAME, json!(is_dir)),
			(date_created::NAME, json!(metadata.created_at)),
			(date_modified::NAME, json!(metadata.modified_at)),
			(date_indexed::NAME, json!(indexed_at)),
		]
	};

	let pub_id = sd_utils::uuid_to_bytes(Uuid::new_v4());

	let created_path = sync
		.write_ops(
			db,
			(
				sync.shared_create(
					prisma_sync::file_path::SyncId {
						pub_id: pub_id.clone(),
					},
					params,
				),
				db.file_path().create(pub_id, {
					use file_path::*;
					vec![
						location::connect(prisma::location::id::equals(location.id)),
						materialized_path::set(Some(materialized_path.into_owned())),
						name::set(Some(name.into_owned())),
						extension::set(Some(extension.into_owned())),
						inode::set(Some(inode_to_db(metadata.inode))),
						cas_id::set(cas_id),
						is_dir::set(Some(is_dir)),
						size_in_bytes_bytes::set(Some(
							metadata.size_in_bytes.to_be_bytes().to_vec(),
						)),
						date_created::set(Some(metadata.created_at.into())),
						date_modified::set(Some(metadata.modified_at.into())),
						date_indexed::set(Some(indexed_at.into())),
						hidden::set(Some(metadata.hidden)),
					]
				}),
			),
		)
		.await?;

	Ok(created_path)
}

pub fn filter_existing_file_path_params(
	IsolatedFilePathData {
		materialized_path,
		is_dir,
		location_id,
		name,
		extension,
		..
	}: &IsolatedFilePathData,
) -> Vec<file_path::WhereParam> {
	vec![
		file_path::location_id::equals(Some(*location_id)),
		file_path::materialized_path::equals(Some(materialized_path.to_string())),
		file_path::is_dir::equals(Some(*is_dir)),
		file_path::name::equals(Some(name.to_string())),
		file_path::extension::equals(Some(extension.to_string())),
	]
}

/// With this function we try to do a loose filtering of file paths, to avoid having to do check
/// twice for directories and for files. This is because directories have a trailing `/` or `\` in
/// the materialized path
#[allow(unused)]
pub fn loose_find_existing_file_path_params(
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	full_path: impl AsRef<Path>,
) -> Result<Vec<file_path::WhereParam>, FilePathError> {
	let location_path = location_path.as_ref();
	let full_path = full_path.as_ref();

	let file_iso_file_path =
		IsolatedFilePathData::new(location_id, location_path, full_path, false)?;

	let dir_iso_file_path = IsolatedFilePathData::new(location_id, location_path, full_path, true)?;

	Ok(vec![
		file_path::location_id::equals(Some(location_id)),
		file_path::materialized_path::equals(Some(
			file_iso_file_path.materialized_path.to_string(),
		)),
		file_path::name::in_vec(vec![
			file_iso_file_path.name.to_string(),
			dir_iso_file_path.name.to_string(),
		]),
		file_path::extension::in_vec(vec![
			file_iso_file_path.extension.to_string(),
			dir_iso_file_path.extension.to_string(),
		]),
	])
}

pub async fn ensure_sub_path_is_in_location(
	location_path: impl AsRef<Path>,
	sub_path: impl AsRef<Path>,
) -> Result<PathBuf, FilePathError> {
	let mut sub_path = sub_path.as_ref();
	let location_path = location_path.as_ref();
	if sub_path.starts_with(MAIN_SEPARATOR_STR) {
		if sub_path == Path::new(MAIN_SEPARATOR_STR) {
			// We're dealing with the location root path here
			return Ok(location_path.to_path_buf());
		}
		// SAFETY: we just checked that it starts with the separator
		sub_path = sub_path
			.strip_prefix(MAIN_SEPARATOR_STR)
			.expect("we just checked that it starts with the separator");
	}

	if !sub_path.starts_with(location_path) {
		// If the sub_path doesn't start with the location_path, we have to check if it's a
		// materialized path received from the frontend, then we check if the full path exists
		let full_path = location_path.join(sub_path);

		match fs::metadata(&full_path).await {
			Ok(_) => Ok(full_path),
			Err(e) if e.kind() == io::ErrorKind::NotFound => Err(FilePathError::InvalidSubPath {
				sub_path: sub_path.into(),
				location_path: location_path.into(),
			}),
			Err(e) => Err(FileIOError::from((full_path, e)).into()),
		}
	} else {
		Ok(sub_path.to_path_buf())
	}
}

pub async fn ensure_file_path_exists<E>(
	sub_path: impl AsRef<Path>,
	iso_file_path: &IsolatedFilePathData<'_>,
	db: &PrismaClient,
	error_fn: impl FnOnce(Box<Path>) -> E,
) -> Result<(), E>
where
	E: From<QueryError>,
{
	if !check_file_path_exists(iso_file_path, db).await? {
		Err(error_fn(sub_path.as_ref().into()))
	} else {
		Ok(())
	}
}

pub async fn check_file_path_exists<E>(
	iso_file_path: &IsolatedFilePathData<'_>,
	db: &PrismaClient,
) -> Result<bool, E>
where
	E: From<QueryError>,
{
	Ok(iso_file_path.is_root()
		|| db
			.file_path()
			.count(filter_existing_file_path_params(iso_file_path))
			.exec()
			.await? > 0)
}

pub async fn ensure_sub_path_is_directory(
	location_path: impl AsRef<Path>,
	sub_path: impl AsRef<Path>,
) -> Result<(), FilePathError> {
	let mut sub_path = sub_path.as_ref();

	if sub_path == Path::new(MAIN_SEPARATOR_STR) {
		// Sub path for the location root path is always a directory
		return Ok(());
	}

	match fs::metadata(sub_path).await {
		Ok(meta) => {
			if meta.is_file() {
				Err(FilePathError::SubPathNotDirectory(sub_path.into()))
			} else {
				Ok(())
			}
		}
		Err(e) if e.kind() == io::ErrorKind::NotFound => {
			if sub_path.starts_with("/") {
				// SAFETY: we just checked that it starts with the separator
				sub_path = sub_path
					.strip_prefix("/")
					.expect("we just checked that it starts with the separator");
			}

			let location_path = location_path.as_ref();
			let full_path = location_path.join(sub_path);
			match fs::metadata(&full_path).await {
				Ok(meta) => {
					if meta.is_file() {
						Err(FilePathError::SubPathNotDirectory(sub_path.into()))
					} else {
						Ok(())
					}
				}
				Err(e) if e.kind() == io::ErrorKind::NotFound => {
					Err(FilePathError::InvalidSubPath {
						sub_path: sub_path.into(),
						location_path: location_path.into(),
					})
				}
				Err(e) => Err(FileIOError::from((full_path, e)).into()),
			}
		}
		Err(e) => Err(FileIOError::from((sub_path, e)).into()),
	}
}

#[allow(unused)] // TODO remove this annotation when we can use it on windows
pub fn get_inode(metadata: &Metadata) -> u64 {
	#[cfg(target_family = "unix")]
	{
		use std::os::unix::fs::MetadataExt;

		metadata.ino()
	}

	#[cfg(target_family = "windows")]
	{
		// TODO use this when it's stable and remove winapi-utils dependency

		// use std::os::windows::fs::MetadataExt;

		//
		// 	metadata
		// 		.file_index()
		// 		.expect("This function must not be called from a `DirEntry`'s `Metadata")
		//

		todo!("Use metadata: {:#?}", metadata)
	}
}

#[allow(unused)]
pub async fn get_inode_from_path(path: impl AsRef<Path>) -> Result<u64, FilePathError> {
	#[cfg(target_family = "unix")]
	{
		// TODO use this when it's stable and remove winapi-utils dependency
		let metadata = fs::metadata(path.as_ref())
			.await
			.map_err(|e| FileIOError::from((path, e)))?;

		Ok(get_inode(&metadata))
	}

	#[cfg(target_family = "windows")]
	{
		use winapi_util::{file::information, Handle};

		let info = Handle::from_path_any(path.as_ref())
			.and_then(|ref handle| information(handle))
			.map_err(|e| FileIOError::from((path, e)))?;

		Ok(info.file_index())
	}
}

pub trait MetadataExt {
	fn created_or_now(&self) -> SystemTime;

	fn modified_or_now(&self) -> SystemTime;
}

impl MetadataExt for Metadata {
	fn created_or_now(&self) -> SystemTime {
		self.created().unwrap_or_else(|_| SystemTime::now())
	}

	fn modified_or_now(&self) -> SystemTime {
		self.modified().unwrap_or_else(|_| SystemTime::now())
	}
}



File: ./src/custom_uri/mpsc_to_async_write.rs
-------------------------------------------------
use std::{
	io,
	pin::Pin,
	task::{Context, Poll},
};

use bytes::Bytes;
use tokio::io::AsyncWrite;
use tokio_util::sync::PollSender;

/// Allowing wrapping an `mpsc::Sender` into an `AsyncWrite`
pub struct MpscToAsyncWrite(PollSender<io::Result<Bytes>>);

impl MpscToAsyncWrite {
	pub fn new(sender: PollSender<io::Result<Bytes>>) -> Self {
		Self(sender)
	}
}

impl AsyncWrite for MpscToAsyncWrite {
	fn poll_write(
		mut self: Pin<&mut Self>,
		cx: &mut Context<'_>,
		buf: &[u8],
	) -> Poll<Result<usize, io::Error>> {
		#[allow(clippy::unwrap_used)]
		match self.0.poll_reserve(cx) {
			Poll::Ready(Ok(())) => {
				self.0.send_item(Ok(Bytes::from(buf.to_vec()))).unwrap();
				Poll::Ready(Ok(buf.len()))
			}
			Poll::Ready(Err(_)) => todo!(),
			Poll::Pending => Poll::Pending,
		}
	}

	fn poll_flush(self: Pin<&mut Self>, _cx: &mut Context<'_>) -> Poll<Result<(), io::Error>> {
		Poll::Ready(Ok(()))
	}

	fn poll_shutdown(self: Pin<&mut Self>, _cx: &mut Context<'_>) -> Poll<Result<(), io::Error>> {
		Poll::Ready(Ok(()))
	}
}



File: ./src/custom_uri/mod.rs
-------------------------------------------------
use crate::{
	api::{utils::InvalidateOperationEvent, CoreEvent},
	library::Library,
	location::file_path_helper::{file_path_to_handle_custom_uri, IsolatedFilePathData},
	object::media::thumbnail::WEBP_EXTENSION,
	p2p::{operations, IdentityOrRemoteIdentity},
	prisma::{file_path, location},
	util::{db::*, InfallibleResponse},
	Node,
};

use std::{
	cmp::min,
	ffi::OsStr,
	fmt::Debug,
	fs::Metadata,
	io::{self, SeekFrom},
	path::{Path, PathBuf},
	str::FromStr,
	sync::{atomic::Ordering, Arc},
};

use async_stream::stream;
use axum::{
	body::{self, Body, BoxBody, Full, StreamBody},
	extract::{self, State},
	http::{HeaderValue, Request, Response, StatusCode},
	middleware,
	routing::get,
	Router,
};
use bytes::Bytes;

use mini_moka::sync::Cache;
use sd_file_ext::text::is_text;
use sd_p2p::{spaceblock::Range, spacetunnel::RemoteIdentity};
use tokio::{
	fs::{self, File},
	io::{AsyncReadExt, AsyncSeekExt},
};
use tokio_util::sync::PollSender;
use tracing::error;
use uuid::Uuid;

use self::{mpsc_to_async_write::MpscToAsyncWrite, serve_file::serve_file, utils::*};

mod async_read_body;
mod mpsc_to_async_write;
mod serve_file;
mod utils;

type CacheKey = (Uuid, file_path::id::Type);

#[derive(Debug, Clone)]
struct CacheValue {
	name: PathBuf,
	ext: String,
	file_path_pub_id: Uuid,
	serve_from: ServeFrom,
}

const MAX_TEXT_READ_LENGTH: usize = 10 * 1024; // 10KB

#[derive(Debug, Clone)]
pub enum ServeFrom {
	/// Serve from the local filesystem
	Local,
	/// Serve from a specific instance
	Remote(RemoteIdentity),
}

#[derive(Clone)]
struct LocalState {
	node: Arc<Node>,

	// This LRU cache allows us to avoid doing a DB lookup on every request.
	// The main advantage of this LRU Cache is for video files. Video files are fetch in multiple chunks and the cache prevents a DB lookup on every chunk reducing the request time from 15-25ms to 1-10ms.
	// TODO: We should listen to events when deleting or moving a location and evict the cache accordingly.
	file_metadata_cache: Arc<Cache<CacheKey, CacheValue>>,
}

type ExtractedPath = extract::Path<(String, String, String)>;

async fn get_or_init_lru_entry(
	state: &LocalState,
	extract::Path((lib_id, loc_id, path_id)): ExtractedPath,
) -> Result<(CacheValue, Arc<Library>), Response<BoxBody>> {
	let library_id = Uuid::from_str(&lib_id).map_err(bad_request)?;
	let location_id = loc_id.parse::<location::id::Type>().map_err(bad_request)?;
	let file_path_id = path_id
		.parse::<file_path::id::Type>()
		.map_err(bad_request)?;

	let lru_cache_key = (library_id, file_path_id);
	let library = state
		.node
		.libraries
		.get_library(&library_id)
		.await
		.ok_or_else(|| internal_server_error(()))?;

	if let Some(entry) = state.file_metadata_cache.get(&lru_cache_key) {
		Ok((entry, library))
	} else {
		let file_path = library
			.db
			.file_path()
			.find_unique(file_path::id::equals(file_path_id))
			// TODO: This query could be seen as a security issue as it could load the private key (`identity`) when we 100% don't need it. We are gonna wanna fix that!
			.select(file_path_to_handle_custom_uri::select())
			.exec()
			.await
			.map_err(internal_server_error)?
			.ok_or_else(|| not_found(()))?;

		let location = maybe_missing(&file_path.location, "file_path.location")
			.map_err(internal_server_error)?;
		let path = maybe_missing(&location.path, "file_path.location.path")
			.map_err(internal_server_error)?;
		let instance = maybe_missing(&location.instance, "file_path.location.instance")
			.map_err(internal_server_error)?;

		let path = Path::new(path)
			.join(IsolatedFilePathData::try_from((location_id, &file_path)).map_err(not_found)?);

		let identity = IdentityOrRemoteIdentity::from_bytes(&instance.identity)
			.map_err(internal_server_error)?
			.remote_identity();

		let lru_entry = CacheValue {
			name: path,
			ext: maybe_missing(file_path.extension, "extension").map_err(not_found)?,
			file_path_pub_id: Uuid::from_slice(&file_path.pub_id).map_err(internal_server_error)?,
			serve_from: if identity == library.identity.to_remote_identity() {
				ServeFrom::Local
			} else {
				ServeFrom::Remote(identity)
			},
		};

		state
			.file_metadata_cache
			.insert(lru_cache_key, lru_entry.clone());

		Ok((lru_entry, library))
	}
}

// We are using Axum on all platforms because Tauri's custom URI protocols can't be async!
pub fn router(node: Arc<Node>) -> Router<()> {
	Router::new()
		.route(
			"/thumbnail/*path",
			get(
				|State(state): State<LocalState>,
				 extract::Path(path): extract::Path<String>,
				 request: Request<Body>| async move {
					let thumbnail_path = state.node.config.data_directory().join("thumbnails");
					let path = thumbnail_path.join(path);

					// Prevent directory traversal attacks (Eg. requesting `../../../etc/passwd`)
					// For now we only support `webp` thumbnails.
					(path.starts_with(&thumbnail_path)
						&& path.extension() == Some(WEBP_EXTENSION.as_ref()))
					.then_some(())
					.ok_or_else(|| not_found(()))?;

					let file = File::open(&path).await.map_err(|err| {
						InfallibleResponse::builder()
							.status(if err.kind() == io::ErrorKind::NotFound {
								StatusCode::NOT_FOUND
							} else {
								StatusCode::INTERNAL_SERVER_ERROR
							})
							.body(body::boxed(Full::from("")))
					})?;
					let metadata = file.metadata().await;
					serve_file(
						file,
						metadata,
						request.into_parts().0,
						InfallibleResponse::builder()
							.header("Content-Type", HeaderValue::from_static("image/webp")),
					)
					.await
				},
			),
		)
		.route(
			"/file/:lib_id/:loc_id/:path_id",
			get(
				|State(state): State<LocalState>, path: ExtractedPath, request: Request<Body>| async move {
					let (
						CacheValue {
							name: file_path_full_path,
							ext: extension,
							file_path_pub_id,
							serve_from,
							..
						},
						library,
					) = get_or_init_lru_entry(&state, path).await?;

					match serve_from {
						ServeFrom::Local => {
							let metadata = fs::metadata(&file_path_full_path)
								.await
								.map_err(internal_server_error)?;
							(!metadata.is_dir())
								.then_some(())
								.ok_or_else(|| not_found(()))?;

							let mut file =
								File::open(&file_path_full_path).await.map_err(|err| {
									InfallibleResponse::builder()
										.status(if err.kind() == io::ErrorKind::NotFound {
											StatusCode::NOT_FOUND
										} else {
											StatusCode::INTERNAL_SERVER_ERROR
										})
										.body(body::boxed(Full::from("")))
								})?;

							let resp = InfallibleResponse::builder().header(
								"Content-Type",
								HeaderValue::from_str(
									&infer_the_mime_type(&extension, &mut file, &metadata).await?,
								)
								.map_err(|err| {
									error!("Error converting mime-type into header value: {}", err);
									internal_server_error(())
								})?,
							);

							serve_file(file, Ok(metadata), request.into_parts().0, resp).await
						}
						ServeFrom::Remote(identity) => {
							if !state.node.files_over_p2p_flag.load(Ordering::Relaxed) {
								return Ok(not_found(()));
							}

							// TODO: Support `Range` requests and `ETag` headers
							match state.node.p2p.get_library_service(&library.id) {
								Some(service) => {
									let stream = service
										.connect(state.node.p2p.manager.clone(), &identity)
										.await
										.map_err(|err| {
											not_found(format!(
												"Error connecting to {identity}: {err:?}"
											))
										})?;

									let (tx, mut rx) =
										tokio::sync::mpsc::channel::<io::Result<Bytes>>(150);
									// TODO: We only start a thread because of stupid `ManagerStreamAction2` and libp2p's `!Send/!Sync` bounds on a stream.
									tokio::spawn(async move {
										let Ok(()) = operations::request_file(
											stream,
											&library,
											file_path_pub_id,
											Range::Full,
											MpscToAsyncWrite::new(PollSender::new(tx)),
										)
										.await
										else {
											return;
										};
									});

									// TODO: Content Type
									Ok(InfallibleResponse::builder().status(StatusCode::OK).body(
										body::boxed(StreamBody::new(stream! {
											while let Some(item) = rx.recv().await {
												yield item;
											}
										})),
									))
								}
								None => Ok(not_found(())),
							}
						}
					}
				},
			),
		)
		.route(
			"/local-file-by-path/:path",
			get(
				|extract::Path(path): extract::Path<String>, request: Request<Body>| async move {
					let path = PathBuf::from(path);

					let metadata = fs::metadata(&path).await.map_err(internal_server_error)?;
					(!metadata.is_dir())
						.then_some(())
						.ok_or_else(|| not_found(()))?;

					let mut file = File::open(&path).await.map_err(|err| {
						InfallibleResponse::builder()
							.status(if err.kind() == io::ErrorKind::NotFound {
								StatusCode::NOT_FOUND
							} else {
								StatusCode::INTERNAL_SERVER_ERROR
							})
							.body(body::boxed(Full::from("")))
					})?;

					let resp = InfallibleResponse::builder().header(
						"Content-Type",
						HeaderValue::from_str(&match path.extension().and_then(OsStr::to_str) {
							None => "text/plain".to_string(),
							Some(ext) => infer_the_mime_type(ext, &mut file, &metadata).await?,
						})
						.map_err(|err| {
							error!("Error converting mime-type into header value: {}", err);
							internal_server_error(())
						})?,
					);

					serve_file(file, Ok(metadata), request.into_parts().0, resp).await
				},
			),
		)
		.route_layer(middleware::from_fn(cors_middleware))
		.with_state({
			let file_metadata_cache = Arc::new(Cache::new(150));

			tokio::spawn({
				let file_metadata_cache = file_metadata_cache.clone();
				let mut tx = node.event_bus.0.subscribe();
				async move {
					while let Ok(event) = tx.recv().await {
						if let CoreEvent::InvalidateOperation(e) = event {
							match e {
								InvalidateOperationEvent::Single(event) => {
									// TODO: This is inefficent as any change will invalidate who cache. We need the new invalidation system!!!
									// TODO: It's also error prone and a fine-grained resource based invalidation system would avoid that.
									if event.key == "search.objects" || event.key == "search.paths"
									{
										file_metadata_cache.invalidate_all();
									}
								}
								InvalidateOperationEvent::All => {
									file_metadata_cache.invalidate_all();
								}
							}
						}
					}
				}
			});

			LocalState {
				node,
				file_metadata_cache,
			}
		})
}

// TODO: This should possibly be determined from magic bytes when the file is indexed and stored it in the DB on the file path
async fn infer_the_mime_type(
	ext: &str,
	file: &mut File,
	metadata: &Metadata,
) -> Result<String, Response<BoxBody>> {
	let ext = ext.to_lowercase();
	let mime_type = match ext.as_str() {
		// AAC audio
		"aac" => "audio/aac",
		// Musical Instrument Digital Interface (MIDI)
		"mid" | "midi" => "audio/midi, audio/x-midi",
		// MP3 audio
		"mp3" => "audio/mpeg",
		// MP4 audio
		"m4a" => "audio/mp4",
		// OGG audio
		"oga" => "audio/ogg",
		// Opus audio
		"opus" => "audio/opus",
		// Waveform Audio Format
		"wav" => "audio/wav",
		// WEBM audio
		"weba" => "audio/webm",
		// AVI: Audio Video Interleave
		"avi" => "video/x-msvideo",
		// MP4 video
		"mp4" | "m4v" => "video/mp4",
		// TODO: Bruh
		#[cfg(not(target_os = "macos"))]
		// TODO: Bruh
		// FIX-ME: This media types break macOS video rendering
		// MPEG transport stream
		"ts" => "video/mp2t",
		// TODO: Bruh
		#[cfg(not(target_os = "macos"))]
		// FIX-ME: This media types break macOS video rendering
		// MPEG Video
		"mpeg" => "video/mpeg",
		// OGG video
		"ogv" => "video/ogg",
		// WEBM video
		"webm" => "video/webm",
		// 3GPP audio/video container (TODO: audio/3gpp if it doesn't contain video)
		"3gp" => "video/3gpp",
		// 3GPP2 audio/video container (TODO: audio/3gpp2 if it doesn't contain video)
		"3g2" => "video/3gpp2",
		// Quicktime movies
		"mov" => "video/quicktime",
		// Windows OS/2 Bitmap Graphics
		"bmp" => "image/bmp",
		// Graphics Interchange Format (GIF)
		"gif" => "image/gif",
		// Icon format
		"ico" => "image/vnd.microsoft.icon",
		// JPEG images
		"jpeg" | "jpg" => "image/jpeg",
		// Portable Network Graphics
		"png" => "image/png",
		// Scalable Vector Graphics (SVG)
		"svg" => "image/svg+xml",
		// Tagged Image File Format (TIFF)
		"tif" | "tiff" => "image/tiff",
		// WEBP image
		"webp" => "image/webp",
		// PDF document
		"pdf" => "application/pdf",
		// HEIF images
		"heif" => "image/heif",
		// HEIF images sequence (animated)
		"heifs" => "image/heif-sequence",
		// HEIC images
		"heic" | "hif" => "image/heic",
		// HEIC images sequence (animated)
		"heics" => "image/heic-sequence",
		// AV1 in HEIF images
		"avif" => "image/avif",
		// AV1 in HEIF images sequence (DEPRECATED: https://github.com/AOMediaCodec/av1-avif/pull/86/files)
		"avifs" => "image/avif-sequence",
		// AVC in HEIF images
		"avci" => "image/avci",
		// AVC in HEIF images sequence (animated)
		"avcs" => "image/avcs",
		_ => "text/plain",
	};

	Ok(if mime_type == "text/plain" {
		let mut text_buf = vec![
			0;
			min(
				metadata.len().try_into().unwrap_or(usize::MAX),
				MAX_TEXT_READ_LENGTH
			)
		];
		if !text_buf.is_empty() {
			file.read_exact(&mut text_buf)
				.await
				.map_err(internal_server_error)?;
			file.seek(SeekFrom::Start(0))
				.await
				.map_err(internal_server_error)?;
		}

		let charset = is_text(&text_buf, text_buf.len() == (metadata.len() as usize)).unwrap_or("");

		// Only browser recognized types, everything else should be text/plain
		// https://www.iana.org/assignments/media-types/media-types.xhtml#table-text
		let mime_type = match ext.as_str() {
			// HyperText Markup Language
			"html" | "htm" => "text/html",
			// Cascading Style Sheets
			"css" => "text/css",
			// Javascript
			"js" | "mjs" => "text/javascript",
			// Comma-separated values
			"csv" => "text/csv",
			// Markdown
			"md" | "markdown" => "text/markdown",
			// Rich text format
			"rtf" => "text/rtf",
			// Web Video Text Tracks
			"vtt" => "text/vtt",
			// Extensible Markup Language
			"xml" => "text/xml",
			// Text
			"txt" => "text/plain",
			_ => {
				if charset.is_empty() {
					// "TODO: This filetype is not supported because of the missing mime type!",
					return Err(not_implemented(()));
				};
				mime_type
			}
		};

		format!("{mime_type}; charset={charset}")
	} else {
		mime_type.to_string()
	})
}



File: ./src/custom_uri/serve_file.rs
-------------------------------------------------
use crate::util::InfallibleResponse;

use std::{
	fs::Metadata,
	io::{self, SeekFrom},
	time::UNIX_EPOCH,
};

use axum::{
	body::{self, BoxBody, Full, StreamBody},
	http::{header, request, HeaderValue, Method, Response, StatusCode},
};
use http_range::HttpRange;
use tokio::{fs::File, io::AsyncSeekExt};
use tokio_util::io::ReaderStream;
use tracing::error;

use super::{async_read_body::AsyncReadBody, utils::*};

// default capacity 64KiB
const DEFAULT_CAPACITY: usize = 65536;

/// Serve a Tokio file as a HTTP response.
///
/// This function takes care of:
///  - 304 Not Modified using ETag's
///  - Range requests for partial content
///
/// BE AWARE this function does not do any path traversal protection so that's up to the caller!
pub(crate) async fn serve_file(
	mut file: File,
	metadata: io::Result<Metadata>,
	req: request::Parts,
	mut resp: InfallibleResponse,
) -> Result<Response<BoxBody>, Response<BoxBody>> {
	if let Ok(metadata) = metadata {
		// We only accept range queries if `files.metadata() == Ok(_)`
		// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Ranges
		resp = resp
			.header("Accept-Ranges", HeaderValue::from_static("bytes"))
			.header(
				"Content-Length",
				HeaderValue::from_str(&metadata.len().to_string())
					.expect("number won't fail conversion"),
			);

		// Empty files
		if metadata.len() == 0 {
			return Ok(resp
				.status(StatusCode::OK)
				.header("Content-Length", HeaderValue::from_static("0"))
				.body(body::boxed(Full::from(""))));
		}

		// ETag
		let mut status_code = StatusCode::PARTIAL_CONTENT;
		if let Ok(time) = metadata.modified() {
			let etag_header = format!(
				r#""{}""#,
				// The ETag's can be any value so we just use the modified time to make it easy.
				time.duration_since(UNIX_EPOCH)
					.expect("are you a time traveller? cause that's the only explanation for this error")
					.as_millis()
			);

			// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag
			if let Ok(etag_header) = HeaderValue::from_str(&etag_header) {
				resp = resp.header("etag", etag_header);
			} else {
				error!("Failed to convert ETag into header value!");
			}

			// Used for normal requests
			if let Some(etag) = req.headers.get("If-None-Match") {
				if etag.as_bytes() == etag_header.as_bytes() {
					return Ok(resp
						.status(StatusCode::NOT_MODIFIED)
						.body(body::boxed(Full::from(""))));
				}
			}

			// Used checking if the resource has been modified since starting the download
			if let Some(if_range) = req.headers.get("If-Range") {
				// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/If-Range
				if if_range.as_bytes() != etag_header.as_bytes() {
					status_code = StatusCode::OK
				}
			}
		};

		// https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests
		if req.method == Method::GET {
			if let Some(range) = req.headers.get("range") {
				// TODO: Error handling
				let ranges = HttpRange::parse(range.to_str().map_err(bad_request)?, metadata.len())
					.map_err(bad_request)?;

				// TODO: Multipart requests are not support, yet
				if ranges.len() != 1 {
					return Ok(resp
						.header(
							header::CONTENT_RANGE,
							HeaderValue::from_str(&format!("bytes */{}", metadata.len()))
								.map_err(internal_server_error)?,
						)
						.status(StatusCode::RANGE_NOT_SATISFIABLE)
						.body(body::boxed(Full::from(""))));
				}
				let range = ranges.first().expect("checked above");

				if (range.start + range.length) > metadata.len() {
					return Ok(resp
						.header(
							header::CONTENT_RANGE,
							HeaderValue::from_str(&format!("bytes */{}", metadata.len()))
								.map_err(internal_server_error)?,
						)
						.status(StatusCode::RANGE_NOT_SATISFIABLE)
						.body(body::boxed(Full::from(""))));
				}

				file.seek(SeekFrom::Start(range.start))
					.await
					.map_err(internal_server_error)?;

				return Ok(resp
					.status(status_code)
					.header(
						"Content-Range",
						HeaderValue::from_str(&format!(
							"bytes {}-{}/{}",
							range.start,
							range.start + range.length - 1,
							metadata.len()
						))
						.map_err(internal_server_error)?,
					)
					.header(
						"Content-Length",
						HeaderValue::from_str(&range.length.to_string())
							.map_err(internal_server_error)?,
					)
					.body(body::boxed(AsyncReadBody::with_capacity_limited(
						file,
						DEFAULT_CAPACITY,
						range.length,
					))));
			}
		}
	}

	Ok(resp.body(body::boxed(StreamBody::new(ReaderStream::new(file)))))
}



File: ./src/custom_uri/utils.rs
-------------------------------------------------
use std::{fmt::Debug, panic::Location};

use axum::{
	body::{self, BoxBody},
	http::{self, HeaderValue, Method, Request, Response, StatusCode},
	middleware::Next,
};
use http_body::Full;
use tracing::debug;

use crate::util::InfallibleResponse;

#[track_caller]
pub(crate) fn bad_request(err: impl Debug) -> http::Response<BoxBody> {
	debug!("400: Bad Request at {}: {err:?}", Location::caller());

	InfallibleResponse::builder()
		.status(StatusCode::BAD_REQUEST)
		.body(body::boxed(Full::from("")))
}

#[track_caller]
pub(crate) fn not_found(err: impl Debug) -> http::Response<BoxBody> {
	debug!("404: Not Found at {}: {err:?}", Location::caller());

	InfallibleResponse::builder()
		.status(StatusCode::NOT_FOUND)
		.body(body::boxed(Full::from("")))
}

#[track_caller]
pub(crate) fn internal_server_error(err: impl Debug) -> http::Response<BoxBody> {
	debug!(
		"500: Internal Server Error at {}: {err:?}",
		Location::caller()
	);

	InfallibleResponse::builder()
		.status(StatusCode::INTERNAL_SERVER_ERROR)
		.body(body::boxed(Full::from("")))
}

#[track_caller]
pub(crate) fn not_implemented(err: impl Debug) -> http::Response<BoxBody> {
	debug!("501: Not Implemented at {}: {err:?}", Location::caller());

	InfallibleResponse::builder()
		.status(StatusCode::NOT_IMPLEMENTED)
		.body(body::boxed(Full::from("")))
}

pub(crate) async fn cors_middleware<B>(req: Request<B>, next: Next<B>) -> Response<BoxBody> {
	if req.method() == Method::OPTIONS {
		return Response::builder()
			.header("Access-Control-Allow-Methods", "GET, HEAD, POST, OPTIONS")
			.header("Access-Control-Allow-Origin", "*")
			.header("Access-Control-Allow-Headers", "*")
			.header("Access-Control-Max-Age", "86400")
			.status(StatusCode::OK)
			.body(body::boxed(Full::from("")))
			.expect("Invalid static response!");
	}

	let mut response = next.run(req).await;

	{
		let headers = response.headers_mut();

		headers.insert("Access-Control-Allow-Origin", HeaderValue::from_static("*"));

		headers.insert(
			"Access-Control-Allow-Headers",
			HeaderValue::from_static("*"),
		);

		// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Connection
		headers.insert("Connection", HeaderValue::from_static("Keep-Alive"));

		headers.insert("Server", HeaderValue::from_static("Spacedrive"));
	}

	response
}



File: ./src/custom_uri/async_read_body.rs
-------------------------------------------------
use std::{
	io,
	pin::Pin,
	task::{Context, Poll},
};

use axum::http::HeaderMap;
use bytes::Bytes;
use futures::Stream;
use http_body::Body;
use pin_project_lite::pin_project;
use tokio::io::{AsyncRead, AsyncReadExt, Take};
use tokio_util::io::ReaderStream;

// This code was taken from: https://github.com/tower-rs/tower-http/blob/e8eb54966604ea7fa574a2a25e55232f5cfe675b/tower-http/src/services/fs/mod.rs#L30
pin_project! {
	// NOTE: This could potentially be upstreamed to `http-body`.
	/// Adapter that turns an [`impl AsyncRead`][tokio::io::AsyncRead] to an [`impl Body`][http_body::Body].
	#[derive(Debug)]
	pub struct AsyncReadBody<T> {
		#[pin]
		reader: ReaderStream<T>,
	}
}

impl<T> AsyncReadBody<T>
where
	T: AsyncRead,
{
	pub(crate) fn with_capacity_limited(
		read: T,
		capacity: usize,
		max_read_bytes: u64,
	) -> AsyncReadBody<Take<T>> {
		AsyncReadBody {
			reader: ReaderStream::with_capacity(read.take(max_read_bytes), capacity),
		}
	}
}

impl<T> Body for AsyncReadBody<T>
where
	T: AsyncRead,
{
	type Data = Bytes;
	type Error = io::Error;

	fn poll_data(
		self: Pin<&mut Self>,
		cx: &mut Context<'_>,
	) -> Poll<Option<Result<Self::Data, Self::Error>>> {
		self.project().reader.poll_next(cx)
	}

	fn poll_trailers(
		self: Pin<&mut Self>,
		_cx: &mut Context<'_>,
	) -> Poll<Result<Option<HeaderMap>, Self::Error>> {
		Poll::Ready(Ok(None))
	}
}



File: ./src/lib.rs
-------------------------------------------------
#![warn(clippy::unwrap_used, clippy::panic)]

use crate::{
	api::{CoreEvent, Router},
	location::LocationManagerError,
	object::media::thumbnail::actor::Thumbnailer,
};

use api::notifications::{Notification, NotificationData, NotificationId};
use chrono::{DateTime, Utc};
use node::config;
use notifications::Notifications;
use reqwest::{RequestBuilder, Response};
pub use sd_prisma::*;

use std::{
	fmt,
	path::{Path, PathBuf},
	sync::{atomic::AtomicBool, Arc},
};

use thiserror::Error;
use tokio::{fs, sync::broadcast};
use tracing::{error, info, warn};
use tracing_appender::{
	non_blocking::{NonBlocking, WorkerGuard},
	rolling::{RollingFileAppender, Rotation},
};
use tracing_subscriber::{
	filter::{Directive, FromEnvError, LevelFilter},
	fmt as tracing_fmt,
	prelude::*,
	EnvFilter,
};

pub mod api;
mod auth;
pub mod custom_uri;
mod env;
pub(crate) mod job;
pub mod library;
pub(crate) mod location;
pub(crate) mod node;
pub(crate) mod notifications;
pub(crate) mod object;
pub(crate) mod p2p;
pub(crate) mod preferences;
#[doc(hidden)] // TODO(@Oscar): Make this private when breaking out `utils` into `sd-utils`
pub mod util;
pub(crate) mod volume;

pub use env::Env;

pub(crate) use sd_core_sync as sync;

/// Represents a single running instance of the Spacedrive core.
/// Holds references to all the services that make up the Spacedrive core.
pub struct Node {
	pub data_dir: PathBuf,
	pub config: Arc<config::Manager>,
	pub libraries: Arc<library::Libraries>,
	pub jobs: Arc<job::Jobs>,
	pub locations: location::Locations,
	pub p2p: Arc<p2p::P2PManager>,
	pub event_bus: (broadcast::Sender<CoreEvent>, broadcast::Receiver<CoreEvent>),
	pub notifications: Notifications,
	pub thumbnailer: Thumbnailer,
	pub files_over_p2p_flag: Arc<AtomicBool>,
	pub env: env::Env,
	pub http: reqwest::Client,
}

impl fmt::Debug for Node {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		f.debug_struct("Node")
			.field("data_dir", &self.data_dir)
			.finish()
	}
}

impl Node {
	pub async fn new(
		data_dir: impl AsRef<Path>,
		env: env::Env,
	) -> Result<(Arc<Node>, Arc<Router>), NodeError> {
		let data_dir = data_dir.as_ref();

		info!("Starting core with data directory '{}'", data_dir.display());

		#[cfg(debug_assertions)]
		let init_data = util::debug_initializer::InitConfig::load(data_dir).await?;

		// This error is ignored because it's throwing on mobile despite the folder existing.
		let _ = fs::create_dir_all(&data_dir).await;

		let event_bus = broadcast::channel(1024);
		let config = config::Manager::new(data_dir.to_path_buf())
			.await
			.map_err(NodeError::FailedToInitializeConfig)?;

		let (locations, locations_actor) = location::Locations::new();
		let (jobs, jobs_actor) = job::Jobs::new();
		let libraries = library::Libraries::new(data_dir.join("libraries")).await?;
		let (p2p, p2p_actor) = p2p::P2PManager::new(config.clone(), libraries.clone()).await?;
		let node = Arc::new(Node {
			data_dir: data_dir.to_path_buf(),
			jobs,
			locations,
			notifications: notifications::Notifications::new(),
			p2p,
			thumbnailer: Thumbnailer::new(
				data_dir.to_path_buf(),
				libraries.clone(),
				event_bus.0.clone(),
				config.preferences_watcher(),
			)
			.await,
			config,
			event_bus,
			libraries,
			files_over_p2p_flag: Arc::new(AtomicBool::new(false)),
			http: reqwest::Client::new(),
			env,
		});

		// Restore backend feature flags
		for feature in node.config.get().await.features {
			feature.restore(&node);
		}

		// Setup start actors that depend on the `Node`
		#[cfg(debug_assertions)]
		if let Some(init_data) = init_data {
			init_data.apply(&node.libraries, &node).await?;
		}

		// Be REALLY careful about ordering here or you'll get unreliable deadlock's!
		locations_actor.start(node.clone());
		node.libraries.init(&node).await?;
		jobs_actor.start(node.clone());
		p2p_actor.start(node.clone());

		let router = api::mount();

		info!("Spacedrive online.");
		Ok((node, router))
	}

	pub fn init_logger(data_dir: impl AsRef<Path>) -> Result<WorkerGuard, FromEnvError> {
		let (logfile, guard) = NonBlocking::new(
			RollingFileAppender::builder()
				.filename_prefix("sd.log")
				.rotation(Rotation::DAILY)
				.max_log_files(4)
				.build(data_dir.as_ref().join("logs"))
				.expect("Error setting up log file!"),
		);

		// Set a default if the user hasn't set an override
		if std::env::var("RUST_LOG") == Err(std::env::VarError::NotPresent) {
			let directive: Directive = if cfg!(debug_assertions) {
				LevelFilter::DEBUG
			} else {
				LevelFilter::INFO
			}
			.into();
			std::env::set_var("RUST_LOG", directive.to_string());
		}

		let collector = tracing_subscriber::registry()
			.with(
				tracing_fmt::Subscriber::new()
					.with_file(true)
					.with_line_number(true)
					.with_ansi(false)
					.with_writer(logfile)
					.with_filter(
						EnvFilter::builder()
							.from_env()?
							.add_directive("info".parse()?),
					),
			)
			.with(
				tracing_fmt::Subscriber::new()
					.with_file(true)
					.with_line_number(true)
					.with_writer(std::io::stdout)
					.with_filter(
						EnvFilter::builder()
							.from_env()?
							// We don't wanna blow up the logs
							.add_directive("sd_core::location::manager=info".parse()?),
					),
			);

		tracing::collect::set_global_default(collector)
			.map_err(|err| {
				eprintln!("Error initializing global logger: {:?}", err);
			})
			.ok();

		std::panic::set_hook(Box::new(move |panic| {
			if let Some(location) = panic.location() {
				tracing::error!(
					message = %panic,
					panic.file = format!("{}:{}", location.file(), location.line()),
					panic.column = location.column(),
				);
			} else {
				tracing::error!(message = %panic);
			}
		}));

		Ok(guard)
	}

	pub async fn shutdown(&self) {
		info!("Spacedrive shutting down...");
		self.thumbnailer.shutdown().await;
		self.jobs.shutdown().await;
		self.p2p.shutdown().await;
		info!("Spacedrive Core shutdown successful!");
	}

	pub(crate) fn emit(&self, event: CoreEvent) {
		if let Err(e) = self.event_bus.0.send(event) {
			warn!("Error sending event to event bus: {e:?}");
		}
	}

	pub async fn emit_notification(&self, data: NotificationData, expires: Option<DateTime<Utc>>) {
		let notification = Notification {
			id: NotificationId::Node(self.notifications._internal_next_id()),
			data,
			read: false,
			expires,
		};

		match self
			.config
			.write(|cfg| cfg.notifications.push(notification.clone()))
			.await
		{
			Ok(_) => {
				self.notifications._internal_send(notification);
			}
			Err(err) => {
				error!("Error saving notification to config: {:?}", err);
			}
		}
	}

	pub async fn add_auth_header(&self, mut req: RequestBuilder) -> RequestBuilder {
		if let Some(auth_token) = self.config.get().await.auth_token {
			req = req.header("authorization", auth_token.to_header());
		};

		req
	}

	pub async fn authed_api_request(&self, req: RequestBuilder) -> Result<Response, rspc::Error> {
		let Some(auth_token) = self.config.get().await.auth_token else {
			return Err(rspc::Error::new(
				rspc::ErrorCode::Unauthorized,
				"No auth token".to_string(),
			));
		};

		let req = req.header("authorization", auth_token.to_header());

		req.send().await.map_err(|_| {
			rspc::Error::new(
				rspc::ErrorCode::InternalServerError,
				"Request failed".to_string(),
			)
		})
	}
}

/// Error type for Node related errors.
#[derive(Error, Debug)]
pub enum NodeError {
	#[error("NodeError::FailedToInitializeConfig({0})")]
	FailedToInitializeConfig(config::NodeConfigError),
	#[error("failed to initialize library manager: {0}")]
	FailedToInitializeLibraryManager(#[from] library::LibraryManagerError),
	#[error("failed to initialize location manager: {0}")]
	LocationManager(#[from] LocationManagerError),
	#[error("failed to initialize p2p manager: {0}")]
	P2PManager(#[from] sd_p2p::ManagerError),
	#[error("invalid platform integer: {0}")]
	InvalidPlatformInt(u8),
	#[cfg(debug_assertions)]
	#[error("init config error: {0}")]
	InitConfig(#[from] util::debug_initializer::InitConfigError),
	#[error("logger error: {0}")]
	Logger(#[from] FromEnvError),
}



File: ./src/auth.rs
-------------------------------------------------
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct OAuthToken {
	access_token: String,
	refresh_token: String,
	token_type: String,
	expires_in: i32,
}

impl OAuthToken {
	pub fn to_header(&self) -> String {
		format!("{} {}", &self.token_type, self.access_token)
	}
}

pub const DEVICE_CODE_URN: &str = "urn:ietf:params:oauth:grant-type:device_code";



File: ./src/library/cat.rs
-------------------------------------------------
use crate::prisma::object;
use prisma_client_rust::not;
use sd_file_ext::kind::ObjectKind;
use serde::{Deserialize, Serialize};
use specta::Type;
use std::vec;

use strum_macros::{EnumString, EnumVariantNames};

/// Meow
#[derive(
	Serialize,
	Deserialize,
	Type,
	Debug,
	PartialEq,
	Eq,
	PartialOrd,
	Ord,
	EnumVariantNames,
	EnumString,
	Clone,
	Copy,
)]
pub enum Category {
	Recents,
	Favorites,
	Albums,
	Photos,
	Videos,
	Movies,
	Music,
	Documents,
	Downloads,
	Encrypted,
	Projects,
	Applications,
	Archives,
	Databases,
	Games,
	Books,
	Contacts,
	Trash,
	Screenshots,
}

impl Category {
	// this should really be done without unimplemented! and on another type but ehh
	fn to_object_kind(self) -> ObjectKind {
		match self {
			Category::Photos => ObjectKind::Image,
			Category::Videos => ObjectKind::Video,
			Category::Music => ObjectKind::Audio,
			Category::Books => ObjectKind::Book,
			Category::Encrypted => ObjectKind::Encrypted,
			Category::Databases => ObjectKind::Database,
			Category::Archives => ObjectKind::Archive,
			Category::Applications => ObjectKind::Executable,
			Category::Screenshots => ObjectKind::Screenshot,
			_ => unimplemented!("Category::to_object_kind() for {:?}", self),
		}
	}

	pub fn to_where_param(self) -> object::WhereParam {
		match self {
			Category::Recents => not![object::date_accessed::equals(None)],
			Category::Favorites => object::favorite::equals(Some(true)),
			Category::Photos
			| Category::Videos
			| Category::Music
			| Category::Encrypted
			| Category::Databases
			| Category::Archives
			| Category::Applications
			| Category::Books => object::kind::equals(Some(self.to_object_kind() as i32)),
			_ => object::id::equals(-1),
		}
	}
}



File: ./src/library/library.rs
-------------------------------------------------
use crate::{
	api::{
		notifications::{Notification, NotificationData, NotificationId},
		CoreEvent,
	},
	location::file_path_helper::{file_path_to_full_path, IsolatedFilePathData},
	notifications,
	object::{media::thumbnail::get_indexed_thumbnail_path, orphan_remover::OrphanRemoverActor},
	prisma::{file_path, location, PrismaClient},
	sync,
	util::{db::maybe_missing, error::FileIOError},
	Node,
};

use sd_p2p::spacetunnel::Identity;
use sd_prisma::prisma::notification;

use std::{
	collections::HashMap,
	fmt::{Debug, Formatter},
	path::{Path, PathBuf},
	sync::Arc,
};

use chrono::{DateTime, Utc};
use tokio::{fs, io, sync::broadcast, sync::RwLock};
use tracing::warn;
use uuid::Uuid;

use super::{LibraryConfig, LibraryManagerError};

// TODO: Finish this
// pub enum LibraryNew {
// 	InitialSync,
// 	Encrypted,
// 	Loaded(LoadedLibrary),
//  Deleting,
// }

pub struct Library {
	/// id holds the ID of the current library.
	pub id: Uuid,
	/// config holds the configuration of the current library.
	/// KEEP PRIVATE: Access through `Self::config` method.
	config: RwLock<LibraryConfig>,
	/// db holds the database client for the current library.
	pub db: Arc<PrismaClient>,
	pub sync: Arc<sync::Manager>,
	/// key manager that provides encryption keys to functions that require them
	// pub key_manager: Arc<KeyManager>,
	/// p2p identity
	pub identity: Arc<Identity>,
	pub orphan_remover: OrphanRemoverActor,
	// The UUID which matches `config.instance_id`'s primary key.
	pub instance_uuid: Uuid,

	notifications: notifications::Notifications,

	// Look, I think this shouldn't be here but our current invalidation system needs it.
	// TODO(@Oscar): Get rid of this with the new invalidation system.
	event_bus_tx: broadcast::Sender<CoreEvent>,
}

impl Debug for Library {
	fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
		// Rolling out this implementation because `NodeContext` contains a DynJob which is
		// troublesome to implement Debug trait
		f.debug_struct("LibraryContext")
			.field("id", &self.id)
			.field("instance_uuid", &self.instance_uuid)
			.field("config", &self.config)
			.field("db", &self.db)
			.finish()
	}
}

impl Library {
	pub async fn new(
		id: Uuid,
		config: LibraryConfig,
		instance_uuid: Uuid,
		identity: Arc<Identity>,
		db: Arc<PrismaClient>,
		node: &Arc<Node>,
		sync: Arc<sync::Manager>,
	) -> Arc<Self> {
		Arc::new(Self {
			id,
			config: RwLock::new(config),
			sync,
			db: db.clone(),
			// key_manager,
			identity,
			orphan_remover: OrphanRemoverActor::spawn(db),
			notifications: node.notifications.clone(),
			instance_uuid,
			event_bus_tx: node.event_bus.0.clone(),
		})
	}

	pub async fn config(&self) -> LibraryConfig {
		self.config.read().await.clone()
	}

	pub async fn update_config(
		&self,
		update_fn: impl FnOnce(&mut LibraryConfig),
		config_path: impl AsRef<Path>,
	) -> Result<(), LibraryManagerError> {
		let mut config = self.config.write().await;

		update_fn(&mut config);

		config.save(config_path).await.map_err(Into::into)
	}

	// TODO: Remove this once we replace the old invalidation system
	pub(crate) fn emit(&self, event: CoreEvent) {
		if let Err(e) = self.event_bus_tx.send(event) {
			warn!("Error sending event to event bus: {e:?}");
		}
	}

	pub async fn thumbnail_exists(&self, node: &Node, cas_id: &str) -> Result<bool, FileIOError> {
		let thumb_path = get_indexed_thumbnail_path(node, cas_id, self.id);

		match fs::metadata(&thumb_path).await {
			Ok(_) => Ok(true),
			Err(e) if e.kind() == io::ErrorKind::NotFound => Ok(false),
			Err(e) => Err(FileIOError::from((thumb_path, e))),
		}
	}

	/// Returns the full path of a file
	pub async fn get_file_paths(
		&self,
		ids: Vec<file_path::id::Type>,
	) -> Result<HashMap<file_path::id::Type, Option<PathBuf>>, LibraryManagerError> {
		let mut out = ids
			.iter()
			.copied()
			.map(|id| (id, None))
			.collect::<HashMap<_, _>>();

		out.extend(
			self.db
				.file_path()
				.find_many(vec![
					// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
					file_path::location::is(vec![location::instance_id::equals(Some(
						self.config().await.instance_id,
					))]),
					file_path::id::in_vec(ids),
				])
				.select(file_path_to_full_path::select())
				.exec()
				.await?
				.into_iter()
				.flat_map(|file_path| {
					let location = maybe_missing(&file_path.location, "file_path.location")?;

					Ok::<_, LibraryManagerError>((
						file_path.id,
						location
							.path
							.as_ref()
							.map(|location_path| {
								IsolatedFilePathData::try_from((location.id, &file_path))
									.map(|data| Path::new(&location_path).join(data))
							})
							.transpose()?,
					))
				}),
		);

		Ok(out)
	}

	/// Create a new notification which will be stored into the DB and emitted to the UI.
	pub async fn emit_notification(&self, data: NotificationData, expires: Option<DateTime<Utc>>) {
		let result = match self
			.db
			.notification()
			.create(
				match rmp_serde::to_vec(&data).map_err(|err| err.to_string()) {
					Ok(data) => data,
					Err(err) => {
						warn!(
							"Failed to serialize notification data for library '{}': {}",
							self.id, err
						);
						return;
					}
				},
				expires
					.map(|e| vec![notification::expires_at::set(Some(e.fixed_offset()))])
					.unwrap_or_default(),
			)
			.exec()
			.await
		{
			Ok(result) => result,
			Err(err) => {
				warn!(
					"Failed to create notification in library '{}': {}",
					self.id, err
				);
				return;
			}
		};

		self.notifications._internal_send(Notification {
			id: NotificationId::Library(self.id, result.id as u32),
			data,
			read: false,
			expires,
		});
	}
}



File: ./src/library/config.rs
-------------------------------------------------
use crate::{
	node::{config::NodeConfig, Platform},
	p2p::IdentityOrRemoteIdentity,
	prisma::{file_path, indexer_rule, PrismaClient},
	util::{
		db::maybe_missing,
		error::FileIOError,
		version_manager::{Kind, ManagedVersion, VersionManager, VersionManagerError},
	},
};

use sd_p2p::spacetunnel::Identity;
use sd_prisma::prisma::{instance, location, node};

use std::path::Path;

use chrono::Utc;
use int_enum::IntEnum;
use prisma_client_rust::not;
use serde::{Deserialize, Serialize};
use serde_json::{json, Map, Value};
use serde_repr::{Deserialize_repr, Serialize_repr};
use specta::Type;
use thiserror::Error;
use tokio::fs;
use tracing::error;
use uuid::Uuid;

use super::name::LibraryName;

/// LibraryConfig holds the configuration for a specific library. This is stored as a '{uuid}.sdlibrary' file.
#[derive(Debug, Clone, Serialize, Deserialize, Type)]
pub struct LibraryConfig {
	/// name is the display name of the library. This is used in the UI and is set by the user.
	pub name: LibraryName,
	/// description is a user set description of the library. This is used in the UI and is set by the user.
	pub description: Option<String>,
	/// id of the current instance so we know who this `.db` is. This can be looked up within the `Instance` table.
	pub instance_id: i32,

	version: LibraryConfigVersion,
}

#[derive(
	IntEnum,
	Debug,
	Clone,
	Copy,
	Eq,
	PartialEq,
	strum::Display,
	Serialize_repr,
	Deserialize_repr,
	Type,
)]
#[repr(u64)]
pub enum LibraryConfigVersion {
	V0 = 0,
	V1 = 1,
	V2 = 2,
	V3 = 3,
	V4 = 4,
	V5 = 5,
	V6 = 6,
	V7 = 7,
	V8 = 8,
	V9 = 9,
}

impl ManagedVersion<LibraryConfigVersion> for LibraryConfig {
	const LATEST_VERSION: LibraryConfigVersion = LibraryConfigVersion::V9;

	const KIND: Kind = Kind::Json("version");

	type MigrationError = LibraryConfigError;
}

impl LibraryConfig {
	pub(crate) async fn new(
		name: LibraryName,
		description: Option<String>,
		instance_id: i32,
		path: impl AsRef<Path>,
	) -> Result<Self, LibraryConfigError> {
		let this = Self {
			name,
			description,
			instance_id,
			version: Self::LATEST_VERSION,
		};

		this.save(path).await.map(|()| this)
	}

	pub(crate) async fn load(
		path: impl AsRef<Path>,
		node_config: &NodeConfig,
		db: &PrismaClient,
	) -> Result<Self, LibraryConfigError> {
		let path = path.as_ref();

		VersionManager::<Self, LibraryConfigVersion>::migrate_and_load(
			path,
			|current, next| async move {
				match (current, next) {
					(LibraryConfigVersion::V0, LibraryConfigVersion::V1) => {
						let rules = vec![
							String::from("No OS protected"),
							String::from("No Hidden"),
							String::from("No Git"),
							String::from("Only Images"),
						];

						db._batch(
							rules
								.into_iter()
								.enumerate()
								.map(|(i, name)| {
									db.indexer_rule().update_many(
										vec![indexer_rule::name::equals(Some(name))],
										vec![indexer_rule::pub_id::set(sd_utils::uuid_to_bytes(
											Uuid::from_u128(i as u128),
										))],
									)
								})
								.collect::<Vec<_>>(),
						)
						.await?;
					}

					(LibraryConfigVersion::V1, LibraryConfigVersion::V2) => {
						let mut config = serde_json::from_slice::<Map<String, Value>>(
							&fs::read(path).await.map_err(|e| {
								VersionManagerError::FileIO(FileIOError::from((path, e)))
							})?,
						)
						.map_err(VersionManagerError::SerdeJson)?;

						config.insert(
							String::from("identity"),
							Value::Array(
								Identity::new()
									.to_bytes()
									.into_iter()
									.map(Into::into)
									.collect(),
							),
						);

						fs::write(
							path,
							&serde_json::to_vec(&config).map_err(VersionManagerError::SerdeJson)?,
						)
						.await
						.map_err(|e| VersionManagerError::FileIO(FileIOError::from((path, e))))?;
					}

					(LibraryConfigVersion::V2, LibraryConfigVersion::V3) => {
						// The fact I have to migrate this hurts my soul
						if db.node().count(vec![]).exec().await? != 1 {
							return Err(LibraryConfigError::TooManyNodes);
						}

						db.node()
							.update_many(
								vec![],
								vec![
									node::pub_id::set(node_config.id.as_bytes().to_vec()),
									node::node_peer_id::set(Some(
										node_config.keypair.peer_id().to_string(),
									)),
								],
							)
							.exec()
							.await?;

						let mut config = serde_json::from_slice::<Map<String, Value>>(
							&fs::read(path).await.map_err(|e| {
								VersionManagerError::FileIO(FileIOError::from((path, e)))
							})?,
						)
						.map_err(VersionManagerError::SerdeJson)?;

						config.insert(String::from("node_id"), json!(node_config.id.to_string()));

						fs::write(
							path,
							&serde_json::to_vec(&config).map_err(VersionManagerError::SerdeJson)?,
						)
						.await
						.map_err(|e| VersionManagerError::FileIO(FileIOError::from((path, e))))?;
					}

					(LibraryConfigVersion::V3, LibraryConfigVersion::V4) => {
						// -_-
					}

					(LibraryConfigVersion::V4, LibraryConfigVersion::V5) => loop {
						let paths = db
							.file_path()
							.find_many(vec![not![file_path::size_in_bytes::equals(None)]])
							.take(500)
							.select(file_path::select!({ id size_in_bytes }))
							.exec()
							.await?;

						if paths.is_empty() {
							break;
						}

						db._batch(
							paths
								.into_iter()
								.filter_map(|path| {
									maybe_missing(path.size_in_bytes, "file_path.size_in_bytes")
										.map_or_else(
											|e| {
												error!("{e:#?}");
												None
											},
											Some,
										)
										.map(|size_in_bytes| {
											let size =
												if let Ok(size) = size_in_bytes.parse::<u64>() {
													Some(size.to_be_bytes().to_vec())
												} else {
													error!(
											"File path <id='{}'> had invalid size: '{}'",
											path.id, size_in_bytes
										);
													None
												};

											db.file_path().update(
												file_path::id::equals(path.id),
												vec![
													file_path::size_in_bytes_bytes::set(size),
													file_path::size_in_bytes::set(None),
												],
											)
										})
								})
								.collect::<Vec<_>>(),
						)
						.await?;
					},

					(LibraryConfigVersion::V5, LibraryConfigVersion::V6) => {
						let nodes = db.node().find_many(vec![]).exec().await?;
						if nodes.is_empty() {
							error!("6 - No nodes found... How did you even get this far? but this is fine we can fix it.");
						} else if nodes.len() > 1 {
							error!("6 - More than one node found in the DB... This can't be automatically reconciled!");
							return Err(LibraryConfigError::TooManyNodes);
						}

						let node = nodes.first();
						let now = Utc::now().fixed_offset();
						let instance_id = Uuid::new_v4();

						instance::Create {
							pub_id: instance_id.as_bytes().to_vec(),
							identity: node
								.and_then(|n| n.identity.clone())
								.unwrap_or_else(|| Identity::new().to_bytes()),
							node_id: node_config.id.as_bytes().to_vec(),
							node_name: node_config.name.clone(),
							node_platform: Platform::current() as i32,
							last_seen: now,
							date_created: node.map(|n| n.date_created).unwrap_or_else(|| now),
							_params: vec![],
						}
						.to_query(db)
						.exec()
						.await?;

						let mut config = serde_json::from_slice::<Map<String, Value>>(
							&fs::read(path).await.map_err(|e| {
								VersionManagerError::FileIO(FileIOError::from((path, e)))
							})?,
						)
						.map_err(VersionManagerError::SerdeJson)?;

						config.remove("node_id");
						config.remove("identity");

						config.insert(String::from("instance_id"), json!(instance_id.to_string()));

						fs::write(
							path,
							&serde_json::to_vec(&config).map_err(VersionManagerError::SerdeJson)?,
						)
						.await
						.map_err(|e| VersionManagerError::FileIO(FileIOError::from((path, e))))?;
					}

					(LibraryConfigVersion::V6, LibraryConfigVersion::V7) => {
						let instances = db.instance().find_many(vec![]).exec().await?;

						if instances.len() > 1 {
							error!("7 - More than one instance found in the DB... This can't be automatically reconciled!");
							return Err(LibraryConfigError::TooManyInstances);
						}

						let Some(instance) = instances.first() else {
							error!("7 - No instance found... How did you even get this far?!");
							return Err(LibraryConfigError::MissingInstance);
						};

						let mut config = serde_json::from_slice::<Map<String, Value>>(
							&fs::read(path).await.map_err(|e| {
								VersionManagerError::FileIO(FileIOError::from((path, e)))
							})?,
						)
						.map_err(VersionManagerError::SerdeJson)?;

						config.remove("instance_id");
						config.insert(String::from("instance_id"), json!(instance.id));

						fs::write(
							path,
							&serde_json::to_vec(&config).map_err(VersionManagerError::SerdeJson)?,
						)
						.await
						.map_err(|e| VersionManagerError::FileIO(FileIOError::from((path, e))))?;

						// We are relinking all locations to the current instance.
						// If you have more than one node in your database and you're not @Oscar, something went horribly wrong so this is fine.
						db.location()
							.update_many(
								vec![],
								vec![location::instance_id::set(Some(instance.id))],
							)
							.exec()
							.await?;
					}

					(LibraryConfigVersion::V7, LibraryConfigVersion::V8) => {
						let instances = db.instance().find_many(vec![]).exec().await?;
						let Some(instance) = instances.first() else {
							error!("8 - No nodes found... How did you even get this far?!");
							return Err(LibraryConfigError::MissingInstance);
						};

						// This should be in 7 but it's added to ensure to hell it runs.
						let mut config = serde_json::from_slice::<Map<String, Value>>(
							&fs::read(path).await.map_err(|e| {
								VersionManagerError::FileIO(FileIOError::from((path, e)))
							})?,
						)
						.map_err(VersionManagerError::SerdeJson)?;

						config.remove("instance_id");
						config.insert(String::from("instance_id"), json!(instance.id));

						fs::write(
							path,
							&serde_json::to_vec(&config).map_err(VersionManagerError::SerdeJson)?,
						)
						.await
						.map_err(|e| VersionManagerError::FileIO(FileIOError::from((path, e))))?;
					}

					(LibraryConfigVersion::V8, LibraryConfigVersion::V9) => {
						db._batch(
							db.instance()
								.find_many(vec![])
								.exec()
								.await?
								.into_iter()
								.map(|i| {
									db.instance().update(
										instance::id::equals(i.id),
										vec![instance::identity::set(
									// This code is assuming you only have the current node.
									// If you've paired your node with another node, reset your db.
									IdentityOrRemoteIdentity::Identity(
										Identity::from_bytes(&i.identity).expect(
											"Invalid identity detected in DB during migrations",
										),
									)
									.to_bytes(),
								)],
									)
								})
								.collect::<Vec<_>>(),
						)
						.await?;
					}

					_ => {
						error!("Library config version is not handled: {:?}", current);
						return Err(VersionManagerError::UnexpectedMigration {
							current_version: current.int_value(),
							next_version: next.int_value(),
						}
						.into());
					}
				}
				Ok(())
			},
		)
		.await
	}

	pub(crate) async fn save(&self, path: impl AsRef<Path>) -> Result<(), LibraryConfigError> {
		let path = path.as_ref();
		fs::write(path, &serde_json::to_vec(self)?)
			.await
			.map_err(|e| FileIOError::from((path, e)).into())
	}
}

#[derive(Error, Debug)]
pub enum LibraryConfigError {
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error("there are too many nodes in the database, this should not happen!")]
	TooManyNodes,
	#[error("there are too many instances in the database, this should not happen!")]
	TooManyInstances,
	#[error("missing instances")]
	MissingInstance,

	#[error(transparent)]
	SerdeJson(#[from] serde_json::Error),
	#[error(transparent)]
	VersionManager(#[from] VersionManagerError<LibraryConfigVersion>),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
}



File: ./src/library/manager/error.rs
-------------------------------------------------
use crate::{
	library::LibraryConfigError,
	location::{indexer, LocationManagerError},
	p2p::IdentityOrRemoteIdentityErr,
	util::{
		db::{self, MissingFieldError},
		error::{FileIOError, NonUtf8PathError},
	},
};

use thiserror::Error;
use tracing::error;

#[derive(Error, Debug)]
pub enum LibraryManagerError {
	#[error("error serializing or deserializing the JSON in the config file: {0}")]
	Json(#[from] serde_json::Error),
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error("library not found error")]
	LibraryNotFound,
	#[error("failed to parse uuid: {0}")]
	Uuid(#[from] uuid::Error),
	#[error("failed to run indexer rules seeder: {0}")]
	IndexerRulesSeeder(#[from] indexer::rules::seed::SeederError),
	// #[error("failed to initialise the key manager: {0}")]
	// KeyManager(#[from] sd_crypto::Error),
	#[error("error migrating the library: {0}")]
	MigrationError(#[from] db::MigrationError),
	#[error("invalid library configuration: {0}")]
	InvalidConfig(String),
	#[error(transparent)]
	NonUtf8Path(#[from] NonUtf8PathError),
	#[error("failed to watch locations: {0}")]
	LocationWatcher(#[from] LocationManagerError),
	#[error("failed to parse library p2p identity: {0}")]
	Identity(#[from] IdentityOrRemoteIdentityErr),
	#[error("failed to load private key for instance p2p identity")]
	InvalidIdentity,
	#[error("current instance with id '{0}' was not found in the database")]
	CurrentInstanceNotFound(String),
	#[error("missing-field: {0}")]
	MissingField(#[from] MissingFieldError),

	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	LibraryConfig(#[from] LibraryConfigError),
}

impl From<LibraryManagerError> for rspc::Error {
	fn from(error: LibraryManagerError) -> Self {
		rspc::Error::with_cause(
			rspc::ErrorCode::InternalServerError,
			error.to_string(),
			error,
		)
	}
}



File: ./src/library/manager/mod.rs
-------------------------------------------------
use crate::{
	api::{utils::InvalidateOperationEvent, CoreEvent},
	invalidate_query,
	location::{
		indexer,
		metadata::{LocationMetadataError, SpacedriveLocationMetadataFile},
	},
	node::Platform,
	object::tag,
	p2p::{self, IdentityOrRemoteIdentity},
	prisma::location,
	sync,
	util::{
		db,
		error::{FileIOError, NonUtf8PathError},
		mpscrr, MaybeUndefined,
	},
	volume::watcher::spawn_volume_watcher,
	Node,
};

use sd_core_sync::SyncMessage;
use sd_p2p::spacetunnel::Identity;
use sd_prisma::prisma::instance;

use std::{
	collections::HashMap,
	path::{Path, PathBuf},
	str::FromStr,
	sync::{atomic::AtomicBool, Arc},
};

use chrono::Utc;
use futures_concurrency::future::{Join, TryJoin};
use tokio::{fs, io, sync::RwLock};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use super::{Library, LibraryConfig, LibraryName};

mod error;

pub use error::*;

/// Event that is emitted to subscribers of the library manager.
#[derive(Debug, Clone)]
pub enum LibraryManagerEvent {
	Load(Arc<Library>),
	Edit(Arc<Library>),
	// TODO(@Oscar): Replace this with pairing -> ready state transitions
	InstancesModified(Arc<Library>),
	Delete(Arc<Library>),
}

/// is a singleton that manages all libraries for a node.
pub struct Libraries {
	/// libraries_dir holds the path to the directory where libraries are stored.
	pub libraries_dir: PathBuf,
	/// libraries holds the list of libraries which are currently loaded into the node.
	libraries: RwLock<HashMap<Uuid, Arc<Library>>>,
	// Transmit side of `self.rx` channel
	tx: mpscrr::Sender<LibraryManagerEvent, ()>,
	/// A channel for receiving events from the library manager.
	pub rx: mpscrr::Receiver<LibraryManagerEvent, ()>,
	pub emit_messages_flag: Arc<AtomicBool>,
}

impl Libraries {
	pub(crate) async fn new(libraries_dir: PathBuf) -> Result<Arc<Self>, LibraryManagerError> {
		fs::create_dir_all(&libraries_dir)
			.await
			.map_err(|e| FileIOError::from((&libraries_dir, e)))?;

		let (tx, rx) = mpscrr::unbounded_channel();
		Ok(Arc::new(Self {
			libraries_dir,
			libraries: Default::default(),
			tx,
			rx,
			emit_messages_flag: Arc::new(AtomicBool::new(false)),
		}))
	}

	/// Loads the initial libraries from disk.
	///
	/// `Arc<LibraryManager>` is constructed and passed to other managers for them to subscribe (`self.rx.subscribe`) then this method is run to load the initial libraries and trigger the subscriptions.
	pub async fn init(self: &Arc<Self>, node: &Arc<Node>) -> Result<(), LibraryManagerError> {
		let mut read_dir = fs::read_dir(&self.libraries_dir)
			.await
			.map_err(|e| FileIOError::from((&self.libraries_dir, e)))?;

		while let Some(entry) = read_dir
			.next_entry()
			.await
			.map_err(|e| FileIOError::from((&self.libraries_dir, e)))?
		{
			let config_path = entry.path();
			if config_path
				.extension()
				.map(|ext| ext == "sdlibrary")
				.unwrap_or(false)
				&& entry
					.metadata()
					.await
					.map_err(|e| FileIOError::from((&config_path, e)))?
					.is_file()
			{
				let Some(Ok(library_id)) = config_path
					.file_stem()
					.and_then(|v| v.to_str().map(Uuid::from_str))
				else {
					warn!(
						"Attempted to load library from path '{}' \
						but it has an invalid filename. Skipping...",
						config_path.display()
					);
					continue;
				};

				let db_path = config_path.with_extension("db");
				match fs::metadata(&db_path).await {
					Ok(_) => {}
					Err(e) if e.kind() == io::ErrorKind::NotFound => {
						warn!("Found library '{}' but no matching database file was found. Skipping...", config_path.display());
						continue;
					}
					Err(e) => return Err(FileIOError::from((db_path, e)).into()),
				}

				let library_arc = self
					.load(library_id, &db_path, config_path, None, true, node)
					.await?;

				spawn_volume_watcher(library_arc.clone());
			}
		}

		Ok(())
	}

	/// create creates a new library with the given config and mounts it into the running [LibraryManager].
	pub(crate) async fn create(
		self: &Arc<Self>,
		name: LibraryName,
		description: Option<String>,
		node: &Arc<Node>,
	) -> Result<Arc<Library>, LibraryManagerError> {
		self.create_with_uuid(Uuid::new_v4(), name, description, true, None, node)
			.await
	}

	pub(crate) async fn create_with_uuid(
		self: &Arc<Self>,
		id: Uuid,
		name: LibraryName,
		description: Option<String>,
		should_seed: bool,
		// `None` will fallback to default as library must be created with at least one instance
		instance: Option<instance::Create>,
		node: &Arc<Node>,
	) -> Result<Arc<Library>, LibraryManagerError> {
		if name.as_ref().is_empty() || name.as_ref().chars().all(|x| x.is_whitespace()) {
			return Err(LibraryManagerError::InvalidConfig(
				"name cannot be empty".to_string(),
			));
		}

		let config_path = self.libraries_dir.join(format!("{id}.sdlibrary"));

		let config = LibraryConfig::new(
			name,
			description,
			// First instance will be zero
			0,
			&config_path,
		)
		.await?;

		debug!(
			"Created library '{}' config at '{}'",
			id,
			config_path.display()
		);

		let node_cfg = node.config.get().await;
		let now = Utc::now().fixed_offset();
		let library = self
			.load(
				id,
				self.libraries_dir.join(format!("{id}.db")),
				config_path,
				Some({
					let mut create = instance.unwrap_or_else(|| instance::Create {
						pub_id: Uuid::new_v4().as_bytes().to_vec(),
						identity: IdentityOrRemoteIdentity::Identity(Identity::new()).to_bytes(),
						node_id: node_cfg.id.as_bytes().to_vec(),
						node_name: node_cfg.name.clone(),
						node_platform: Platform::current() as i32,
						last_seen: now,
						date_created: now,
						_params: vec![],
					});
					create._params.push(instance::id::set(config.instance_id));
					create
				}),
				should_seed,
				node,
			)
			.await?;

		debug!("Loaded library '{id:?}'");

		if should_seed {
			tag::seed::new_library(&library).await?;
			indexer::rules::seed::new_or_existing_library(&library).await?;
			debug!("Seeded library '{id:?}'");
		}

		invalidate_query!(library, "library.list");

		Ok(library)
	}

	/// `LoadedLibrary.id` can be used to get the library's id.
	pub async fn get_all(&self) -> Vec<Arc<Library>> {
		self.libraries
			.read()
			.await
			.iter()
			.map(|v| v.1.clone())
			.collect()
	}

	pub(crate) async fn edit(
		&self,
		id: Uuid,
		name: Option<LibraryName>,
		description: MaybeUndefined<String>,
	) -> Result<(), LibraryManagerError> {
		// check library is valid
		let libraries = self.libraries.read().await;
		let library = Arc::clone(
			libraries
				.get(&id)
				.ok_or(LibraryManagerError::LibraryNotFound)?,
		);

		library
			.update_config(
				|config| {
					// update the library
					if let Some(name) = name {
						config.name = name;
					}
					match description {
						MaybeUndefined::Undefined => {}
						MaybeUndefined::Null => config.description = None,
						MaybeUndefined::Value(description) => {
							config.description = Some(description)
						}
					}
				},
				self.libraries_dir.join(format!("{id}.sdlibrary")),
			)
			.await?;

		self.tx
			.emit(LibraryManagerEvent::Edit(Arc::clone(&library)))
			.await;

		invalidate_query!(library, "library.list");

		Ok(())
	}

	pub async fn delete(&self, id: &Uuid) -> Result<(), LibraryManagerError> {
		// As we're holding a write lock here, we know nothing will change during this function
		let mut libraries_write_guard = self.libraries.write().await;

		// TODO: Library go into "deletion" state until it's finished!

		let library = libraries_write_guard
			.get(id)
			.ok_or(LibraryManagerError::LibraryNotFound)?;

		self.tx
			.emit(LibraryManagerEvent::Delete(library.clone()))
			.await;

		if let Ok(location_paths) = library
			.db
			.location()
			.find_many(vec![])
			.select(location::select!({ path }))
			.exec()
			.await
			.map(|locations| locations.into_iter().filter_map(|location| location.path))
			.map_err(|e| error!("Failed to fetch locations for library deletion: {e:#?}"))
		{
			location_paths
				.map(|location_path| async move {
					if let Some(mut sd_metadata) =
						SpacedriveLocationMetadataFile::try_load(location_path).await?
					{
						sd_metadata.remove_library(*id).await?;
					}

					Ok::<_, LocationMetadataError>(())
				})
				.collect::<Vec<_>>()
				.join()
				.await
				.into_iter()
				.for_each(|res| {
					if let Err(e) = res {
						error!("Failed to remove library from location metadata: {e:#?}");
					}
				});
		}

		let db_path = self.libraries_dir.join(format!("{}.db", library.id));
		let sd_lib_path = self.libraries_dir.join(format!("{}.sdlibrary", library.id));

		(
			async {
				fs::remove_file(&db_path)
					.await
					.map_err(|e| LibraryManagerError::FileIO(FileIOError::from((db_path, e))))
			},
			async {
				fs::remove_file(&sd_lib_path)
					.await
					.map_err(|e| LibraryManagerError::FileIO(FileIOError::from((sd_lib_path, e))))
			},
		)
			.try_join()
			.await?;

		// We only remove here after files deletion
		let library = libraries_write_guard
			.remove(id)
			.expect("we have exclusive access and checked it exists!");

		info!("Removed Library <id='{}'>", library.id);

		invalidate_query!(library, "library.list");

		Ok(())
	}

	// get_ctx will return the library context for the given library id.
	pub async fn get_library(&self, library_id: &Uuid) -> Option<Arc<Library>> {
		self.libraries.read().await.get(library_id).cloned()
	}

	// get_ctx will return the library context for the given library id.
	pub async fn hash_library(&self, library_id: &Uuid) -> bool {
		self.libraries.read().await.get(library_id).is_some()
	}

	/// load the library from a given path.
	pub async fn load(
		self: &Arc<Self>,
		id: Uuid,
		db_path: impl AsRef<Path>,
		config_path: impl AsRef<Path>,
		create: Option<instance::Create>,
		should_seed: bool,
		node: &Arc<Node>,
	) -> Result<Arc<Library>, LibraryManagerError> {
		let db_path = db_path.as_ref();
		let config_path = config_path.as_ref();

		let db_url = format!(
			"file:{}?socket_timeout=15&connection_limit=1",
			db_path.as_os_str().to_str().ok_or_else(|| {
				LibraryManagerError::NonUtf8Path(NonUtf8PathError(db_path.into()))
			})?
		);
		let db = Arc::new(db::load_and_migrate(&db_url).await?);

		if let Some(create) = create {
			create.to_query(&db).exec().await?;
		}

		let node_config = node.config.get().await;
		let config = LibraryConfig::load(config_path, &node_config, &db).await?;

		let instance = db
			.instance()
			.find_unique(instance::id::equals(config.instance_id))
			.exec()
			.await?
			.ok_or_else(|| {
				LibraryManagerError::CurrentInstanceNotFound(config.instance_id.to_string())
			})?;
		let identity = Arc::new(
			match IdentityOrRemoteIdentity::from_bytes(&instance.identity)? {
				IdentityOrRemoteIdentity::Identity(identity) => identity,
				IdentityOrRemoteIdentity::RemoteIdentity(_) => {
					return Err(LibraryManagerError::InvalidIdentity)
				}
			},
		);

		let instance_id = Uuid::from_slice(&instance.pub_id)?;
		let curr_platform = Platform::current() as i32;
		let instance_node_id = Uuid::from_slice(&instance.node_id)?;
		if instance_node_id != node_config.id
			|| instance.node_platform != curr_platform
			|| instance.node_name != node_config.name
		{
			info!(
				"Detected that the library '{}' has changed node from '{}' to '{}'. Reconciling node data...",
				id, instance_node_id, node_config.id
			);

			db.instance()
				.update(
					instance::id::equals(instance.id),
					vec![
						instance::node_id::set(node_config.id.as_bytes().to_vec()),
						instance::node_platform::set(curr_platform),
						instance::node_name::set(node_config.name),
					],
				)
				.exec()
				.await?;
		}

		// TODO: Move this reconciliation into P2P and do reconciliation of both local and remote nodes.

		// let key_manager = Arc::new(KeyManager::new(vec![]).await?);
		// seed_keymanager(&db, &key_manager).await?;

		let mut sync = sync::Manager::new(&db, instance_id, &self.emit_messages_flag);

		let library = Library::new(
			id,
			config,
			instance_id,
			identity,
			// key_manager,
			db,
			node,
			Arc::new(sync.manager),
		)
		.await;

		// This is an exception. Generally subscribe to this by `self.tx.subscribe`.
		tokio::spawn({
			let library = library.clone();
			let node = node.clone();

			async move {
				loop {
					let Ok(msg) = sync.rx.recv().await else {
						continue;
					};

					match msg {
						// TODO: Any sync event invalidates the entire React Query cache this is a hacky workaround until the new invalidation system.
						SyncMessage::Ingested => node.emit(CoreEvent::InvalidateOperation(
							InvalidateOperationEvent::all(),
						)),
						SyncMessage::Created => {
							p2p::sync::originator(id, &library.sync, &node.p2p).await
						}
					}
				}
			}
		});

		self.tx
			.emit(LibraryManagerEvent::Load(library.clone()))
			.await;

		self.libraries
			.write()
			.await
			.insert(library.id, Arc::clone(&library));

		if should_seed {
			library.orphan_remover.invoke().await;
			indexer::rules::seed::new_or_existing_library(&library).await?;
		}

		for location in library
			.db
			.location()
			.find_many(vec![
				// TODO(N): This isn't gonna work with removable media and this will likely permanently break if the DB is restored from a backup.
				location::instance_id::equals(Some(instance.id)),
			])
			.exec()
			.await?
		{
			if let Err(e) = node.locations.add(location.id, library.clone()).await {
				error!("Failed to watch location on startup: {e}");
			};
		}

		if let Err(e) = node.jobs.clone().cold_resume(node, &library).await {
			error!("Failed to resume jobs for library. {:#?}", e);
		}

		Ok(library)
	}

	pub async fn update_instances(&self, library: Arc<Library>) {
		self.tx
			.emit(LibraryManagerEvent::InstancesModified(library))
			.await;
	}
}



File: ./src/library/mod.rs
-------------------------------------------------
// pub(crate) mod cat;
mod config;
#[allow(clippy::module_inception)]
mod library;
mod manager;
mod name;

// pub use cat::*;
pub use config::*;
pub use library::*;
pub use manager::*;
pub use name::*;

pub type LibraryId = uuid::Uuid;



File: ./src/library/name.rs
-------------------------------------------------
use std::ops::Deref;

use specta::Type;
use thiserror::Error;

use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Clone, Type)]
pub struct LibraryName(String);

#[derive(Debug, Error)]
pub enum LibraryNameError {
	#[error("empty")]
	Empty,
	#[error("needs-trim")]
	NeedsTrim,
}

impl LibraryName {
	pub fn new(name: impl Into<String>) -> Result<Self, LibraryNameError> {
		let name = name.into();

		if name.is_empty() {
			return Err(LibraryNameError::Empty);
		}

		if name.starts_with(' ') || name.ends_with(' ') {
			return Err(LibraryNameError::NeedsTrim);
		}

		Ok(Self(name))
	}
}

impl<'de> Deserialize<'de> for LibraryName {
	fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
	where
		D: serde::Deserializer<'de>,
	{
		LibraryName::new(String::deserialize(deserializer)?).map_err(serde::de::Error::custom)
	}
}

impl AsRef<str> for LibraryName {
	fn as_ref(&self) -> &str {
		&self.0
	}
}

impl Deref for LibraryName {
	type Target = String;

	fn deref(&self) -> &Self::Target {
		&self.0
	}
}

impl From<LibraryName> for String {
	fn from(name: LibraryName) -> Self {
		name.0
	}
}



File: ./src/object/orphan_remover.rs
-------------------------------------------------
use crate::prisma::{object, tag_on_object, PrismaClient};

use std::{sync::Arc, time::Duration};

use tokio::{
	select,
	sync::mpsc,
	time::{interval_at, Instant, MissedTickBehavior},
};
use tracing::{error, trace};

const TEN_SECONDS: Duration = Duration::from_secs(10);
const ONE_MINUTE: Duration = Duration::from_secs(60);

// Actor that can be invoked to find and delete objects with no matching file paths
#[derive(Clone)]
pub struct OrphanRemoverActor {
	tx: mpsc::Sender<()>,
}

impl OrphanRemoverActor {
	pub fn spawn(db: Arc<PrismaClient>) -> Self {
		let (tx, mut rx) = mpsc::channel(4);

		tokio::spawn(async move {
			let mut last_checked = Instant::now();

			let mut check_interval = interval_at(Instant::now() + ONE_MINUTE, ONE_MINUTE);
			check_interval.set_missed_tick_behavior(MissedTickBehavior::Skip);

			loop {
				// Here we wait for a signal or for the tick interval to be reached
				select! {
					_ =  check_interval.tick() => {}
					signal = rx.recv() => {
						if signal.is_none() {
							break;
						}
					}
				}

				// For any of them we process a clean up if a time since the last one already passed
				if last_checked.elapsed() > TEN_SECONDS {
					Self::process_clean_up(&db).await;
					last_checked = Instant::now();
				}
			}
		});

		Self { tx }
	}

	pub async fn invoke(&self) {
		self.tx.send(()).await.ok();
	}

	async fn process_clean_up(db: &PrismaClient) {
		loop {
			let Ok(objects_ids) = db
				.object()
				.find_many(vec![object::file_paths::none(vec![])])
				.take(512)
				.select(object::select!({ id }))
				.exec()
				.await
				.map(|objects| {
					objects
						.into_iter()
						.map(|object| object.id)
						.collect::<Vec<_>>()
				})
				.map_err(|e| error!("Failed to fetch orphaned objects: {e:#?}"))
			else {
				break;
			};

			if objects_ids.is_empty() {
				break;
			}

			trace!("Removing {} orphaned objects", objects_ids.len());

			if let Err(e) = db
				._batch((
					db.tag_on_object()
						.delete_many(vec![tag_on_object::object_id::in_vec(objects_ids.clone())]),
					db.object()
						.delete_many(vec![object::id::in_vec(objects_ids)]),
				))
				.await
			{
				error!("Failed to remove orphaned objects: {e:#?}");
			}
		}
	}
}



File: ./src/object/mod.rs
-------------------------------------------------
use crate::prisma::{file_path, object};

use serde::{Deserialize, Serialize};
use specta::Type;

pub mod cas;
pub mod file_identifier;
pub mod fs;
pub mod media;
pub mod orphan_remover;
pub mod tag;
pub mod validation;

// Objects are primarily created by the identifier from Paths
// Some Objects are purely virtual, unless they have one or more associated Paths, which refer to a file found in a Location
// Objects are what can be added to Spaces

// Object selectables!
object::select!(object_for_file_identifier {
	pub_id
	file_paths: select { pub_id cas_id extension is_dir materialized_path name }
});

// The response to provide the Explorer when looking at Objects
#[derive(Debug, Serialize, Deserialize, Type)]
pub struct ObjectsForExplorer {
	pub objects: Vec<ObjectData>,
	// pub context: ExplorerContext,
}

#[derive(Debug, Serialize, Deserialize, Type)]
pub enum ObjectData {
	Object(Box<object::Data>),
	Path(Box<file_path::Data>),
}



File: ./src/object/file_identifier/shallow.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::JobError,
	library::Library,
	location::file_path_helper::{
		ensure_file_path_exists, ensure_sub_path_is_directory, ensure_sub_path_is_in_location,
		file_path_for_file_identifier, IsolatedFilePathData,
	},
	prisma::{file_path, location, PrismaClient, SortOrder},
	util::db::maybe_missing,
};

use std::path::{Path, PathBuf};

use prisma_client_rust::or;
use serde::{Deserialize, Serialize};
use tracing::{debug, trace, warn};

use super::{process_identifier_file_paths, FileIdentifierJobError, CHUNK_SIZE};

#[derive(Serialize, Deserialize)]
pub struct ShallowFileIdentifierJobState {
	cursor: file_path::id::Type,
	sub_iso_file_path: IsolatedFilePathData<'static>,
}

pub async fn shallow(
	location: &location::Data,
	sub_path: &PathBuf,
	library: &Library,
) -> Result<(), JobError> {
	let Library { db, .. } = library;

	debug!("Identifying orphan File Paths...");

	let location_id = location.id;
	let location_path = maybe_missing(&location.path, "location.path").map(Path::new)?;

	let sub_iso_file_path = if sub_path != Path::new("") {
		let full_path = ensure_sub_path_is_in_location(location_path, &sub_path)
			.await
			.map_err(FileIdentifierJobError::from)?;
		ensure_sub_path_is_directory(location_path, &sub_path)
			.await
			.map_err(FileIdentifierJobError::from)?;

		let sub_iso_file_path =
			IsolatedFilePathData::new(location_id, location_path, &full_path, true)
				.map_err(FileIdentifierJobError::from)?;

		ensure_file_path_exists(
			&sub_path,
			&sub_iso_file_path,
			db,
			FileIdentifierJobError::SubPathNotFound,
		)
		.await?;

		sub_iso_file_path
	} else {
		IsolatedFilePathData::new(location_id, location_path, location_path, true)
			.map_err(FileIdentifierJobError::from)?
	};

	let orphan_count = count_orphan_file_paths(db, location_id, &sub_iso_file_path).await?;

	if orphan_count == 0 {
		return Ok(());
	}

	let task_count = (orphan_count as f64 / CHUNK_SIZE as f64).ceil() as usize;
	debug!(
		"Found {} orphan Paths. Will execute {} tasks...",
		orphan_count, task_count
	);

	let Some(first_path) = db
		.file_path()
		.find_first(orphan_path_filters(location_id, None, &sub_iso_file_path))
		// .order_by(file_path::id::order(Direction::Asc))
		.select(file_path::select!({ id }))
		.exec()
		.await?
	else {
		warn!("No orphan Paths found due to another Job finishing first");
		return Ok(());
	};

	// Initializing `state.data` here because we need a complete state in case of early finish
	let mut data = ShallowFileIdentifierJobState {
		cursor: first_path.id,
		sub_iso_file_path,
	};

	for step_number in 0..task_count {
		let ShallowFileIdentifierJobState {
			cursor,
			sub_iso_file_path,
		} = &mut data;

		// get chunk of orphans to process
		let file_paths =
			get_orphan_file_paths(&library.db, location.id, *cursor, sub_iso_file_path).await?;

		let (_, _, new_cursor) = process_identifier_file_paths(
			location,
			&file_paths,
			step_number,
			*cursor,
			library,
			orphan_count,
		)
		.await?;
		*cursor = new_cursor;
	}

	invalidate_query!(library, "search.paths");
	invalidate_query!(library, "search.objects");

	Ok(())
}

fn orphan_path_filters(
	location_id: location::id::Type,
	file_path_id: Option<file_path::id::Type>,
	sub_iso_file_path: &IsolatedFilePathData<'_>,
) -> Vec<file_path::WhereParam> {
	sd_utils::chain_optional_iter(
		[
			or!(
				file_path::object_id::equals(None),
				file_path::cas_id::equals(None)
			),
			file_path::is_dir::equals(Some(false)),
			file_path::location_id::equals(Some(location_id)),
			file_path::materialized_path::equals(Some(
				sub_iso_file_path
					.materialized_path_for_children()
					.expect("sub path for shallow identifier must be a directory"),
			)),
			file_path::size_in_bytes_bytes::not(Some(0u64.to_be_bytes().to_vec())),
		],
		[file_path_id.map(file_path::id::gte)],
	)
}

async fn count_orphan_file_paths(
	db: &PrismaClient,
	location_id: location::id::Type,
	sub_iso_file_path: &IsolatedFilePathData<'_>,
) -> Result<usize, prisma_client_rust::QueryError> {
	db.file_path()
		.count(orphan_path_filters(location_id, None, sub_iso_file_path))
		.exec()
		.await
		.map(|c| c as usize)
}

async fn get_orphan_file_paths(
	db: &PrismaClient,
	location_id: location::id::Type,
	file_path_id_cursor: file_path::id::Type,
	sub_iso_file_path: &IsolatedFilePathData<'_>,
) -> Result<Vec<file_path_for_file_identifier::Data>, prisma_client_rust::QueryError> {
	trace!(
		"Querying {} orphan Paths at cursor: {:?}",
		CHUNK_SIZE,
		file_path_id_cursor
	);
	db.file_path()
		.find_many(orphan_path_filters(
			location_id,
			Some(file_path_id_cursor),
			sub_iso_file_path,
		))
		.order_by(file_path::id::order(SortOrder::Asc))
		// .cursor(cursor.into())
		.take(CHUNK_SIZE as i64)
		// .skip(1)
		.select(file_path_for_file_identifier::select())
		.exec()
		.await
}



File: ./src/object/file_identifier/file_identifier_job.rs
-------------------------------------------------
use crate::{
	job::{
		CurrentStep, JobError, JobInitOutput, JobReportUpdate, JobResult, JobRunMetadata,
		JobStepOutput, StatefulJob, WorkerContext,
	},
	library::Library,
	location::file_path_helper::{
		ensure_file_path_exists, ensure_sub_path_is_directory, ensure_sub_path_is_in_location,
		file_path_for_file_identifier, IsolatedFilePathData,
	},
	prisma::{file_path, location, PrismaClient, SortOrder},
	util::db::maybe_missing,
};

use std::{
	hash::{Hash, Hasher},
	path::{Path, PathBuf},
};

use prisma_client_rust::or;
use serde::{Deserialize, Serialize};
use serde_json::json;
use tracing::{debug, info, trace};

use super::{process_identifier_file_paths, FileIdentifierJobError, CHUNK_SIZE};

/// `FileIdentifierJobInit` takes file_paths without an object_id from a location
/// or starting from a `sub_path` (getting every descendent from this `sub_path`
/// and uniquely identifies them:
/// - first: generating the cas_id and extracting metadata
/// - finally: creating unique object records, and linking them to their file_paths
#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct FileIdentifierJobInit {
	pub location: location::Data,
	pub sub_path: Option<PathBuf>, // subpath to start from
}

impl Hash for FileIdentifierJobInit {
	fn hash<H: Hasher>(&self, state: &mut H) {
		self.location.id.hash(state);
		if let Some(ref sub_path) = self.sub_path {
			sub_path.hash(state);
		}
	}
}

#[derive(Serialize, Deserialize, Debug)]
pub struct FileIdentifierJobData {
	location_path: PathBuf,
	maybe_sub_iso_file_path: Option<IsolatedFilePathData<'static>>,
}

#[derive(Serialize, Deserialize, Default, Debug)]
pub struct FileIdentifierJobRunMetadata {
	cursor: file_path::id::Type,
	total_orphan_paths: usize,
	total_objects_created: usize,
	total_objects_linked: usize,
	total_objects_ignored: usize,
}

impl JobRunMetadata for FileIdentifierJobRunMetadata {
	fn update(&mut self, new_data: Self) {
		self.total_orphan_paths += new_data.total_orphan_paths;
		self.total_objects_created += new_data.total_objects_created;
		self.total_objects_linked += new_data.total_objects_linked;
		self.total_objects_ignored += new_data.total_objects_ignored;
		self.cursor = new_data.cursor;
	}
}

#[async_trait::async_trait]
impl StatefulJob for FileIdentifierJobInit {
	type Data = FileIdentifierJobData;
	type Step = ();
	type RunMetadata = FileIdentifierJobRunMetadata;

	const NAME: &'static str = "file_identifier";
	const IS_BATCHED: bool = true;

	fn target_location(&self) -> location::id::Type {
		self.location.id
	}

	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let init = self;
		let Library { db, .. } = &*ctx.library;

		debug!("Identifying orphan File Paths...");

		let location_id = init.location.id;

		let location_path = maybe_missing(&init.location.path, "location.path").map(Path::new)?;

		let maybe_sub_iso_file_path = match &init.sub_path {
			Some(sub_path) if sub_path != Path::new("") => {
				let full_path = ensure_sub_path_is_in_location(location_path, sub_path)
					.await
					.map_err(FileIdentifierJobError::from)?;
				ensure_sub_path_is_directory(location_path, sub_path)
					.await
					.map_err(FileIdentifierJobError::from)?;

				let sub_iso_file_path =
					IsolatedFilePathData::new(location_id, location_path, &full_path, true)
						.map_err(FileIdentifierJobError::from)?;

				ensure_file_path_exists(
					sub_path,
					&sub_iso_file_path,
					db,
					FileIdentifierJobError::SubPathNotFound,
				)
				.await?;

				Some(sub_iso_file_path)
			}
			_ => None,
		};

		let orphan_count =
			count_orphan_file_paths(db, location_id, &maybe_sub_iso_file_path).await?;

		// Initializing `state.data` here because we need a complete state in case of early finish
		*data = Some(FileIdentifierJobData {
			location_path: location_path.to_path_buf(),
			maybe_sub_iso_file_path,
		});

		let data = data.as_ref().expect("we just set it");

		if orphan_count == 0 {
			return Err(JobError::EarlyFinish {
				name: <Self as StatefulJob>::NAME.to_string(),
				reason: "Found no orphan file paths to process".to_string(),
			});
		}

		debug!("Found {} orphan file paths", orphan_count);

		let task_count = (orphan_count as f64 / CHUNK_SIZE as f64).ceil() as usize;
		debug!(
			"Found {} orphan Paths. Will execute {} tasks...",
			orphan_count, task_count
		);

		let first_path = db
			.file_path()
			.find_first(orphan_path_filters(
				location_id,
				None,
				&data.maybe_sub_iso_file_path,
			))
			.select(file_path::select!({ id }))
			.exec()
			.await?
			.expect("We already validated before that there are orphans `file_path`s");

		ctx.progress(vec![
			JobReportUpdate::TaskCount(orphan_count),
			JobReportUpdate::Message(format!("Found {orphan_count} files to be identified")),
		]);

		Ok((
			FileIdentifierJobRunMetadata {
				total_orphan_paths: orphan_count,
				cursor: first_path.id,
				..Default::default()
			},
			vec![(); task_count],
		)
			.into())
	}

	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		CurrentStep { step_number, .. }: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		run_metadata: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		let init = self;
		let location = &init.location;

		let mut new_metadata = Self::RunMetadata::default();

		// get chunk of orphans to process
		let file_paths = get_orphan_file_paths(
			&ctx.library.db,
			location.id,
			run_metadata.cursor,
			&data.maybe_sub_iso_file_path,
		)
		.await?;

		// if no file paths found, abort entire job early, there is nothing to do
		// if we hit this error, there is something wrong with the data/query
		if file_paths.is_empty() {
			return Err(JobError::EarlyFinish {
				name: <Self as StatefulJob>::NAME.to_string(),
				reason: "Expected orphan Paths not returned from database query for this chunk"
					.to_string(),
			});
		}

		let (total_objects_created, total_objects_linked, new_cursor) =
			process_identifier_file_paths(
				location,
				&file_paths,
				step_number,
				run_metadata.cursor,
				&ctx.library,
				run_metadata.total_orphan_paths,
			)
			.await?;

		new_metadata.total_objects_created = total_objects_created;
		new_metadata.total_objects_linked = total_objects_linked;
		new_metadata.cursor = new_cursor;

		ctx.progress(vec![
			JobReportUpdate::CompletedTaskCount(step_number * CHUNK_SIZE + file_paths.len()),
			JobReportUpdate::Message(format!(
				"Processed {} of {} orphan Paths",
				step_number * CHUNK_SIZE,
				run_metadata.total_orphan_paths
			)),
		]);

		Ok(new_metadata.into())
	}

	async fn finalize(
		&self,
		_: &WorkerContext,
		_data: &Option<Self::Data>,
		run_metadata: &Self::RunMetadata,
	) -> JobResult {
		let init = self;
		info!("Finalizing identifier job: {:?}", &run_metadata);

		Ok(Some(json!({"init: ": init, "run_metadata": run_metadata})))
	}
}

fn orphan_path_filters(
	location_id: location::id::Type,
	file_path_id: Option<file_path::id::Type>,
	maybe_sub_iso_file_path: &Option<IsolatedFilePathData<'_>>,
) -> Vec<file_path::WhereParam> {
	sd_utils::chain_optional_iter(
		[
			or!(
				file_path::object_id::equals(None),
				file_path::cas_id::equals(None)
			),
			file_path::is_dir::equals(Some(false)),
			file_path::location_id::equals(Some(location_id)),
			file_path::size_in_bytes_bytes::not(Some(0u64.to_be_bytes().to_vec())),
		],
		[
			// this is a workaround for the cursor not working properly
			file_path_id.map(file_path::id::gte),
			maybe_sub_iso_file_path.as_ref().map(|sub_iso_file_path| {
				file_path::materialized_path::starts_with(
					sub_iso_file_path
						.materialized_path_for_children()
						.expect("sub path iso_file_path must be a directory"),
				)
			}),
		],
	)
}

async fn count_orphan_file_paths(
	db: &PrismaClient,
	location_id: location::id::Type,
	maybe_sub_materialized_path: &Option<IsolatedFilePathData<'_>>,
) -> Result<usize, prisma_client_rust::QueryError> {
	db.file_path()
		.count(orphan_path_filters(
			location_id,
			None,
			maybe_sub_materialized_path,
		))
		.exec()
		.await
		.map(|c| c as usize)
}

async fn get_orphan_file_paths(
	db: &PrismaClient,
	location_id: location::id::Type,
	file_path_id: file_path::id::Type,
	maybe_sub_materialized_path: &Option<IsolatedFilePathData<'_>>,
) -> Result<Vec<file_path_for_file_identifier::Data>, prisma_client_rust::QueryError> {
	trace!(
		"Querying {} orphan Paths at cursor: {:?}",
		CHUNK_SIZE,
		file_path_id
	);
	db.file_path()
		.find_many(orphan_path_filters(
			location_id,
			Some(file_path_id),
			maybe_sub_materialized_path,
		))
		.order_by(file_path::id::order(SortOrder::Asc))
		.take(CHUNK_SIZE as i64)
		// .skip(1)
		.select(file_path_for_file_identifier::select())
		.exec()
		.await
}



File: ./src/object/file_identifier/mod.rs
-------------------------------------------------
use crate::{
	job::JobError,
	library::Library,
	location::file_path_helper::{
		file_path_for_file_identifier, FilePathError, IsolatedFilePathData,
	},
	object::{cas::generate_cas_id, object_for_file_identifier},
	prisma::{file_path, location, object, PrismaClient},
	util::{db::maybe_missing, error::FileIOError},
};

use sd_file_ext::{extensions::Extension, kind::ObjectKind};

use sd_prisma::prisma_sync;
use sd_sync::{CRDTOperation, OperationFactory};
use sd_utils::uuid_to_bytes;

use std::{
	collections::{HashMap, HashSet},
	fmt::Debug,
	path::Path,
};

use futures::future::join_all;
use serde_json::json;
use tokio::fs;
use tracing::{error, trace};
use uuid::Uuid;

pub mod file_identifier_job;
mod shallow;

pub use shallow::*;

// we break these jobs into chunks of 100 to improve performance
const CHUNK_SIZE: usize = 100;

#[derive(thiserror::Error, Debug)]
pub enum FileIdentifierJobError {
	#[error("received sub path not in database: <path='{}'>", .0.display())]
	SubPathNotFound(Box<Path>),

	// Internal Errors
	#[error(transparent)]
	FilePathError(#[from] FilePathError),
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
}

#[derive(Debug, Clone)]
pub struct FileMetadata {
	pub cas_id: Option<String>,
	pub kind: ObjectKind,
	pub fs_metadata: std::fs::Metadata,
}

impl FileMetadata {
	/// Assembles `create_unchecked` params for a given file path
	pub async fn new(
		location_path: impl AsRef<Path>,
		iso_file_path: &IsolatedFilePathData<'_>, // TODO: use dedicated CreateUnchecked type
	) -> Result<FileMetadata, FileIOError> {
		let path = location_path.as_ref().join(iso_file_path);

		let fs_metadata = fs::metadata(&path)
			.await
			.map_err(|e| FileIOError::from((&path, e)))?;

		assert!(
			!fs_metadata.is_dir(),
			"We can't generate cas_id for directories"
		);

		// derive Object kind
		let kind = Extension::resolve_conflicting(&path, false)
			.await
			.map(Into::into)
			.unwrap_or(ObjectKind::Unknown);

		let cas_id = if fs_metadata.len() != 0 {
			generate_cas_id(&path, fs_metadata.len())
				.await
				.map(Some)
				.map_err(|e| FileIOError::from((&path, e)))?
		} else {
			// We can't do shit with empty files
			None
		};

		trace!("Analyzed file: {path:?} {cas_id:?} {kind:?}");

		Ok(FileMetadata {
			cas_id,
			kind,
			fs_metadata,
		})
	}
}

async fn identifier_job_step(
	Library { db, sync, .. }: &Library,
	location: &location::Data,
	file_paths: &[file_path_for_file_identifier::Data],
) -> Result<(usize, usize), JobError> {
	let location_path = maybe_missing(&location.path, "location.path").map(Path::new)?;

	let file_paths_metadatas = join_all(
		file_paths
			.iter()
			.filter_map(|file_path| {
				IsolatedFilePathData::try_from((location.id, file_path))
					.map(|iso_file_path| (iso_file_path, file_path))
					.map_err(|e| error!("Failed to extract isolated file path data: {e:#?}"))
					.ok()
			})
			.map(|(iso_file_path, file_path)| async move {
				FileMetadata::new(&location_path, &iso_file_path)
					.await
					.map(|metadata| {
						(
							// SAFETY: This should never happen
							Uuid::from_slice(&file_path.pub_id)
								.expect("file_path.pub_id is invalid!"),
							(metadata, file_path),
						)
					})
					.map_err(|e| {
						#[cfg(target_os = "windows")]
						{
							// Handle case where file is on-demand (NTFS only)
							if e.source.raw_os_error().map_or(false, |code| code == 362) {
								error!("Failed to extract metadata from on-demand file: {e:#?}");
							} else {
								error!("Failed to extract file metadata: {e:#?}")
							}
						}

						#[cfg(not(target_os = "windows"))]
						{
							error!("Failed to extract file metadata: {e:#?}");
						}
					})
					.ok()
			}),
	)
	.await
	.into_iter()
	.flatten()
	.collect::<HashMap<_, _>>();

	let unique_cas_ids = file_paths_metadatas
		.values()
		.filter_map(|(metadata, _)| metadata.cas_id.clone())
		.collect::<HashSet<_>>()
		.into_iter()
		.collect();

	// Assign cas_id to each file path
	sync.write_ops(
		db,
		file_paths_metadatas
			.iter()
			.map(|(pub_id, (metadata, _))| {
				(
					sync.shared_update(
						prisma_sync::file_path::SyncId {
							pub_id: sd_utils::uuid_to_bytes(*pub_id),
						},
						file_path::cas_id::NAME,
						json!(&metadata.cas_id),
					),
					db.file_path().update(
						file_path::pub_id::equals(sd_utils::uuid_to_bytes(*pub_id)),
						vec![file_path::cas_id::set(metadata.cas_id.clone())],
					),
				)
			})
			.unzip::<_, _, _, Vec<_>>(),
	)
	.await?;

	// Retrieves objects that are already connected to file paths with the same id
	let existing_objects = db
		.object()
		.find_many(vec![object::file_paths::some(vec![
			file_path::cas_id::in_vec(unique_cas_ids),
		])])
		.select(object_for_file_identifier::select())
		.exec()
		.await?;

	let existing_object_cas_ids = existing_objects
		.iter()
		.flat_map(|object| {
			object
				.file_paths
				.iter()
				.filter_map(|file_path| file_path.cas_id.as_ref())
		})
		.collect::<HashSet<_>>();

	// Attempt to associate each file path with an object that has been
	// connected to file paths with the same cas_id
	let updated_file_paths = sync
		.write_ops(
			db,
			file_paths_metadatas
				.iter()
				.filter_map(|(pub_id, (metadata, file_path))| {
					// Filtering out files without cas_id due to being empty
					metadata
						.cas_id
						.is_some()
						.then_some((pub_id, (metadata, file_path)))
				})
				.flat_map(|(pub_id, (metadata, _))| {
					existing_objects
						.iter()
						.find(|object| {
							object
								.file_paths
								.iter()
								.any(|file_path| file_path.cas_id == metadata.cas_id)
						})
						.map(|object| (*pub_id, object))
				})
				.map(|(pub_id, object)| {
					let (crdt_op, db_op) = connect_file_path_to_object(
						pub_id,
						// SAFETY: This pub_id is generated by the uuid lib, but we have to store bytes in sqlite
						Uuid::from_slice(&object.pub_id).expect("uuid bytes are invalid"),
						sync,
						db,
					);

					(crdt_op, db_op.select(file_path::select!({ pub_id })))
				})
				.unzip::<_, _, Vec<_>, Vec<_>>(),
		)
		.await?;

	trace!(
		"Found {} existing Objects in Library, linking file paths...",
		existing_objects.len()
	);

	// extract objects that don't already exist in the database
	let file_paths_requiring_new_object = file_paths_metadatas
		.into_iter()
		.filter(|(_, (FileMetadata { cas_id, .. }, _))| {
			cas_id
				.as_ref()
				.map(|cas_id| !existing_object_cas_ids.contains(cas_id))
				.unwrap_or(true)
		})
		.collect::<Vec<_>>();

	let total_created = if !file_paths_requiring_new_object.is_empty() {
		trace!(
			"Creating {} new Objects in Library",
			file_paths_requiring_new_object.len(),
		);

		let (object_create_args, file_path_update_args): (Vec<_>, Vec<_>) =
			file_paths_requiring_new_object
				.iter()
				.map(
					|(
						file_path_pub_id,
						(
							FileMetadata { kind, .. },
							file_path_for_file_identifier::Data { date_created, .. },
						),
					)| {
						let object_pub_id = Uuid::new_v4();
						let sync_id = || prisma_sync::object::SyncId {
							pub_id: sd_utils::uuid_to_bytes(object_pub_id),
						};

						let kind = *kind as i32;

						let (sync_params, db_params): (Vec<_>, Vec<_>) = [
							(
								(object::date_created::NAME, json!(date_created)),
								object::date_created::set(*date_created),
							),
							(
								(object::kind::NAME, json!(kind)),
								object::kind::set(Some(kind)),
							),
						]
						.into_iter()
						.unzip();

						(
							(
								sync.shared_create(sync_id(), sync_params),
								object::create_unchecked(uuid_to_bytes(object_pub_id), db_params),
							),
							{
								let (crdt_op, db_op) = connect_file_path_to_object(
									*file_path_pub_id,
									object_pub_id,
									sync,
									db,
								);

								(crdt_op, db_op.select(file_path::select!({ pub_id })))
							},
						)
					},
				)
				.unzip();

		// create new object records with assembled values
		let total_created_files = sync
			.write_ops(db, {
				let (sync, db_params): (Vec<_>, Vec<_>) = object_create_args.into_iter().unzip();

				(
					sync.into_iter().flatten().collect(),
					db.object().create_many(db_params),
				)
			})
			.await
			.unwrap_or_else(|e| {
				error!("Error inserting files: {:#?}", e);
				0
			});

		trace!("Created {} new Objects in Library", total_created_files);

		if total_created_files > 0 {
			trace!("Updating file paths with created objects");

			sync.write_ops(db, {
				let data: (Vec<_>, Vec<_>) = file_path_update_args.into_iter().unzip();

				data
			})
			.await?;

			trace!("Updated file paths with created objects");
		}

		total_created_files as usize
	} else {
		0
	};

	Ok((total_created, updated_file_paths.len()))
}

fn connect_file_path_to_object<'db>(
	file_path_id: Uuid,
	object_id: Uuid,
	sync: &crate::sync::Manager,
	db: &'db PrismaClient,
) -> (CRDTOperation, file_path::UpdateQuery<'db>) {
	#[cfg(debug_assertions)]
	trace!("Connecting <FilePath id={file_path_id}> to <Object pub_id={object_id}'>");

	let vec_id = object_id.as_bytes().to_vec();

	(
		sync.shared_update(
			prisma_sync::file_path::SyncId {
				pub_id: sd_utils::uuid_to_bytes(file_path_id),
			},
			file_path::object::NAME,
			json!(prisma_sync::object::SyncId {
				pub_id: vec_id.clone()
			}),
		),
		db.file_path().update(
			file_path::pub_id::equals(sd_utils::uuid_to_bytes(file_path_id)),
			vec![file_path::object::connect(object::pub_id::equals(vec_id))],
		),
	)
}

async fn process_identifier_file_paths(
	location: &location::Data,
	file_paths: &[file_path_for_file_identifier::Data],
	step_number: usize,
	cursor: file_path::id::Type,
	library: &Library,
	orphan_count: usize,
) -> Result<(usize, usize, file_path::id::Type), JobError> {
	trace!(
		"Processing {:?} orphan Paths. ({} completed of {})",
		file_paths.len(),
		step_number,
		orphan_count
	);

	let (total_objects_created, total_objects_linked) =
		identifier_job_step(library, location, file_paths).await?;

	Ok((
		total_objects_created,
		total_objects_linked,
		// returns a new cursor to the last row of this chunk or the current one
		file_paths
			.last()
			.map(|last_row| last_row.id)
			.unwrap_or(cursor),
	))
}



File: ./src/object/cas.rs
-------------------------------------------------
use std::path::Path;

use blake3::Hasher;
use static_assertions::const_assert;
use tokio::{
	fs::{self, File},
	io::{self, AsyncReadExt, AsyncSeekExt, SeekFrom},
};

const SAMPLE_COUNT: u64 = 4;
const SAMPLE_SIZE: u64 = 1024 * 10;
const HEADER_OR_FOOTER_SIZE: u64 = 1024 * 8;

// minimum file size of 100KiB, to avoid sample hashing for small files as they can be smaller than the total sample size
const MINIMUM_FILE_SIZE: u64 = 1024 * 100;

// Asserting that nobody messed up our consts
const_assert!((HEADER_OR_FOOTER_SIZE * 2 + SAMPLE_COUNT * SAMPLE_SIZE) < MINIMUM_FILE_SIZE);

// Asserting that the sample size is larger than header/footer size, as the same buffer is used for both
const_assert!(SAMPLE_SIZE > HEADER_OR_FOOTER_SIZE);

pub async fn generate_cas_id(path: impl AsRef<Path>, size: u64) -> Result<String, io::Error> {
	let mut hasher = Hasher::new();
	hasher.update(&size.to_le_bytes());

	if size <= MINIMUM_FILE_SIZE {
		// For small files, we hash the whole file
		hasher.update(&fs::read(path).await?);
	} else {
		let mut file = File::open(path).await?;
		let mut buf = vec![0; SAMPLE_SIZE as usize].into_boxed_slice();

		// Hashing the header
		let mut current_pos = file
			.read_exact(&mut buf[..HEADER_OR_FOOTER_SIZE as usize])
			.await? as u64;
		hasher.update(&buf[..HEADER_OR_FOOTER_SIZE as usize]);

		// Sample hashing the inner content of the file
		let seek_jump = (size - HEADER_OR_FOOTER_SIZE * 2) / SAMPLE_COUNT;
		loop {
			file.read_exact(&mut buf).await?;
			hasher.update(&buf);

			if current_pos >= (HEADER_OR_FOOTER_SIZE + seek_jump * (SAMPLE_COUNT - 1)) {
				break;
			}

			current_pos = file.seek(SeekFrom::Start(current_pos + seek_jump)).await?;
		}

		// Hashing the footer
		file.seek(SeekFrom::End(-(HEADER_OR_FOOTER_SIZE as i64)))
			.await?;
		file.read_exact(&mut buf[..HEADER_OR_FOOTER_SIZE as usize])
			.await?;
		hasher.update(&buf[..HEADER_OR_FOOTER_SIZE as usize]);
	}

	Ok(hasher.finalize().to_hex()[..16].to_string())
}



File: ./src/object/fs/copy.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{
		CurrentStep, JobError, JobInitOutput, JobResult, JobRunErrors, JobStepOutput, StatefulJob,
		WorkerContext,
	},
	library::Library,
	location::file_path_helper::{join_location_relative_path, IsolatedFilePathData},
	prisma::{file_path, location},
	util::{db::maybe_missing, error::FileIOError},
};

use std::{hash::Hash, path::PathBuf};

use futures_concurrency::future::TryJoin;
use serde::{Deserialize, Serialize};
use serde_json::json;
use specta::Type;
use tokio::{fs, io};
use tracing::{trace, warn};

use super::{
	construct_target_filename, error::FileSystemJobsError, fetch_source_and_target_location_paths,
	find_available_filename_for_duplicate, get_file_data_from_isolated_file_path,
	get_many_files_datas, FileData,
};

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct FileCopierJobData {
	sources_location_path: PathBuf,
}

#[derive(Serialize, Deserialize, Hash, Type, Debug)]
pub struct FileCopierJobInit {
	pub source_location_id: location::id::Type,
	pub target_location_id: location::id::Type,
	pub sources_file_path_ids: Vec<file_path::id::Type>,
	pub target_location_relative_directory_path: PathBuf,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct FileCopierJobStep {
	pub source_file_data: FileData,
	pub target_full_path: PathBuf,
}

#[async_trait::async_trait]
impl StatefulJob for FileCopierJobInit {
	type Data = FileCopierJobData;
	type Step = FileCopierJobStep;
	type RunMetadata = ();

	const NAME: &'static str = "file_copier";

	fn target_location(&self) -> location::id::Type {
		self.target_location_id
	}

	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let init = self;
		let Library { db, .. } = &*ctx.library;

		let (sources_location_path, targets_location_path) =
			fetch_source_and_target_location_paths(
				db,
				init.source_location_id,
				init.target_location_id,
			)
			.await?;

		let steps = get_many_files_datas(db, &sources_location_path, &init.sources_file_path_ids)
			.await?
			.into_iter()
			.map(|file_data| async {
				// add the currently viewed subdirectory to the location root
				let mut full_target_path = join_location_relative_path(
					&targets_location_path,
					&init.target_location_relative_directory_path,
				);

				full_target_path.push(construct_target_filename(&file_data)?);

				if file_data.full_path == full_target_path {
					full_target_path =
						find_available_filename_for_duplicate(full_target_path).await?;
				}

				Ok::<_, FileSystemJobsError>(FileCopierJobStep {
					source_file_data: file_data,
					target_full_path: full_target_path,
				})
			})
			.collect::<Vec<_>>()
			.try_join()
			.await?;

		*data = Some(FileCopierJobData {
			sources_location_path,
		});

		Ok(steps.into())
	}

	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		CurrentStep {
			step: FileCopierJobStep {
				source_file_data,
				target_full_path,
			},
			..
		}: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		_: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		let init = self;

		if maybe_missing(source_file_data.file_path.is_dir, "file_path.is_dir")? {
			let mut more_steps = Vec::new();

			fs::create_dir_all(target_full_path)
				.await
				.map_err(|e| FileIOError::from((target_full_path, e)))?;

			let mut read_dir = fs::read_dir(&source_file_data.full_path)
				.await
				.map_err(|e| FileIOError::from((&source_file_data.full_path, e)))?;

			while let Some(children_entry) = read_dir
				.next_entry()
				.await
				.map_err(|e| FileIOError::from((&source_file_data.full_path, e)))?
			{
				let children_path = children_entry.path();
				let target_children_full_path = target_full_path.join(
					children_path
						.strip_prefix(&source_file_data.full_path)
						.expect("We got the children path from the read_dir, so it should be a child of the source path"),
				);

				match get_file_data_from_isolated_file_path(
					&ctx.library.db,
					&data.sources_location_path,
					&IsolatedFilePathData::new(
						init.source_location_id,
						&data.sources_location_path,
						&children_path,
						children_entry
							.metadata()
							.await
							.map_err(|e| FileIOError::from((&children_path, e)))?
							.is_dir(),
					)
					.map_err(FileSystemJobsError::from)?,
				)
				.await
				{
					Ok(source_file_data) => {
						// Currently not supporting file_name suffixes children files in a directory being copied
						more_steps.push(FileCopierJobStep {
							target_full_path: target_children_full_path,
							source_file_data,
						});
					}
					Err(FileSystemJobsError::FilePathNotFound(path)) => {
						// FilePath doesn't exist in the database, it possibly wasn't indexed, so we skip it
						warn!(
							"Skipping duplicating {} as it wasn't indexed",
							path.display()
						);
					}
					Err(e) => return Err(e.into()),
				}
			}

			Ok(more_steps.into())
		} else {
			match fs::metadata(target_full_path).await {
				Ok(_) => {
					// Already exist a file with this name, so we need to find an available name
					match find_available_filename_for_duplicate(target_full_path).await {
						Ok(new_path) => {
							fs::copy(&source_file_data.full_path, &new_path)
								.await
								// Using the ? here because we don't want to increase the completed task
								// count in case of file system errors
								.map_err(|e| FileIOError::from((new_path, e)))?;

							Ok(().into())
						}

						Err(FileSystemJobsError::FailedToFindAvailableName(path)) => {
							Ok(JobRunErrors(vec![
								FileSystemJobsError::WouldOverwrite(path).to_string()
							])
							.into())
						}

						Err(e) => Err(e.into()),
					}
				}
				Err(e) if e.kind() == io::ErrorKind::NotFound => {
					trace!(
						"Copying from {} to {}",
						source_file_data.full_path.display(),
						target_full_path.display()
					);

					fs::copy(&source_file_data.full_path, &target_full_path)
						.await
						// Using the ? here because we don't want to increase the completed task
						// count in case of file system errors
						.map_err(|e| FileIOError::from((target_full_path, e)))?;

					Ok(().into())
				}
				Err(e) => Err(FileIOError::from((target_full_path, e)).into()),
			}
		}
	}

	async fn finalize(
		&self,
		ctx: &WorkerContext,
		_data: &Option<Self::Data>,
		_run_metadata: &Self::RunMetadata,
	) -> JobResult {
		let init = self;

		invalidate_query!(ctx.library, "search.paths");

		Ok(Some(json!({ "init": init })))
	}
}



File: ./src/object/fs/decrypt.rs
-------------------------------------------------
// use crate::{
// 	invalidate_query,
// 	job::{
// 		JobError, JobInitData, JobReportUpdate, JobResult, JobState, StatefulJob, WorkerContext,
// 	},
// 	library::Library,
// 	location::{file_path_helper:: location::id::Type},
// 	util::error::FileIOError,
// };

// use sd_crypto::{crypto::Decryptor, header::file::FileHeader, Protected};

// use serde::{Deserialize, Serialize};
// use specta::Type;
// use tokio::fs::File;

// use super::{get_location_path_from_location_id, get_many_files_datas, FileData, BYTES_EXT};
// pub struct FileDecryptorJob;

// // decrypt could have an option to restore metadata (and another specific option for file name? - would turn "output file" into "output path" in the UI)
// #[derive(Serialize, Deserialize, Debug, Type, Hash)]
// pub struct FileDecryptorJobInit {
// 	pub location_id: location::id::Type,
// 	pub file_path_ids: Vec<file_path::id::Type>,
// 	pub mount_associated_key: bool,
// 	pub password: Option<String>, // if this is set, we can assume the user chose password decryption
// 	pub save_to_library: Option<bool>,
// }

// impl JobInitData for FileDecryptorJobInit {
// 	type Job = FileDecryptorJob;
// }

// #[async_trait::async_trait]
// impl StatefulJob for FileDecryptorJob {
// 	type Init = FileDecryptorJobInit;
// 	type Data = ();
// 	type Step = FileData;

// 	const NAME: &'static str = "file_decryptor";

// 	fn new() -> Self {
// 		Self {}
// 	}

// 	async fn init(&self, ctx: WorkerContext, state: &mut JobState<Self>) -> Result<(), JobError> {
// 		let Library { db, .. } = &*ctx.library;

// 		state.steps = get_many_files_datas(
// 			db,
// 			get_location_path_from_location_id(db, state.init.location_id).await?,
// 			&state.init.file_path_ids,
// 		)
// 		.await?
// 		.into();

// 		ctx.progress(vec![JobReportUpdate::TaskCount(state.steps.len())]);

// 		Ok(())
// 	}

// 	async fn execute_step(
// 		&self,
// 		ctx: WorkerContext,
// 		state: &mut JobState<Self>,
// 	) -> Result<(), JobError> {
// 		let step = &state.steps[0];
// 		let key_manager = &ctx.library.key_manager;

// 		// handle overwriting checks, and making sure there's enough available space
// 		let output_path = {
// 			let mut path = step.full_path.clone();
// 			let extension = path.extension().map_or("decrypted", |ext| {
// 				if ext == BYTES_EXT {
// 					""
// 				} else {
// 					"decrypted"
// 				}
// 			});
// 			path.set_extension(extension);
// 			path
// 		};

// 		let mut reader = File::open(&step.full_path)
// 			.await
// 			.map_err(|e| FileIOError::from((&step.full_path, e)))?;
// 		let mut writer = File::create(&output_path)
// 			.await
// 			.map_err(|e| FileIOError::from((output_path, e)))?;

// 		let (header, aad) = FileHeader::from_reader(&mut reader).await?;

// 		let master_key = if let Some(password) = state.init.password.clone() {
// 			if let Some(save_to_library) = state.init.save_to_library {
// 				// we can do this first, as `find_key_index` requires a successful decryption (just like `decrypt_master_key`)
// 				let password_bytes = Protected::new(password.as_bytes().to_vec());

// 				if save_to_library {
// 					let index = header.find_key_index(password_bytes.clone()).await?;

// 					// inherit the encryption algorithm from the keyslot
// 					key_manager
// 						.add_to_keystore(
// 							Protected::new(password),
// 							header.algorithm,
// 							header.keyslots[index].hashing_algorithm,
// 							false,
// 							false,
// 							Some(header.keyslots[index].salt),
// 						)
// 						.await?;
// 				}

// 				header.decrypt_master_key(password_bytes).await?
// 			} else {
// 				return Err(JobError::JobDataNotFound(String::from(
// 					"Password decryption selected, but save to library boolean was not included",
// 				)));
// 			}
// 		} else {
// 			if state.init.mount_associated_key {
// 				for key in key_manager.dump_keystore().iter().filter(|x| {
// 					header
// 						.keyslots
// 						.iter()
// 						.any(|k| k.content_salt == x.content_salt)
// 				}) {
// 					key_manager.mount(key.uuid).await.ok();
// 				}
// 			}

// 			let keys = key_manager.enumerate_hashed_keys();

// 			header.decrypt_master_key_from_prehashed(keys).await?
// 		};

// 		let decryptor = Decryptor::new(master_key, header.nonce, header.algorithm)?;

// 		decryptor
// 			.decrypt_streams(&mut reader, &mut writer, &aad)
// 			.await?;

// 		// need to decrypt preview media/metadata, and maybe add an option in the UI so the user can chosoe to restore these values
// 		// for now this can't easily be implemented, as we don't know what the new object id for the file will be (we know the old one, but it may differ)

// 		ctx.progress(vec![JobReportUpdate::CompletedTaskCount(
// 			state.step_number + 1,
// 		)]);

// 		Ok(())
// 	}

// 	async fn finalize(&self, ctx: WorkerContext, state: &mut JobState<Self>) -> JobResult {
// 		invalidate_query!(ctx.library, "search.paths");

// 		// mark job as successful
// 		Ok(Some(serde_json::to_value(&state.init)?))
// 	}
// }



File: ./src/object/fs/cut.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{
		CurrentStep, JobError, JobInitOutput, JobResult, JobRunErrors, JobStepOutput, StatefulJob,
		WorkerContext,
	},
	library::Library,
	location::file_path_helper::push_location_relative_path,
	object::fs::{construct_target_filename, error::FileSystemJobsError},
	prisma::{file_path, location},
	util::error::FileIOError,
};

use std::{hash::Hash, path::PathBuf};

use serde::{Deserialize, Serialize};
use serde_json::json;
use specta::Type;
use tokio::{fs, io};
use tracing::{trace, warn};

use super::{fetch_source_and_target_location_paths, get_many_files_datas, FileData};

#[derive(Serialize, Deserialize, Hash, Type, Debug)]
pub struct FileCutterJobInit {
	pub source_location_id: location::id::Type,
	pub target_location_id: location::id::Type,
	pub sources_file_path_ids: Vec<file_path::id::Type>,
	pub target_location_relative_directory_path: PathBuf,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct FileCutterJobData {
	full_target_directory_path: PathBuf,
}

#[async_trait::async_trait]
impl StatefulJob for FileCutterJobInit {
	type Data = FileCutterJobData;
	type Step = FileData;
	type RunMetadata = ();

	const NAME: &'static str = "file_cutter";

	fn target_location(&self) -> location::id::Type {
		self.target_location_id
	}

	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let init = self;
		let Library { db, .. } = &*ctx.library;

		let (sources_location_path, targets_location_path) =
			fetch_source_and_target_location_paths(
				db,
				init.source_location_id,
				init.target_location_id,
			)
			.await?;

		let full_target_directory_path = push_location_relative_path(
			targets_location_path,
			&init.target_location_relative_directory_path,
		);

		*data = Some(FileCutterJobData {
			full_target_directory_path,
		});

		let steps =
			get_many_files_datas(db, &sources_location_path, &init.sources_file_path_ids).await?;

		Ok(steps.into())
	}

	async fn execute_step(
		&self,
		_: &WorkerContext,
		CurrentStep {
			step: file_data, ..
		}: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		_: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		let full_output = data
			.full_target_directory_path
			.join(construct_target_filename(file_data)?);

		if file_data.full_path == full_output {
			// File is already here, do nothing
			Ok(().into())
		} else {
			match fs::metadata(&full_output).await {
				Ok(_) => {
					warn!(
						"Skipping {} as it would be overwritten",
						full_output.display()
					);

					Ok(JobRunErrors(vec![FileSystemJobsError::WouldOverwrite(
						full_output.into_boxed_path(),
					)
					.to_string()])
					.into())
				}
				Err(e) if e.kind() == io::ErrorKind::NotFound => {
					trace!(
						"Cutting {} to {}",
						file_data.full_path.display(),
						full_output.display()
					);

					fs::rename(&file_data.full_path, &full_output)
						.await
						.map_err(|e| FileIOError::from((&file_data.full_path, e)))?;

					Ok(().into())
				}

				Err(e) => return Err(FileIOError::from((&full_output, e)).into()),
			}
		}
	}

	async fn finalize(
		&self,
		ctx: &WorkerContext,
		_data: &Option<Self::Data>,
		_run_metadata: &Self::RunMetadata,
	) -> JobResult {
		let init = self;
		invalidate_query!(ctx.library, "search.paths");

		Ok(Some(json!({ "init": init })))
	}
}



File: ./src/object/fs/convert.rs
-------------------------------------------------



File: ./src/object/fs/error.rs
-------------------------------------------------
use crate::{
	location::{file_path_helper::FilePathError, LocationError},
	prisma::file_path,
	util::{
		db::MissingFieldError,
		error::{FileIOError, NonUtf8PathError},
	},
};

use std::path::Path;

use prisma_client_rust::QueryError;
use thiserror::Error;

/// Error type for file system related jobs errors
#[derive(Error, Debug)]
pub enum FileSystemJobsError {
	#[error("Location error: {0}")]
	Location(#[from] LocationError),
	#[error("file_path not in database: <path='{}'>", .0.display())]
	FilePathNotFound(Box<Path>),
	#[error("file_path id not in database: <id='{0}'>")]
	FilePathIdNotFound(file_path::id::Type),
	#[error("failed to create file or folder on disk")]
	CreateFileOrFolder(FileIOError),
	#[error("database error: {0}")]
	Database(#[from] QueryError),
	#[error(transparent)]
	FilePath(#[from] FilePathError),
	#[error("action would overwrite another file: {}", .0.display())]
	WouldOverwrite(Box<Path>),
	#[error("missing-field: {0}")]
	MissingField(#[from] MissingFieldError),
	#[error("no parent for path, which is supposed to be directory: <path='{}'>", .0.display())]
	MissingParentPath(Box<Path>),
	#[error("no stem on file path, but it's supposed to be a file: <path='{}'>", .0.display())]
	MissingFileStem(Box<Path>),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	NonUTF8Path(#[from] NonUtf8PathError),
	#[error("failed to find an available name to avoid duplication: <path='{}'>", .0.display())]
	FailedToFindAvailableName(Box<Path>),
}

impl From<FileSystemJobsError> for rspc::Error {
	fn from(e: FileSystemJobsError) -> Self {
		Self::with_cause(rspc::ErrorCode::InternalServerError, e.to_string(), e)
	}
}



File: ./src/object/fs/delete.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{
		CurrentStep, JobError, JobInitOutput, JobResult, JobStepOutput, StatefulJob, WorkerContext,
	},
	library::Library,
	location::get_location_path_from_location_id,
	prisma::{file_path, location},
	util::{db::maybe_missing, error::FileIOError},
};

use std::hash::Hash;

use serde::{Deserialize, Serialize};
use serde_json::json;
use specta::Type;
use tokio::{fs, io};
use tracing::warn;

use super::{error::FileSystemJobsError, get_many_files_datas, FileData};

#[derive(Serialize, Deserialize, Hash, Type, Debug)]
pub struct FileDeleterJobInit {
	pub location_id: location::id::Type,
	pub file_path_ids: Vec<file_path::id::Type>,
}

#[async_trait::async_trait]
impl StatefulJob for FileDeleterJobInit {
	type Data = ();
	type Step = FileData;
	type RunMetadata = ();

	const NAME: &'static str = "file_deleter";

	fn target_location(&self) -> location::id::Type {
		self.location_id
	}

	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let init = self;
		let Library { db, .. } = &*ctx.library;

		let steps = get_many_files_datas(
			db,
			get_location_path_from_location_id(db, init.location_id).await?,
			&init.file_path_ids,
		)
		.await
		.map_err(FileSystemJobsError::from)?;

		// Must fill in the data, otherwise the job will not run
		*data = Some(());

		Ok(steps.into())
	}

	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		CurrentStep { step, .. }: CurrentStep<'_, Self::Step>,
		_: &Self::Data,
		_: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		// need to handle stuff such as querying prisma for all paths of a file, and deleting all of those if requested (with a checkbox in the ui)
		// maybe a files.countOccurances/and or files.getPath(location_id, path_id) to show how many of these files would be deleted (and where?)

		match if maybe_missing(step.file_path.is_dir, "file_path.is_dir")? {
			fs::remove_dir_all(&step.full_path).await
		} else {
			fs::remove_file(&step.full_path).await
		} {
			Ok(()) => { /*	Everything is awesome! */ }
			Err(e) if e.kind() == io::ErrorKind::NotFound => {
				warn!(
					"File not found in the file system, will remove from database: {}",
					step.full_path.display()
				);
				ctx.library
					.db
					.file_path()
					.delete(file_path::id::equals(step.file_path.id))
					.exec()
					.await?;
			}
			Err(e) => {
				return Err(JobError::from(FileIOError::from((&step.full_path, e))));
			}
		}

		Ok(().into())
	}

	async fn finalize(
		&self,
		ctx: &WorkerContext,
		_data: &Option<Self::Data>,
		_run_metadata: &Self::RunMetadata,
	) -> JobResult {
		let init = self;
		invalidate_query!(ctx.library, "search.paths");

		ctx.library.orphan_remover.invoke().await;

		Ok(Some(json!({ "init": init })))
	}
}



File: ./src/object/fs/sync.rs
-------------------------------------------------



File: ./src/object/fs/encrypt.rs
-------------------------------------------------
// use crate::{
// 	invalidate_query,
// 	job::*,
// 	library::Library,
// 	location::{file_path_helper:: location::id::Type},
// 	util::error::{FileIOError, NonUtf8PathError},
// };

// use sd_crypto::{
// 	crypto::Encryptor,
// 	header::{file::FileHeader, keyslot::Keyslot},
// 	primitives::{LATEST_FILE_HEADER, LATEST_KEYSLOT, LATEST_METADATA, LATEST_PREVIEW_MEDIA},
// 	types::{Algorithm, Key},
// };

// use chrono::FixedOffset;
// use serde::{Deserialize, Serialize};
// use specta::Type;
// use tokio::{
// 	fs::{self, File},
// 	io,
// };
// use tracing::{error, warn};
// use uuid::Uuid;

// use super::{
// 	error::FileSystemJobsError, get_location_path_from_location_id, get_many_files_datas, FileData,
// 	BYTES_EXT,
// };

// pub struct FileEncryptorJob;

// #[derive(Serialize, Deserialize, Type, Hash)]
// pub struct FileEncryptorJobInit {
// 	pub location_id: location::id::Type,
// 	pub file_path_ids: Vec<file_path::id::Type>,
// 	pub key_uuid: Uuid,
// 	pub algorithm: Algorithm,
// 	pub metadata: bool,
// 	pub preview_media: bool,
// }

// #[derive(Serialize, Deserialize)]
// pub struct Metadata {
// 	pub file_path_id: file_path::id::Type,
// 	pub name: String,
// 	pub hidden: bool,
// 	pub favorite: bool,
// 	pub important: bool,
// 	pub note: Option<String>,
// 	pub date_created: chrono::DateTime<FixedOffset>,
// }

// impl JobInitData for FileEncryptorJobInit {
// 	type Job = FileEncryptorJob;
// }

// #[async_trait::async_trait]
// impl StatefulJob for FileEncryptorJob {
// 	type Init = FileEncryptorJobInit;
// 	type Data = ();
// 	type Step = FileData;

// 	const NAME: &'static str = "file_encryptor";

// 	fn new() -> Self {
// 		Self {}
// 	}

// 	async fn init(&self, ctx: WorkerContext, state: &mut JobState<Self>) -> Result<(), JobError> {
// 		let Library { db, .. } = &*ctx.library;

// 		state.steps = get_many_files_datas(
// 			db,
// 			get_location_path_from_location_id(db, state.init.location_id).await?,
// 			&state.init.file_path_ids,
// 		)
// 		.await?
// 		.into();

// 		ctx.progress(vec![JobReportUpdate::TaskCount(state.steps.len())]);

// 		Ok(())
// 	}

// 	async fn execute_step(
// 		&self,
// 		ctx: WorkerContext,
// 		state: &mut JobState<Self>,
// 	) -> Result<(), JobError> {
// 		let step = &state.steps[0];

// 		let Library { key_manager, .. } = &*ctx.library;

// 		if !step.file_path.is_dir {
// 			// handle overwriting checks, and making sure there's enough available space

// 			let user_key = key_manager
// 				.access_keymount(state.init.key_uuid)
// 				.await?
// 				.hashed_key;

// 			let user_key_details = key_manager.access_keystore(state.init.key_uuid).await?;

// 			let output_path = {
// 				let mut path = step.full_path.clone();
// 				let extension = path.extension().map_or_else(
// 					|| Ok("bytes".to_string()),
// 					|extension| {
// 						Ok::<String, JobError>(format!(
// 							"{}{BYTES_EXT}",
// 							extension.to_str().ok_or(FileSystemJobsError::FilePath(
// 								NonUtf8PathError(step.full_path.clone().into_boxed_path()).into()
// 							))?
// 						))
// 					},
// 				)?;

// 				path.set_extension(extension);
// 				path
// 			};

// 			let _guard = ctx
// 				.library
// 				.location_manager()
// 				.temporary_ignore_events_for_path(
// 					state.init.location_id,
// 					ctx.library.clone(),
// 					&output_path,
// 				)
// 				.await
// 				.map_or_else(
// 					|e| {
// 						error!(
// 							"Failed to make location manager ignore the path {}; Error: {e:#?}",
// 							output_path.display()
// 						);
// 						None
// 					},
// 					Some,
// 				);

// 			let mut reader = File::open(&step.full_path)
// 				.await
// 				.map_err(|e| FileIOError::from((&step.full_path, e)))?;
// 			let mut writer = File::create(&output_path)
// 				.await
// 				.map_err(|e| FileIOError::from((output_path, e)))?;

// 			let master_key = Key::generate();

// 			let mut header = FileHeader::new(
// 				LATEST_FILE_HEADER,
// 				state.init.algorithm,
// 				vec![
// 					Keyslot::new(
// 						LATEST_KEYSLOT,
// 						state.init.algorithm,
// 						user_key_details.hashing_algorithm,
// 						user_key_details.content_salt,
// 						user_key,
// 						master_key.clone(),
// 					)
// 					.await?,
// 				],
// 			)?;

// 			if state.init.metadata || state.init.preview_media {
// 				// if any are requested, we can make the query as it'll be used at least once
// 				if let Some(ref object) = step.file_path.object {
// 					if state.init.metadata {
// 						let metadata = Metadata {
// 							file_path_id: step.file_path.id,
// 							name: step.file_path.materialized_path.clone(),
// 							hidden: object.hidden,
// 							favorite: object.favorite,
// 							important: object.important,
// 							note: object.note.clone(),
// 							date_created: object.date_created,
// 						};

// 						header
// 							.add_metadata(
// 								LATEST_METADATA,
// 								state.init.algorithm,
// 								master_key.clone(),
// 								&metadata,
// 							)
// 							.await?;
// 					}

// 					// if state.init.preview_media
// 					// 	&& (object.has_thumbnail
// 					// 		|| object.has_video_preview || object.has_thumbstrip)

// 					// may not be the best - preview media (thumbnail) isn't guaranteed to be webp
// 					let thumbnail_path = ctx
// 						.library
// 						.config()
// 						.data_directory()
// 						.join("thumbnails")
// 						.join(
// 							step.file_path
// 								.cas_id
// 								.as_ref()
// 								.ok_or(JobError::MissingCasId)?,
// 						)
// 						.with_extension("wepb");

// 					match fs::read(&thumbnail_path).await {
// 						Ok(thumbnail_bytes) => {
// 							header
// 								.add_preview_media(
// 									LATEST_PREVIEW_MEDIA,
// 									state.init.algorithm,
// 									master_key.clone(),
// 									&thumbnail_bytes,
// 								)
// 								.await?;
// 						}
// 						Err(e) if e.kind() == io::ErrorKind::NotFound => {
// 							// If the file just doesn't exist, then we don't care
// 						}
// 						Err(e) => {
// 							return Err(FileIOError::from((thumbnail_path, e)).into());
// 						}
// 					}
// 				} else {
// 					// should use container encryption if it's a directory
// 					warn!("skipping metadata/preview media inclusion, no associated object found")
// 				}
// 			}

// 			header.write(&mut writer).await?;

// 			let encryptor = Encryptor::new(master_key, header.nonce, header.algorithm)?;

// 			encryptor
// 				.encrypt_streams(&mut reader, &mut writer, &header.generate_aad())
// 				.await?;
// 		} else {
// 			warn!(
// 				"encryption is skipping {}/{} as it isn't a file",
// 				step.file_path.materialized_path, step.file_path.name
// 			)
// 		}

// 		ctx.progress(vec![JobReportUpdate::CompletedTaskCount(
// 			state.step_number + 1,
// 		)]);

// 		Ok(())
// 	}

// 	async fn finalize(&self, ctx: WorkerContext, state: &mut JobState<Self>) -> JobResult {
// 		invalidate_query!(ctx.library, "search.paths");

// 		// mark job as successful
// 		Ok(Some(serde_json::to_value(&state.init)?))
// 	}
// }



File: ./src/object/fs/mod.rs
-------------------------------------------------
use crate::{
	location::{
		file_path_helper::{file_path_with_object, IsolatedFilePathData},
		LocationError,
	},
	prisma::{file_path, location, PrismaClient},
	util::{
		db::maybe_missing,
		error::{FileIOError, NonUtf8PathError},
	},
};

use std::{
	ffi::OsStr,
	path::{Path, PathBuf},
};

use once_cell::sync::Lazy;
use regex::Regex;
use serde::{Deserialize, Serialize};

pub mod delete;
pub mod erase;

pub mod copy;
pub mod cut;

// pub mod decrypt;
// pub mod encrypt;

pub mod error;

use error::FileSystemJobsError;
use tokio::{fs, io};

static DUPLICATE_PATTERN: Lazy<Regex> =
	Lazy::new(|| Regex::new(r" \(\d+\)").expect("Failed to compile hardcoded regex"));

// pub const BYTES_EXT: &str = ".bytes";

#[derive(Serialize, Deserialize, Debug, Clone, Eq, PartialEq)]
pub enum ObjectType {
	File,
	Directory,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct FileData {
	pub file_path: file_path_with_object::Data,
	pub full_path: PathBuf,
}

pub async fn get_many_files_datas(
	db: &PrismaClient,
	location_path: impl AsRef<Path>,
	file_path_ids: &[file_path::id::Type],
) -> Result<Vec<FileData>, FileSystemJobsError> {
	let location_path = location_path.as_ref();

	db._batch(
		file_path_ids
			.iter()
			.map(|file_path_id| {
				db.file_path()
					.find_unique(file_path::id::equals(*file_path_id))
					.include(file_path_with_object::include())
			})
			// FIXME:(fogodev -> Brendonovich) this collect is a workaround to a weird higher ranker lifetime error on
			// the _batch function, it should be removed once the error is fixed
			.collect::<Vec<_>>(),
	)
	.await?
	.into_iter()
	.zip(file_path_ids.iter())
	.map(|(maybe_file_path, file_path_id)| {
		maybe_file_path
			.ok_or(FileSystemJobsError::FilePathIdNotFound(*file_path_id))
			.and_then(|path_data| {
				Ok(FileData {
					full_path: location_path.join(IsolatedFilePathData::try_from(&path_data)?),
					file_path: path_data,
				})
			})
	})
	.collect::<Result<Vec<_>, _>>()
}

pub async fn get_file_data_from_isolated_file_path(
	db: &PrismaClient,
	location_path: impl AsRef<Path>,
	iso_file_path: &IsolatedFilePathData<'_>,
) -> Result<FileData, FileSystemJobsError> {
	let location_path = location_path.as_ref();
	db.file_path()
		.find_unique(iso_file_path.into())
		.include(file_path_with_object::include())
		.exec()
		.await?
		.ok_or_else(|| {
			FileSystemJobsError::FilePathNotFound(
				location_path.join(iso_file_path).into_boxed_path(),
			)
		})
		.and_then(|path_data| {
			Ok(FileData {
				full_path: location_path.join(IsolatedFilePathData::try_from(&path_data)?),
				file_path: path_data,
			})
		})
}

pub async fn fetch_source_and_target_location_paths(
	db: &PrismaClient,
	source_location_id: location::id::Type,
	target_location_id: location::id::Type,
) -> Result<(PathBuf, PathBuf), FileSystemJobsError> {
	match db
		._batch((
			db.location()
				.find_unique(location::id::equals(source_location_id)),
			db.location()
				.find_unique(location::id::equals(target_location_id)),
		))
		.await?
	{
		(Some(source_location), Some(target_location)) => Ok((
			maybe_missing(source_location.path.map(PathBuf::from), "location.path")?,
			maybe_missing(target_location.path.map(PathBuf::from), "location.path")?,
		)),
		(None, _) => Err(LocationError::IdNotFound(source_location_id))?,
		(_, None) => Err(LocationError::IdNotFound(target_location_id))?,
	}
}

fn construct_target_filename(source_file_data: &FileData) -> Result<String, FileSystemJobsError> {
	// extension wizardry for cloning and such
	// if no suffix has been selected, just use the file name
	// if a suffix is provided and it's a directory, use the directory name + suffix
	// if a suffix is provided and it's a file, use the (file name + suffix).extension

	Ok(
		if *maybe_missing(&source_file_data.file_path.is_dir, "file_path.is_dir")?
			|| source_file_data.file_path.extension.is_none()
			|| source_file_data.file_path.extension == Some(String::new())
		{
			maybe_missing(&source_file_data.file_path.name, "file_path.name")?.clone()
		} else {
			format!(
				"{}.{}",
				maybe_missing(&source_file_data.file_path.name, "file_path.name")?,
				maybe_missing(&source_file_data.file_path.extension, "file_path.extension")?
			)
		},
	)
}

pub fn append_digit_to_filename(
	final_path: &mut PathBuf,
	file_name: &str,
	ext: Option<&str>,
	current_int: u32,
) {
	let new_file_name = if let Some(found) = DUPLICATE_PATTERN.find_iter(file_name).last() {
		&file_name[..found.start()]
	} else {
		file_name
	}
	.to_string();

	if let Some(ext) = ext {
		final_path.push(format!("{} ({current_int}).{}", new_file_name, ext));
	} else {
		final_path.push(format!("{new_file_name} ({current_int})"));
	}
}

pub async fn find_available_filename_for_duplicate(
	target_path: impl AsRef<Path>,
) -> Result<PathBuf, FileSystemJobsError> {
	let target_path = target_path.as_ref();

	let new_file_name = target_path
		.file_stem()
		.ok_or_else(|| {
			FileSystemJobsError::MissingFileStem(target_path.to_path_buf().into_boxed_path())
		})?
		.to_str()
		.ok_or_else(|| NonUtf8PathError(target_path.to_path_buf().into_boxed_path()))?;

	let new_file_full_path_without_suffix =
		target_path.parent().map(Path::to_path_buf).ok_or_else(|| {
			FileSystemJobsError::MissingParentPath(target_path.to_path_buf().into_boxed_path())
		})?;

	for i in 1..u32::MAX {
		let mut new_file_full_path_candidate = new_file_full_path_without_suffix.clone();

		append_digit_to_filename(
			&mut new_file_full_path_candidate,
			new_file_name,
			target_path.extension().and_then(OsStr::to_str),
			i,
		);

		match fs::metadata(&new_file_full_path_candidate).await {
			Ok(_) => {
				// This candidate already exists, so we try the next one
				continue;
			}
			Err(e) if e.kind() == io::ErrorKind::NotFound => {
				return Ok(new_file_full_path_candidate);
			}
			Err(e) => return Err(FileIOError::from((new_file_full_path_candidate, e)).into()),
		}
	}

	Err(FileSystemJobsError::FailedToFindAvailableName(
		target_path.to_path_buf().into_boxed_path(),
	))
}



File: ./src/object/fs/erase.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{
		CurrentStep, JobError, JobInitOutput, JobResult, JobRunMetadata, JobStepOutput,
		StatefulJob, WorkerContext,
	},
	library::Library,
	location::{file_path_helper::IsolatedFilePathData, get_location_path_from_location_id},
	prisma::{file_path, location},
	util::{db::maybe_missing, error::FileIOError},
};

use std::{hash::Hash, path::PathBuf};

use futures::future::try_join_all;
use serde::{Deserialize, Serialize};
use serde_with::{serde_as, DisplayFromStr};
use specta::Type;
use tokio::{
	fs::{self, OpenOptions},
	io::AsyncWriteExt,
};
use tracing::trace;

use super::{
	error::FileSystemJobsError, get_file_data_from_isolated_file_path, get_many_files_datas,
	FileData,
};

#[serde_as]
#[derive(Serialize, Deserialize, Hash, Type, Debug)]
pub struct FileEraserJobInit {
	pub location_id: location::id::Type,
	pub file_path_ids: Vec<file_path::id::Type>,
	#[specta(type = String)]
	#[serde_as(as = "DisplayFromStr")]
	pub passes: usize,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct FileEraserJobData {
	location_path: PathBuf,
}

#[derive(Serialize, Deserialize, Default, Debug)]
pub struct FileEraserJobRunMetadata {
	diretories_to_remove: Vec<PathBuf>,
}

impl JobRunMetadata for FileEraserJobRunMetadata {
	fn update(&mut self, new_data: Self) {
		self.diretories_to_remove
			.extend(new_data.diretories_to_remove);
	}
}

#[async_trait::async_trait]
impl StatefulJob for FileEraserJobInit {
	type Data = FileEraserJobData;
	type Step = FileData;
	type RunMetadata = FileEraserJobRunMetadata;

	const NAME: &'static str = "file_eraser";

	fn target_location(&self) -> location::id::Type {
		self.location_id
	}

	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let init = self;
		let Library { db, .. } = &*ctx.library;

		let location_path = get_location_path_from_location_id(db, init.location_id)
			.await
			.map_err(FileSystemJobsError::from)?;

		let steps = get_many_files_datas(db, &location_path, &init.file_path_ids).await?;

		*data = Some(FileEraserJobData { location_path });

		Ok((Default::default(), steps).into())
	}

	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		CurrentStep { step, .. }: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		_: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		let init = self;

		// need to handle stuff such as querying prisma for all paths of a file, and deleting all of those if requested (with a checkbox in the ui)
		// maybe a files.countOccurances/and or files.getPath(location_id, path_id) to show how many of these files would be erased (and where?)

		let mut new_metadata = Self::RunMetadata::default();

		if maybe_missing(step.file_path.is_dir, "file_path.is_dir")? {
			let mut more_steps = Vec::new();

			let mut dir = tokio::fs::read_dir(&step.full_path)
				.await
				.map_err(|e| FileIOError::from((&step.full_path, e)))?;

			while let Some(children_entry) = dir
				.next_entry()
				.await
				.map_err(|e| FileIOError::from((&step.full_path, e)))?
			{
				let children_path = children_entry.path();

				more_steps.push(
					get_file_data_from_isolated_file_path(
						&ctx.library.db,
						&data.location_path,
						&IsolatedFilePathData::new(
							init.location_id,
							&data.location_path,
							&children_path,
							children_entry
								.metadata()
								.await
								.map_err(|e| FileIOError::from((&children_path, e)))?
								.is_dir(),
						)
						.map_err(FileSystemJobsError::from)?,
					)
					.await?,
				);
			}
			new_metadata
				.diretories_to_remove
				.push(step.full_path.clone());

			Ok((more_steps, new_metadata).into())
		} else {
			{
				let mut file = OpenOptions::new()
					.read(true)
					.write(true)
					.open(&step.full_path)
					.await
					.map_err(|e| FileIOError::from((&step.full_path, e)))?;
				let file_len = file
					.metadata()
					.await
					.map_err(|e| FileIOError::from((&step.full_path, e)))?
					.len();

				trace!(
					"Overwriting file: {} with {} passes",
					step.full_path.display(),
					init.passes
				);

				sd_crypto::fs::erase::erase(&mut file, file_len as usize, init.passes).await?;

				file.set_len(0)
					.await
					.map_err(|e| FileIOError::from((&step.full_path, e)))?;
				file.flush()
					.await
					.map_err(|e| FileIOError::from((&step.full_path, e)))?;
			}

			fs::remove_file(&step.full_path)
				.await
				.map_err(|e| FileIOError::from((&step.full_path, e)))?;

			Ok(None.into())
		}
	}

	async fn finalize(
		&self,
		ctx: &WorkerContext,
		_data: &Option<Self::Data>,
		run_metadata: &Self::RunMetadata,
	) -> JobResult {
		let init = self;
		try_join_all(
			run_metadata
				.diretories_to_remove
				.iter()
				.cloned()
				.map(|data| async {
					fs::remove_dir_all(&data)
						.await
						.map_err(|e| FileIOError::from((data, e)))
				}),
		)
		.await?;

		invalidate_query!(ctx.library, "search.paths");

		Ok(Some(serde_json::to_value(init)?))
	}
}



File: ./src/object/fs/archive.rs
-------------------------------------------------



File: ./src/object/tag/mod.rs
-------------------------------------------------
pub mod seed;

use chrono::{DateTime, FixedOffset, Utc};
use sd_prisma::prisma_sync;
use sd_sync::*;
use serde::Deserialize;
use serde_json::json;
use specta::Type;

use uuid::Uuid;

use crate::{library::Library, prisma::tag};

#[derive(Type, Deserialize, Clone)]
pub struct TagCreateArgs {
	pub name: String,
	pub color: String,
}

impl TagCreateArgs {
	pub async fn exec(
		self,
		Library { db, sync, .. }: &Library,
	) -> prisma_client_rust::Result<tag::Data> {
		let pub_id = Uuid::new_v4().as_bytes().to_vec();
		let date_created: DateTime<FixedOffset> = Utc::now().into();

		sync.write_ops(
			db,
			(
				sync.shared_create(
					prisma_sync::tag::SyncId {
						pub_id: pub_id.clone(),
					},
					[
						(tag::name::NAME, json!(&self.name)),
						(tag::color::NAME, json!(&self.color)),
						(tag::date_created::NAME, json!(&date_created.to_rfc3339())),
					],
				),
				db.tag().create(
					pub_id,
					vec![
						tag::name::set(Some(self.name)),
						tag::color::set(Some(self.color)),
						tag::date_created::set(Some(date_created)),
					],
				),
			),
		)
		.await
	}
}



File: ./src/object/tag/seed.rs
-------------------------------------------------
use super::TagCreateArgs;
use crate::library::Library;

/// Seeds tags in a new library.
/// Shouldn't be called more than once!
pub async fn new_library(library: &Library) -> prisma_client_rust::Result<()> {
	// remove type after tags are added

	let tags = [
		TagCreateArgs {
			name: "Keepsafe".to_string(),
			color: "#D9188E".to_string(),
		},
		TagCreateArgs {
			name: "Hidden".to_string(),
			color: "#646278".to_string(),
		},
		TagCreateArgs {
			name: "Projects".to_string(),
			color: "#42D097".to_string(),
		},
		TagCreateArgs {
			name: "Memes".to_string(),
			color: "#A718D9".to_string(),
		},
	];

	for tag in tags {
		tag.exec(library).await?;
	}

	Ok(())
}



File: ./src/object/validation/validator_job.rs
-------------------------------------------------
use crate::{
	job::{
		CurrentStep, JobError, JobInitOutput, JobResult, JobStepOutput, StatefulJob, WorkerContext,
	},
	library::Library,
	location::file_path_helper::{
		ensure_file_path_exists, ensure_sub_path_is_directory, ensure_sub_path_is_in_location,
		file_path_for_object_validator, IsolatedFilePathData,
	},
	prisma::{file_path, location},
	util::{db::maybe_missing, error::FileIOError},
};

use std::{
	hash::{Hash, Hasher},
	path::{Path, PathBuf},
};

use sd_prisma::prisma_sync;
use sd_sync::OperationFactory;
use serde::{Deserialize, Serialize};
use serde_json::json;
use tracing::info;

use super::{hash::file_checksum, ValidatorError};

#[derive(Serialize, Deserialize, Debug)]
pub struct ObjectValidatorJobData {
	pub location_path: PathBuf,
	pub task_count: usize,
}

// The validator can
#[derive(Serialize, Deserialize, Debug)]
pub struct ObjectValidatorJobInit {
	pub location: location::Data,
	pub sub_path: Option<PathBuf>,
}

impl Hash for ObjectValidatorJobInit {
	fn hash<H: Hasher>(&self, state: &mut H) {
		self.location.id.hash(state);
		if let Some(ref sub_path) = self.sub_path {
			sub_path.hash(state);
		}
	}
}

// The Validator is able to:
// - generate a full byte checksum for Objects in a Location
// - generate checksums for all Objects missing without one
// - compare two objects and return true if they are the same
#[async_trait::async_trait]
impl StatefulJob for ObjectValidatorJobInit {
	type Data = ObjectValidatorJobData;
	type Step = file_path_for_object_validator::Data;
	type RunMetadata = ();

	const NAME: &'static str = "object_validator";

	fn target_location(&self) -> location::id::Type {
		self.location.id
	}

	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let init = self;
		let Library { db, .. } = &*ctx.library;

		let location_id = init.location.id;

		let location_path =
			maybe_missing(&init.location.path, "location.path").map(PathBuf::from)?;

		let maybe_sub_iso_file_path = match &init.sub_path {
			Some(sub_path) if sub_path != Path::new("") => {
				let full_path = ensure_sub_path_is_in_location(&location_path, sub_path)
					.await
					.map_err(ValidatorError::from)?;
				ensure_sub_path_is_directory(&location_path, sub_path)
					.await
					.map_err(ValidatorError::from)?;

				let sub_iso_file_path =
					IsolatedFilePathData::new(location_id, &location_path, &full_path, true)
						.map_err(ValidatorError::from)?;

				ensure_file_path_exists(
					sub_path,
					&sub_iso_file_path,
					db,
					ValidatorError::SubPathNotFound,
				)
				.await?;

				Some(sub_iso_file_path)
			}
			_ => None,
		};

		let steps = db
			.file_path()
			.find_many(sd_utils::chain_optional_iter(
				[
					file_path::location_id::equals(Some(init.location.id)),
					file_path::is_dir::equals(Some(false)),
					file_path::integrity_checksum::equals(None),
				],
				[maybe_sub_iso_file_path.and_then(|iso_sub_path| {
					iso_sub_path
						.materialized_path_for_children()
						.map(file_path::materialized_path::starts_with)
				})],
			))
			.select(file_path_for_object_validator::select())
			.exec()
			.await?;

		*data = Some(ObjectValidatorJobData {
			location_path,
			task_count: steps.len(),
		});

		Ok(steps.into())
	}

	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		CurrentStep {
			step: file_path, ..
		}: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		_: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		let init = self;
		let Library { db, sync, .. } = &*ctx.library;

		// this is to skip files that already have checksums
		// i'm unsure what the desired behaviour is in this case
		// we can also compare old and new checksums here
		// This if is just to make sure, we already queried objects where integrity_checksum is null
		if file_path.integrity_checksum.is_none() {
			let full_path = data.location_path.join(IsolatedFilePathData::try_from((
				init.location.id,
				file_path,
			))?);
			let checksum = file_checksum(&full_path)
				.await
				.map_err(|e| ValidatorError::FileIO(FileIOError::from((full_path, e))))?;

			sync.write_op(
				db,
				sync.shared_update(
					prisma_sync::file_path::SyncId {
						pub_id: file_path.pub_id.clone(),
					},
					file_path::integrity_checksum::NAME,
					json!(&checksum),
				),
				db.file_path().update(
					file_path::pub_id::equals(file_path.pub_id.clone()),
					vec![file_path::integrity_checksum::set(Some(checksum))],
				),
			)
			.await?;
		}

		Ok(().into())
	}

	async fn finalize(
		&self,
		_: &WorkerContext,
		data: &Option<Self::Data>,
		_run_metadata: &Self::RunMetadata,
	) -> JobResult {
		let init = self;
		let data = data
			.as_ref()
			.expect("critical error: missing data on job state");

		info!(
			"finalizing validator job at {}{}: {} tasks",
			data.location_path.display(),
			init.sub_path
				.as_ref()
				.map(|p| format!("{}", p.display()))
				.unwrap_or_default(),
			data.task_count
		);

		Ok(Some(json!({ "init": init })))
	}
}



File: ./src/object/validation/mod.rs
-------------------------------------------------
use crate::{location::file_path_helper::FilePathError, util::error::FileIOError};

use std::path::Path;

use thiserror::Error;

pub mod hash;
pub mod validator_job;

#[derive(Error, Debug)]
pub enum ValidatorError {
	#[error("sub path not found: <path='{}'>", .0.display())]
	SubPathNotFound(Box<Path>),

	// Internal errors
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error(transparent)]
	FilePath(#[from] FilePathError),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
}



File: ./src/object/validation/hash.rs
-------------------------------------------------
use blake3::Hasher;
use std::path::Path;
use tokio::{
	fs::File,
	io::{self, AsyncReadExt},
};

const BLOCK_LEN: usize = 1048576;

pub async fn file_checksum(path: impl AsRef<Path>) -> Result<String, io::Error> {
	let mut reader = File::open(path).await?;
	let mut context = Hasher::new();
	let mut buffer = vec![0; BLOCK_LEN].into_boxed_slice();
	loop {
		let read_count = reader.read(&mut buffer).await?;
		context.update(&buffer[..read_count]);
		if read_count != BLOCK_LEN {
			break;
		}
	}
	let hex = context.finalize().to_hex();

	Ok(hex.to_string())
}



File: ./src/object/media/thumbnail/preferences.rs
-------------------------------------------------
use serde::{Deserialize, Serialize};
use specta::Type;

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq, Eq, Type)]
pub struct ThumbnailerPreferences {
	background_processing_percentage: u8, // 0-100
}

impl Default for ThumbnailerPreferences {
	fn default() -> Self {
		Self {
			background_processing_percentage: 50, // 50% of CPU cores available
		}
	}
}

impl ThumbnailerPreferences {
	pub fn background_processing_percentage(&self) -> u8 {
		self.background_processing_percentage
	}

	pub fn set_background_processing_percentage(
		&mut self,
		mut background_processing_percentage: u8,
	) -> &mut Self {
		if background_processing_percentage > 100 {
			background_processing_percentage = 100;
		}

		self.background_processing_percentage = background_processing_percentage;

		self
	}
}



File: ./src/object/media/thumbnail/clean_up.rs
-------------------------------------------------
use crate::{library::LibraryId, util::error::FileIOError};

use sd_prisma::prisma::{file_path, PrismaClient};

use std::{collections::HashSet, ffi::OsString, path::PathBuf, sync::Arc};

use futures_concurrency::future::Join;
use tokio::{fs, spawn};
use tracing::{debug, error};

use super::{ThumbnailerError, EPHEMERAL_DIR, WEBP_EXTENSION};

pub(super) async fn process_ephemeral_clean_up(
	thumbnails_directory: Arc<PathBuf>,
	existing_ephemeral_thumbs: HashSet<OsString>,
) {
	let ephemeral_thumbs_dir = thumbnails_directory.join(EPHEMERAL_DIR);

	spawn(async move {
		let mut to_remove = vec![];

		let mut read_ephemeral_thumbs_dir = fs::read_dir(&ephemeral_thumbs_dir)
			.await
			.map_err(|e| FileIOError::from((&ephemeral_thumbs_dir, e)))?;

		while let Some(shard_entry) = read_ephemeral_thumbs_dir
			.next_entry()
			.await
			.map_err(|e| FileIOError::from((&ephemeral_thumbs_dir, e)))?
		{
			let shard_path = shard_entry.path();
			if shard_entry
				.file_type()
				.await
				.map_err(|e| FileIOError::from((&shard_path, e)))?
				.is_dir()
			{
				let mut read_shard_dir = fs::read_dir(&shard_path)
					.await
					.map_err(|e| FileIOError::from((&shard_path, e)))?;

				while let Some(thumb_entry) = read_shard_dir
					.next_entry()
					.await
					.map_err(|e| FileIOError::from((&shard_path, e)))?
				{
					let thumb_path = thumb_entry.path();
					if thumb_path.extension() == Some(WEBP_EXTENSION.as_ref())
						&& !existing_ephemeral_thumbs.contains(&thumb_entry.file_name())
					{
						to_remove.push(async move {
							debug!(
								"Removing stale ephemeral thumbnail: {}",
								thumb_path.display()
							);
							fs::remove_file(&thumb_path).await.map_err(|e| {
								ThumbnailerError::FileIO(FileIOError::from((thumb_path, e)))
							})
						});
					}
				}
			}
		}

		Ok::<_, ThumbnailerError>(to_remove.join().await)
	})
	.await
	.map_or_else(
		|e| error!("Join error on ephemeral clean up: {e:#?}",),
		|fetching_res| {
			fetching_res.map_or_else(
				|e| error!("Error fetching ephemeral thumbs to be removed: {e:#?}"),
				|remove_results| {
					remove_results.into_iter().for_each(|remove_res| {
						if let Err(e) = remove_res {
							error!("Error on ephemeral clean up: {e:#?}");
						}
					})
				},
			)
		},
	)
}

pub(super) async fn process_indexed_clean_up(
	thumbnails_directory: Arc<PathBuf>,
	libraries_ids_and_databases: Vec<(LibraryId, Arc<PrismaClient>)>,
) {
	libraries_ids_and_databases
		.into_iter()
		.map(|(library_id, db)| {
			let library_thumbs_dir = thumbnails_directory.join(library_id.to_string());
			spawn(async move {
				let existing_thumbs = db
					.file_path()
					.find_many(vec![file_path::cas_id::not(None)])
					.select(file_path::select!({ cas_id }))
					.exec()
					.await?
					.into_iter()
					.map(|file_path| {
						OsString::from(format!(
							"{}.webp",
							file_path.cas_id.expect("we filtered right")
						))
					})
					.collect::<HashSet<_>>();

				let mut read_library_thumbs_dir = fs::read_dir(&library_thumbs_dir)
					.await
					.map_err(|e| FileIOError::from((&library_thumbs_dir, e)))?;

				let mut to_remove = vec![];

				while let Some(shard_entry) = read_library_thumbs_dir
					.next_entry()
					.await
					.map_err(|e| FileIOError::from((&library_thumbs_dir, e)))?
				{
					let shard_path = shard_entry.path();
					if shard_entry
						.file_type()
						.await
						.map_err(|e| FileIOError::from((&shard_path, e)))?
						.is_dir()
					{
						let mut read_shard_dir = fs::read_dir(&shard_path)
							.await
							.map_err(|e| FileIOError::from((&shard_path, e)))?;

						while let Some(thumb_entry) = read_shard_dir
							.next_entry()
							.await
							.map_err(|e| FileIOError::from((&shard_path, e)))?
						{
							let thumb_path = thumb_entry.path();
							if thumb_path.extension() == Some(WEBP_EXTENSION.as_ref())
								&& !existing_thumbs.contains(&thumb_entry.file_name())
							{
								to_remove.push(async move {
									debug!(
										"Removing stale indexed thumbnail: {}",
										thumb_path.display()
									);
									fs::remove_file(&thumb_path).await.map_err(|e| {
										ThumbnailerError::FileIO(FileIOError::from((thumb_path, e)))
									})
								});
							}
						}
					}
				}

				Ok::<_, ThumbnailerError>(to_remove.join().await)
			})
		})
		.collect::<Vec<_>>()
		.join()
		.await
		.into_iter()
		.filter_map(|join_res| {
			join_res
				.map_err(|e| error!("Join error on indexed clean up: {e:#?}"))
				.ok()
		})
		.filter_map(|fetching_res| {
			fetching_res
				.map_err(|e| error!("Error fetching indexed thumbs to be removed: {e:#?}"))
				.ok()
		})
		.for_each(|remove_results| {
			remove_results.into_iter().for_each(|remove_res| {
				if let Err(e) = remove_res {
					error!("Error on indexed clean up: {e:#?}");
				}
			})
		})
}



File: ./src/object/media/thumbnail/worker.rs
-------------------------------------------------
use crate::{api::CoreEvent, node::config::NodePreferences};

use std::{collections::HashMap, ffi::OsString, path::PathBuf, pin::pin, sync::Arc};

use sd_prisma::prisma::location;

use async_channel as chan;
use futures_concurrency::stream::Merge;
use tokio::{
	spawn,
	sync::{broadcast, oneshot, watch},
	time::{interval, interval_at, timeout, Instant, MissedTickBehavior},
};
use tokio_stream::{
	wrappers::{IntervalStream, WatchStream},
	StreamExt,
};
use tracing::{debug, error, trace};

use super::{
	actor::DatabaseMessage,
	clean_up::{process_ephemeral_clean_up, process_indexed_clean_up},
	preferences::ThumbnailerPreferences,
	process::{batch_processor, ProcessorControlChannels},
	state::{remove_by_cas_ids, RegisterReporter, ThumbsProcessingSaveState},
	BatchToProcess, ThumbnailKind, HALF_HOUR, ONE_SEC, THIRTY_SECS,
};

#[derive(Debug, Clone)]
pub(super) struct WorkerChannels {
	pub(super) progress_management_rx: chan::Receiver<RegisterReporter>,
	pub(super) databases_rx: chan::Receiver<DatabaseMessage>,
	pub(super) cas_ids_to_delete_rx: chan::Receiver<(Vec<String>, ThumbnailKind)>,
	pub(super) thumbnails_to_generate_rx: chan::Receiver<(BatchToProcess, ThumbnailKind)>,
	pub(super) cancel_rx: chan::Receiver<oneshot::Sender<()>>,
}

pub(super) async fn worker(
	available_parallelism: usize,
	node_preferences_rx: watch::Receiver<NodePreferences>,
	reporter: broadcast::Sender<CoreEvent>,
	thumbnails_directory: Arc<PathBuf>,
	WorkerChannels {
		progress_management_rx,
		databases_rx,
		cas_ids_to_delete_rx,
		thumbnails_to_generate_rx,
		cancel_rx,
	}: WorkerChannels,
) {
	let mut to_remove_interval = interval_at(Instant::now() + THIRTY_SECS, HALF_HOUR);
	to_remove_interval.set_missed_tick_behavior(MissedTickBehavior::Skip);

	let mut idle_interval = interval(ONE_SEC);
	idle_interval.set_missed_tick_behavior(MissedTickBehavior::Skip);

	let mut databases = HashMap::new();

	#[derive(Debug)]
	enum StreamMessage {
		RemovalTick,
		ToDelete((Vec<String>, ThumbnailKind)),
		Database(DatabaseMessage),
		NewBatch((BatchToProcess, ThumbnailKind)),
		Leftovers((BatchToProcess, ThumbnailKind)),
		NewEphemeralThumbnailsFilenames(Vec<OsString>),
		ProgressManagement(RegisterReporter),
		BatchProgress((location::id::Type, u32)),
		Shutdown(oneshot::Sender<()>),
		UpdatedPreferences(ThumbnailerPreferences),
		IdleTick,
	}

	let ThumbsProcessingSaveState {
		mut bookkeeper,
		mut ephemeral_file_names,
		mut queue,
		mut indexed_leftovers_queue,
		mut ephemeral_leftovers_queue,
	} = ThumbsProcessingSaveState::load(thumbnails_directory.as_ref()).await;

	let (generated_ephemeral_thumbnails_tx, ephemeral_thumbnails_cas_ids_rx) = chan::bounded(32);
	let (leftovers_tx, leftovers_rx) = chan::bounded(8);
	let (batch_report_progress_tx, batch_report_progress_rx) = chan::bounded(8);
	let (stop_older_processing_tx, stop_older_processing_rx) = chan::bounded(1);

	let mut shutdown_leftovers_rx = pin!(leftovers_rx.clone());
	let mut shutdowm_batch_report_progress_rx = pin!(batch_report_progress_rx.clone());

	let mut current_batch_processing_rx: Option<oneshot::Receiver<()>> = None;

	let mut msg_stream = pin!((
		IntervalStream::new(to_remove_interval).map(|_| StreamMessage::RemovalTick),
		cas_ids_to_delete_rx.map(StreamMessage::ToDelete),
		databases_rx.map(StreamMessage::Database),
		thumbnails_to_generate_rx.map(StreamMessage::NewBatch),
		leftovers_rx.map(StreamMessage::Leftovers),
		ephemeral_thumbnails_cas_ids_rx.map(StreamMessage::NewEphemeralThumbnailsFilenames),
		progress_management_rx.map(StreamMessage::ProgressManagement),
		batch_report_progress_rx.map(StreamMessage::BatchProgress),
		cancel_rx.map(StreamMessage::Shutdown),
		IntervalStream::new(idle_interval).map(|_| StreamMessage::IdleTick),
		WatchStream::new(node_preferences_rx).map(|node_preferences| {
			StreamMessage::UpdatedPreferences(node_preferences.thumbnailer)
		}),
	)
		.merge());

	let mut thumbnailer_preferences = ThumbnailerPreferences::default();

	while let Some(msg) = msg_stream.next().await {
		match msg {
			StreamMessage::IdleTick => {
				if let Some(done_rx) = current_batch_processing_rx.as_mut() {
					// Checking if the previous run finished or was aborted to clean state
					match done_rx.try_recv() {
						Ok(()) | Err(oneshot::error::TryRecvError::Closed) => {
							current_batch_processing_rx = None;
						}

						Err(oneshot::error::TryRecvError::Empty) => {
							// The previous run is still running
							continue;
						}
					}
				}

				if current_batch_processing_rx.is_none()
					&& (!queue.is_empty()
						|| !indexed_leftovers_queue.is_empty()
						|| !ephemeral_leftovers_queue.is_empty())
				{
					let (done_tx, done_rx) = oneshot::channel();
					current_batch_processing_rx = Some(done_rx);

					let batch_and_kind = if let Some(batch_and_kind) = queue.pop_front() {
						batch_and_kind
					} else if let Some((batch, library_id)) = indexed_leftovers_queue.pop_front() {
						// indexed leftovers have bigger priority
						(batch, ThumbnailKind::Indexed(library_id))
					} else if let Some(batch) = ephemeral_leftovers_queue.pop_front() {
						(batch, ThumbnailKind::Ephemeral)
					} else {
						continue;
					};

					spawn(batch_processor(
						thumbnails_directory.clone(),
						batch_and_kind,
						generated_ephemeral_thumbnails_tx.clone(),
						ProcessorControlChannels {
							stop_rx: stop_older_processing_rx.clone(),
							done_tx,
							batch_report_progress_tx: batch_report_progress_tx.clone(),
						},
						leftovers_tx.clone(),
						reporter.clone(),
						(available_parallelism, thumbnailer_preferences.clone()),
					));
				}
			}

			StreamMessage::RemovalTick => {
				// For any of them we process a clean up if a time since the last one already passed
				if !databases.is_empty() {
					spawn(process_indexed_clean_up(
						thumbnails_directory.clone(),
						databases
							.iter()
							.map(|(id, db)| (*id, Arc::clone(db)))
							.collect::<Vec<_>>(),
					));
				}

				if !ephemeral_file_names.is_empty() {
					spawn(process_ephemeral_clean_up(
						thumbnails_directory.clone(),
						ephemeral_file_names.clone(),
					));
				}
			}

			StreamMessage::ToDelete((cas_ids, kind)) => {
				if !cas_ids.is_empty() {
					if let Err(e) = remove_by_cas_ids(&thumbnails_directory, cas_ids, kind).await {
						error!("Got an error when trying to remove thumbnails: {e:#?}");
					}
				}
			}

			StreamMessage::NewBatch((batch, kind)) => {
				let in_background = batch.in_background;

				if let Some(location_id) = batch.location_id {
					bookkeeper
						.add_work(location_id, batch.batch.len() as u32)
						.await;
				}

				trace!(
					"New {kind:?} batch to process in {}, size: {}",
					if in_background {
						"background"
					} else {
						"foreground"
					},
					batch.batch.len()
				);

				if in_background {
					queue.push_back((batch, kind));
				} else {
					// If a processing must be in foreground, then it takes maximum priority
					queue.push_front((batch, kind));
				}

				// Only sends stop signal if there is a batch being processed
				if !in_background {
					stop_batch(
						&current_batch_processing_rx,
						&stop_older_processing_tx,
						&stop_older_processing_rx,
					)
					.await;
				}
			}

			StreamMessage::Leftovers((batch, ThumbnailKind::Indexed(library_id))) => {
				indexed_leftovers_queue.push_back((batch, library_id))
			}

			StreamMessage::Leftovers((batch, ThumbnailKind::Ephemeral)) => {
				ephemeral_leftovers_queue.push_back(batch)
			}

			StreamMessage::Database(DatabaseMessage::Add(id, db))
			| StreamMessage::Database(DatabaseMessage::Update(id, db)) => {
				databases.insert(id, db);
			}

			StreamMessage::Database(DatabaseMessage::Remove(id)) => {
				databases.remove(&id);
			}

			StreamMessage::NewEphemeralThumbnailsFilenames(new_ephemeral_thumbs) => {
				trace!("New ephemeral thumbnails: {}", new_ephemeral_thumbs.len());
				ephemeral_file_names.extend(new_ephemeral_thumbs);
			}

			StreamMessage::BatchProgress((location_id, progressed)) => {
				bookkeeper.add_progress(location_id, progressed).await;
			}

			StreamMessage::Shutdown(cancel_tx) => {
				debug!("Thumbnail actor is shutting down...");
				let start = Instant::now();

				stop_batch(
					&current_batch_processing_rx,
					&stop_older_processing_tx,
					&stop_older_processing_rx,
				)
				.await;

				// Closing the leftovers channel to stop the batch processor as we already sent
				// an stop signal
				leftovers_tx.close();
				while let Some((batch, kind)) = shutdown_leftovers_rx.next().await {
					match kind {
						ThumbnailKind::Indexed(library_id) => {
							indexed_leftovers_queue.push_back((batch, library_id))
						}
						ThumbnailKind::Ephemeral => ephemeral_leftovers_queue.push_back(batch),
					}
				}

				// Consuming the last progress reports to keep everything up to date
				shutdowm_batch_report_progress_rx.close();
				while let Some((location_id, progressed)) =
					shutdowm_batch_report_progress_rx.next().await
				{
					bookkeeper.add_progress(location_id, progressed).await;
				}

				// Saving state
				ThumbsProcessingSaveState {
					bookkeeper,
					ephemeral_file_names,
					queue,
					indexed_leftovers_queue,
					ephemeral_leftovers_queue,
				}
				.store(thumbnails_directory.as_ref())
				.await;

				// Signaling that we're done shutting down
				cancel_tx.send(()).ok();

				debug!("Thumbnailer has been shutdown in {:?}", start.elapsed());
				return;
			}

			StreamMessage::ProgressManagement((location_id, progress_tx)) => {
				bookkeeper.register_reporter(location_id, progress_tx);
			}

			StreamMessage::UpdatedPreferences(preferences) => {
				thumbnailer_preferences = preferences;
				stop_batch(
					&current_batch_processing_rx,
					&stop_older_processing_tx,
					&stop_older_processing_rx,
				)
				.await;
			}
		}
	}
}

#[inline]
async fn stop_batch(
	current_batch_processing_rx: &Option<oneshot::Receiver<()>>,
	stop_older_processing_tx: &chan::Sender<oneshot::Sender<()>>,
	stop_older_processing_rx: &chan::Receiver<oneshot::Sender<()>>,
) {
	// First stopping the current batch processing
	if current_batch_processing_rx.is_some() {
		trace!("Sending stop signal to older processing");

		let (tx, rx) = oneshot::channel();

		match stop_older_processing_tx.try_send(tx) {
			Ok(()) => {
				// We put a timeout here to avoid a deadlock in case the older processing already
				// finished its batch
				if timeout(ONE_SEC, rx).await.is_err() {
					stop_older_processing_rx.recv().await.ok();
				}
			}
			Err(e) if e.is_full() => {
				// The last signal we sent happened after a batch was already processed
				// So we clean the channel and we're good to go.
				stop_older_processing_rx.recv().await.ok();
			}
			Err(_) => {
				error!("Thumbnail actor died when trying to stop older processing");
			}
		}
	}
}



File: ./src/object/media/thumbnail/mod.rs
-------------------------------------------------
use crate::{
	library::LibraryId,
	util::{error::FileIOError, version_manager::VersionManagerError},
	Node,
};

use sd_file_ext::extensions::{
	DocumentExtension, Extension, ImageExtension, ALL_DOCUMENT_EXTENSIONS, ALL_IMAGE_EXTENSIONS,
};

#[cfg(feature = "ffmpeg")]
use sd_file_ext::extensions::{VideoExtension, ALL_VIDEO_EXTENSIONS};

use std::{
	path::{Path, PathBuf},
	time::Duration,
};

use once_cell::sync::Lazy;
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::task;
use tracing::error;

pub mod actor;
mod clean_up;
mod directory;
pub mod preferences;
mod process;
mod shard;
mod state;
mod worker;

pub use process::{BatchToProcess, GenerateThumbnailArgs};
pub use shard::get_shard_hex;

use directory::ThumbnailVersion;

// Files names constants
const THUMBNAIL_CACHE_DIR_NAME: &str = "thumbnails";
const SAVE_STATE_FILE: &str = "thumbs_to_process.bin";
const VERSION_FILE: &str = "version.txt";
pub const WEBP_EXTENSION: &str = "webp";
const EPHEMERAL_DIR: &str = "ephemeral";

/// This is the target pixel count for all thumbnails to be resized to, and it is eventually downscaled
/// to [`TARGET_QUALITY`].
const TARGET_PX: f32 = 262144_f32;

/// This is the target quality that we render thumbnails at, it is a float between 0-100
/// and is treated as a percentage (so 30% in this case, or it's the same as multiplying by `0.3`).
const TARGET_QUALITY: f32 = 30_f32;

// Some time constants
const ONE_SEC: Duration = Duration::from_secs(1);
const THIRTY_SECS: Duration = Duration::from_secs(30);
const HALF_HOUR: Duration = Duration::from_secs(30 * 60);

#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
pub enum ThumbnailKind {
	Ephemeral,
	Indexed(LibraryId),
}

pub fn get_indexed_thumbnail_path(node: &Node, cas_id: &str, library_id: LibraryId) -> PathBuf {
	get_thumbnail_path(node, cas_id, ThumbnailKind::Indexed(library_id))
}

/// This does not check if a thumbnail exists, it just returns the path that it would exist at
fn get_thumbnail_path(node: &Node, cas_id: &str, kind: ThumbnailKind) -> PathBuf {
	let mut thumb_path = node.config.data_directory();

	thumb_path.push(THUMBNAIL_CACHE_DIR_NAME);
	match kind {
		ThumbnailKind::Ephemeral => thumb_path.push(EPHEMERAL_DIR),
		ThumbnailKind::Indexed(library_id) => {
			thumb_path.push(library_id.to_string());
		}
	}
	thumb_path.push(get_shard_hex(cas_id));
	thumb_path.push(cas_id);
	thumb_path.set_extension(WEBP_EXTENSION);

	thumb_path
}

pub fn get_indexed_thumb_key(cas_id: &str, library_id: LibraryId) -> Vec<String> {
	get_thumb_key(cas_id, ThumbnailKind::Indexed(library_id))
}

pub fn get_ephemeral_thumb_key(cas_id: &str) -> Vec<String> {
	get_thumb_key(cas_id, ThumbnailKind::Ephemeral)
}

// this is used to pass the relevant data to the frontend so it can request the thumbnail
// it supports extending the shard hex to support deeper directory structures in the future
fn get_thumb_key(cas_id: &str, kind: ThumbnailKind) -> Vec<String> {
	vec![
		match kind {
			ThumbnailKind::Ephemeral => String::from(EPHEMERAL_DIR),
			ThumbnailKind::Indexed(library_id) => library_id.to_string(),
		},
		get_shard_hex(cas_id).to_string(),
		cas_id.to_string(),
	]
}

#[cfg(feature = "ffmpeg")]
pub(super) static THUMBNAILABLE_VIDEO_EXTENSIONS: Lazy<Vec<Extension>> = Lazy::new(|| {
	ALL_VIDEO_EXTENSIONS
		.iter()
		.cloned()
		.filter(can_generate_thumbnail_for_video)
		.map(Extension::Video)
		.collect()
});

pub(super) static THUMBNAILABLE_EXTENSIONS: Lazy<Vec<Extension>> = Lazy::new(|| {
	ALL_IMAGE_EXTENSIONS
		.iter()
		.cloned()
		.filter(can_generate_thumbnail_for_image)
		.map(Extension::Image)
		.chain(
			ALL_DOCUMENT_EXTENSIONS
				.iter()
				.cloned()
				.filter(can_generate_thumbnail_for_document)
				.map(Extension::Document),
		)
		.collect()
});

pub(super) static ALL_THUMBNAILABLE_EXTENSIONS: Lazy<Vec<Extension>> = Lazy::new(|| {
	#[cfg(feature = "ffmpeg")]
	return THUMBNAILABLE_EXTENSIONS
		.iter()
		.cloned()
		.chain(THUMBNAILABLE_VIDEO_EXTENSIONS.iter().cloned())
		.collect();

	#[cfg(not(feature = "ffmpeg"))]
	THUMBNAILABLE_EXTENSIONS.clone()
});

#[derive(Error, Debug)]
pub enum ThumbnailerError {
	// Internal errors
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	VersionManager(#[from] VersionManagerError<ThumbnailVersion>),
	#[error("failed to encode webp")]
	WebPEncoding { path: Box<Path>, reason: String },
	#[error("error while converting the image")]
	SdImages {
		path: Box<Path>,
		error: sd_images::Error,
	},
	#[error("failed to execute converting task: {0}")]
	Task(#[from] task::JoinError),
	#[cfg(feature = "ffmpeg")]
	#[error(transparent)]
	FFmpeg(#[from] sd_ffmpeg::Error),
	#[error("thumbnail generation timed out for {}", .0.display())]
	TimedOut(Box<Path>),
}

#[derive(Debug, Serialize, Deserialize, Clone, Copy)]
pub enum ThumbnailerEntryKind {
	Image,
	#[cfg(feature = "ffmpeg")]
	Video,
}

#[derive(Serialize, Deserialize, Default, Debug)]
pub struct ThumbnailerMetadata {
	pub created: u32,
	pub skipped: u32,
}

#[cfg(feature = "ffmpeg")]
pub const fn can_generate_thumbnail_for_video(video_extension: &VideoExtension) -> bool {
	use VideoExtension::*;
	// File extensions that are specifically not supported by the thumbnailer
	!matches!(video_extension, Mpg | Swf | M2v | Hevc | M2ts | Mts | Ts)
}

pub const fn can_generate_thumbnail_for_image(image_extension: &ImageExtension) -> bool {
	use ImageExtension::*;

	matches!(
		image_extension,
		Jpg | Jpeg | Png | Webp | Gif | Svg | Heic | Heics | Heif | Heifs | Avif | Bmp | Ico
	)
}

pub const fn can_generate_thumbnail_for_document(document_extension: &DocumentExtension) -> bool {
	use DocumentExtension::*;

	matches!(document_extension, Pdf)
}



File: ./src/object/media/thumbnail/state.rs
-------------------------------------------------
use crate::{library::LibraryId, util::error::FileIOError};

use std::{
	collections::{hash_map::Entry, HashMap, HashSet, VecDeque},
	ffi::OsString,
	path::Path,
};

use async_channel as chan;
use futures_concurrency::future::TryJoin;
use sd_prisma::prisma::location;
use serde::{Deserialize, Serialize};
use tokio::{fs, io};
use tracing::{error, info, trace};

use super::{
	actor::ActorError, get_shard_hex, BatchToProcess, ThumbnailKind, EPHEMERAL_DIR, SAVE_STATE_FILE,
};

#[derive(Debug, Serialize, Deserialize)]
pub(super) struct ThumbsProcessingSaveState {
	pub(super) bookkeeper: BookKeeper,
	pub(super) ephemeral_file_names: HashSet<OsString>,
	// This queues doubles as LIFO and FIFO, assuming LIFO in case of users asking for a new batch
	// by entering a new directory in the explorer, otherwise processing as FIFO
	pub(super) queue: VecDeque<(BatchToProcess, ThumbnailKind)>,
	// These below are FIFO queues, so we can process leftovers from the previous batch first
	pub(super) indexed_leftovers_queue: VecDeque<(BatchToProcess, LibraryId)>,
	pub(super) ephemeral_leftovers_queue: VecDeque<BatchToProcess>,
}

impl Default for ThumbsProcessingSaveState {
	fn default() -> Self {
		Self {
			bookkeeper: BookKeeper::default(),
			ephemeral_file_names: HashSet::with_capacity(128),
			queue: VecDeque::with_capacity(32),
			indexed_leftovers_queue: VecDeque::with_capacity(8),
			ephemeral_leftovers_queue: VecDeque::with_capacity(8),
		}
	}
}

impl ThumbsProcessingSaveState {
	pub(super) async fn load(thumbnails_directory: impl AsRef<Path>) -> Self {
		let resume_file = thumbnails_directory.as_ref().join(SAVE_STATE_FILE);

		match fs::read(&resume_file).await {
			Ok(bytes) => {
				let this = rmp_serde::from_slice::<Self>(&bytes).unwrap_or_else(|e| {
					error!("Failed to deserialize save state at thumbnailer actor: {e:#?}");
					Self::default()
				});

				if let Err(e) = fs::remove_file(&resume_file).await {
					error!(
						"Failed to remove save state file at thumbnailer actor: {:#?}",
						FileIOError::from((resume_file, e))
					);
				}

				info!(
					"Resuming thumbnailer actor state: Existing ephemeral thumbs: {}; \
					Queued batches waiting processing: {}",
					this.ephemeral_file_names.len(),
					this.queue.len()
						+ this.indexed_leftovers_queue.len()
						+ this.ephemeral_leftovers_queue.len()
				);

				this
			}
			Err(e) if e.kind() == io::ErrorKind::NotFound => {
				trace!("No save state found at thumbnailer actor");
				Self::default()
			}
			Err(e) => {
				error!(
					"Failed to read save state at thumbnailer actor: {:#?}",
					FileIOError::from((resume_file, e))
				);
				Self::default()
			}
		}
	}

	pub(super) async fn store(self, thumbnails_directory: impl AsRef<Path>) {
		let resume_file = thumbnails_directory.as_ref().join(SAVE_STATE_FILE);

		info!(
			"Saving thumbnailer actor state: Existing ephemeral thumbs: {}; \
			Queued batches waiting processing: {}",
			self.ephemeral_file_names.len(),
			self.queue.len()
				+ self.indexed_leftovers_queue.len()
				+ self.ephemeral_leftovers_queue.len()
		);

		let Ok(bytes) = rmp_serde::to_vec_named(&self).map_err(|e| {
			error!("Failed to serialize save state at thumbnailer actor: {e:#?}");
		}) else {
			return;
		};

		if let Err(e) = fs::write(&resume_file, bytes).await {
			error!(
				"Failed to write save state at thumbnailer actor: {:#?}",
				FileIOError::from((resume_file, e))
			);
		}
	}
}

pub(super) async fn remove_by_cas_ids(
	thumbnails_directory: &Path,
	cas_ids: Vec<String>,
	kind: ThumbnailKind,
) -> Result<(), ActorError> {
	let base_dir = match kind {
		ThumbnailKind::Ephemeral => thumbnails_directory.join(EPHEMERAL_DIR),
		ThumbnailKind::Indexed(library_id) => thumbnails_directory.join(library_id.to_string()),
	};

	cas_ids
		.into_iter()
		.map(|cas_id| {
			let thumbnail_path = base_dir.join(format!("{}/{cas_id}.webp", get_shard_hex(&cas_id)));

			trace!("Removing thumbnail: {}", thumbnail_path.display());

			async move {
				match fs::remove_file(&thumbnail_path).await {
					Ok(()) => Ok(()),
					Err(e) if e.kind() == io::ErrorKind::NotFound => Ok(()),
					Err(e) => Err(FileIOError::from((thumbnail_path, e))),
				}
			}
		})
		.collect::<Vec<_>>()
		.try_join()
		.await?;

	Ok(())
}

pub(super) type RegisterReporter = (location::id::Type, chan::Sender<(u32, u32)>);

#[derive(Debug, Serialize, Deserialize)]
pub(super) struct BookKeeper {
	work_progress: HashMap<location::id::Type, (u32, u32)>, // (pending, total)

	// We can't save reporter function or a channel to disk, the job must ask again to be registered
	#[serde(skip, default)]
	reporter_by_location: HashMap<location::id::Type, chan::Sender<(u32, u32)>>,
}
impl Default for BookKeeper {
	fn default() -> Self {
		Self {
			work_progress: HashMap::with_capacity(8),
			reporter_by_location: HashMap::with_capacity(8),
		}
	}
}

impl BookKeeper {
	pub(super) async fn add_work(&mut self, location_id: location::id::Type, thumbs_count: u32) {
		let (in_progress, total) = match self.work_progress.entry(location_id) {
			Entry::Occupied(mut entry) => {
				let (in_progress, total) = entry.get_mut();

				*total += thumbs_count;

				(*in_progress, *total)
			}
			Entry::Vacant(entry) => {
				entry.insert((0, thumbs_count));

				(0, thumbs_count)
			}
		};

		if let Some(progress_tx) = self.reporter_by_location.get(&location_id) {
			if progress_tx.send((in_progress, total)).await.is_err() {
				error!(
					"Failed to send progress update to reporter on location <id='{location_id}'>"
				);
			}
		}
	}

	pub(super) fn register_reporter(
		&mut self,
		location_id: location::id::Type,
		reporter_tx: chan::Sender<(u32, u32)>,
	) {
		self.reporter_by_location.insert(location_id, reporter_tx);
	}

	pub(super) async fn add_progress(&mut self, location_id: location::id::Type, progress: u32) {
		if let Some((current_progress, total)) = self.work_progress.get_mut(&location_id) {
			*current_progress += progress;

			if *current_progress == *total {
				if let Some(progress_tx) = self.reporter_by_location.remove(&location_id) {
					if progress_tx.send((*current_progress, *total)).await.is_err() {
						error!(
							"Failed to send progress update to reporter on location <id='{location_id}'>"
						);
					}
				}

				self.work_progress.remove(&location_id);
			} else if let Some(progress_tx) = self.reporter_by_location.get(&location_id) {
				if progress_tx.send((*current_progress, *total)).await.is_err() {
					error!(
						"Failed to send progress update to reporter on location <id='{location_id}'>"
					);
				}
			}
		}
	}
}



File: ./src/object/media/thumbnail/actor.rs
-------------------------------------------------
use crate::{
	api::CoreEvent,
	library::{Libraries, LibraryId, LibraryManagerEvent},
	node::config::NodePreferences,
	util::error::{FileIOError, NonUtf8PathError},
};

use sd_prisma::prisma::{location, PrismaClient};

use std::{
	path::{Path, PathBuf},
	sync::Arc,
};

use async_channel as chan;
use once_cell::sync::OnceCell;
use thiserror::Error;
use tokio::{
	fs, spawn,
	sync::{broadcast, oneshot, watch, Mutex},
	time::{sleep, Instant},
};
use tracing::{error, trace};
use uuid::Uuid;

use super::{
	directory::init_thumbnail_dir,
	process::{generate_thumbnail, ThumbData},
	state::RegisterReporter,
	worker::{worker, WorkerChannels},
	BatchToProcess, ThumbnailKind, ThumbnailerError, ONE_SEC, THUMBNAIL_CACHE_DIR_NAME,
};

static AVAILABLE_PARALLELISM: OnceCell<usize> = OnceCell::new();

#[derive(Error, Debug)]
pub(super) enum ActorError {
	#[error("database error")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	NonUtf8Path(#[from] NonUtf8PathError),
}

#[derive(Debug)]
pub(super) enum DatabaseMessage {
	Add(Uuid, Arc<PrismaClient>),
	Update(Uuid, Arc<PrismaClient>),
	Remove(Uuid),
}

// Thumbnails directory have the following structure:
// thumbnails/
//  version.txt
//  thumbs_to_process.bin # processing save state
//  ephemeral/ # ephemeral ones have it's own directory
//    <cas_id>[0..3]/ # sharding
//       <cas_id>.webp
//  <library_id>/ # we segregate thumbnails by library
//     <cas_id>[0..3]/ # sharding
//        <cas_id>.webp
pub struct Thumbnailer {
	thumbnails_directory: Arc<PathBuf>,
	cas_ids_to_delete_tx: chan::Sender<(Vec<String>, ThumbnailKind)>,
	thumbnails_to_generate_tx: chan::Sender<(BatchToProcess, ThumbnailKind)>,
	progress_reporter_tx: chan::Sender<RegisterReporter>,
	last_single_thumb_generated: Mutex<Instant>,
	reporter: broadcast::Sender<CoreEvent>,
	cancel_tx: chan::Sender<oneshot::Sender<()>>,
}

impl Thumbnailer {
	pub async fn new(
		data_dir: PathBuf,
		libraries_manager: Arc<Libraries>,
		reporter: broadcast::Sender<CoreEvent>,
		node_preferences_rx: watch::Receiver<NodePreferences>,
	) -> Self {
		let thumbnails_directory = Arc::new(
			init_thumbnail_dir(&data_dir, Arc::clone(&libraries_manager))
				.await
				.unwrap_or_else(|e| {
					error!("Failed to initialize thumbnail directory: {e:#?}");
					let mut thumbnails_directory = data_dir;
					thumbnails_directory.push(THUMBNAIL_CACHE_DIR_NAME);
					thumbnails_directory
				}),
		);

		let (progress_management_tx, progress_management_rx) = chan::bounded(16);

		let (databases_tx, databases_rx) = chan::bounded(4);
		let (thumbnails_to_generate_tx, ephemeral_thumbnails_to_generate_rx) = chan::unbounded();
		let (cas_ids_to_delete_tx, cas_ids_to_delete_rx) = chan::bounded(16);
		let (cancel_tx, cancel_rx) = chan::bounded(1);

		AVAILABLE_PARALLELISM
			.set(std::thread::available_parallelism().map_or_else(
				|e| {
					error!("Failed to get available parallelism: {e:#?}");
					4
				},
				|non_zero| non_zero.get(),
			))
			.ok();

		spawn({
			let progress_management_rx = progress_management_rx.clone();
			let cancel_rx = cancel_rx.clone();
			let thumbnails_directory = Arc::clone(&thumbnails_directory);
			let reporter = reporter.clone();
			let node_preferences = node_preferences_rx.clone();

			async move {
				while let Err(e) = spawn(worker(
					*AVAILABLE_PARALLELISM
						.get()
						.expect("BATCH_SIZE is set at thumbnailer new method"),
					node_preferences.clone(),
					reporter.clone(),
					thumbnails_directory.clone(),
					WorkerChannels {
						progress_management_rx: progress_management_rx.clone(),
						databases_rx: databases_rx.clone(),
						cas_ids_to_delete_rx: cas_ids_to_delete_rx.clone(),
						thumbnails_to_generate_rx: ephemeral_thumbnails_to_generate_rx.clone(),
						cancel_rx: cancel_rx.clone(),
					},
				))
				.await
				{
					error!(
						"Error on Thumbnail Remover Actor; \
						Error: {e}; \
						Restarting the worker loop...",
					);
				}
			}
		});

		spawn({
			let rx = libraries_manager.rx.clone();
			let thumbnails_directory = Arc::clone(&thumbnails_directory);

			async move {
				let subscribe_res = rx
					.subscribe(|event| {
						let databases_tx = databases_tx.clone();

						let thumbnails_directory = &thumbnails_directory;

						async move {
							match event {
								LibraryManagerEvent::Load(library) => {
									let library_dir =
										thumbnails_directory.join(library.id.to_string());

									if let Err(e) = fs::create_dir_all(&library_dir).await {
										error!(
											"Failed to create library dir for thumbnails: {:#?}",
											FileIOError::from((library_dir, e))
										);
									}

									databases_tx
										.send(DatabaseMessage::Add(
											library.id,
											Arc::clone(&library.db),
										))
										.await
										.expect("critical thumbnailer error: databases channel closed on send add")
								}

								LibraryManagerEvent::Edit(library)
								| LibraryManagerEvent::InstancesModified(library) => databases_tx
									.send(DatabaseMessage::Update(
										library.id,
										Arc::clone(&library.db),
									))
									.await
									.expect("critical thumbnailer error: databases channel closed on send update"),

								LibraryManagerEvent::Delete(library) => databases_tx
									.send(DatabaseMessage::Remove(library.id))
									.await
									.expect("critical thumbnailer error: databases channel closed on send delete"),
							}
						}
					})
					.await;

				if subscribe_res.is_err() {
					error!("Thumbnailer actor has crashed...")
				}
			}
		});

		Self {
			thumbnails_directory,
			cas_ids_to_delete_tx,
			thumbnails_to_generate_tx,
			progress_reporter_tx: progress_management_tx,
			last_single_thumb_generated: Mutex::new(Instant::now()),
			reporter,
			cancel_tx,
		}
	}

	#[inline]
	async fn new_batch(&self, batch: BatchToProcess, kind: ThumbnailKind) {
		if !batch.batch.is_empty() {
			self.thumbnails_to_generate_tx
				.send((batch, kind))
				.await
				.expect("critical thumbnailer error: failed to send new batch");
		} else {
			trace!("Empty batch received, skipping...");
		}
	}

	#[inline]
	pub async fn new_ephemeral_thumbnails_batch(&self, batch: BatchToProcess) {
		self.new_batch(batch, ThumbnailKind::Ephemeral).await
	}

	#[inline]
	pub async fn new_indexed_thumbnails_batch(&self, batch: BatchToProcess, library_id: LibraryId) {
		self.new_batch(batch, ThumbnailKind::Indexed(library_id))
			.await
	}

	#[inline]
	pub async fn new_indexed_thumbnails_tracked_batch(
		&self,
		mut batch: BatchToProcess,
		library_id: LibraryId,
		location_id: location::id::Type,
	) {
		batch.location_id = Some(location_id);

		self.new_batch(batch, ThumbnailKind::Indexed(library_id))
			.await;
	}

	#[inline]
	pub async fn register_reporter(
		&self,
		location_id: location::id::Type,
		progress_tx: chan::Sender<(u32, u32)>,
	) {
		self.progress_reporter_tx
			.send((location_id, progress_tx))
			.await
			.expect("critical thumbnailer error: failed to send register reporter fn");
	}

	#[inline]
	async fn remove_cas_ids(&self, cas_ids: Vec<String>, kind: ThumbnailKind) {
		self.cas_ids_to_delete_tx
			.send((cas_ids, kind))
			.await
			.expect("critical thumbnailer error: failed to send cas ids to delete");
	}

	#[inline]
	pub async fn remove_ephemeral_cas_ids(&self, cas_ids: Vec<String>) {
		self.remove_cas_ids(cas_ids, ThumbnailKind::Ephemeral).await
	}

	#[inline]
	pub async fn remove_indexed_cas_ids(&self, cas_ids: Vec<String>, library_id: LibraryId) {
		self.remove_cas_ids(cas_ids, ThumbnailKind::Indexed(library_id))
			.await
	}

	#[inline]
	pub async fn shutdown(&self) {
		let (tx, rx) = oneshot::channel();
		self.cancel_tx
			.send(tx)
			.await
			.expect("critical thumbnailer error: failed to send shutdown signal");

		rx.await
			.expect("critical thumbnailer error: failed to receive shutdown signal response");
	}

	/// WARNING!!!! DON'T USE THIS METHOD IN A LOOP!!!!!!!!!!!!! It will be pretty slow on purpose!
	pub async fn generate_single_indexed_thumbnail(
		&self,
		extension: &str,
		cas_id: String,
		path: impl AsRef<Path>,
		library_id: LibraryId,
	) -> Result<(), ThumbnailerError> {
		self.generate_single_thumbnail(extension, cas_id, path, ThumbnailKind::Indexed(library_id))
			.await
	}

	async fn generate_single_thumbnail(
		&self,
		extension: &str,
		cas_id: String,
		path: impl AsRef<Path>,
		kind: ThumbnailKind,
	) -> Result<(), ThumbnailerError> {
		let mut last_single_thumb_generated_guard = self.last_single_thumb_generated.lock().await;

		let elapsed = Instant::now() - *last_single_thumb_generated_guard;
		if elapsed < ONE_SEC {
			// This will choke up in case someone try to use this method in a loop, otherwise
			// it will consume all the machine resources like a gluton monster from hell
			sleep(ONE_SEC - elapsed).await;
		}

		let res = generate_thumbnail(
			self.thumbnails_directory.as_ref().clone(),
			ThumbData {
				extension,
				cas_id,
				path,
				in_background: false,
				should_regenerate: false,
				kind,
			},
			self.reporter.clone(),
		)
		.await
		.map(|_| ());

		*last_single_thumb_generated_guard = Instant::now();

		res
	}
}



File: ./src/object/media/thumbnail/directory.rs
-------------------------------------------------
use crate::{
	library::{Libraries, LibraryId},
	object::media::thumbnail::ONE_SEC,
	util::{
		error::FileIOError,
		version_manager::{Kind, ManagedVersion, VersionManager, VersionManagerError},
	},
};

use sd_prisma::prisma::{file_path, PrismaClient};
use serde_repr::{Deserialize_repr, Serialize_repr};

use std::{
	collections::{HashMap, HashSet},
	path::{Path, PathBuf},
	sync::Arc,
};

use futures_concurrency::future::{Join, TryJoin};
use int_enum::IntEnum;
use tokio::{
	fs, io, spawn,
	time::{sleep, timeout},
};
use tracing::{debug, error, info, trace, warn};

use super::{
	get_shard_hex, ThumbnailerError, EPHEMERAL_DIR, THIRTY_SECS, THUMBNAIL_CACHE_DIR_NAME,
	VERSION_FILE, WEBP_EXTENSION,
};

#[derive(
	IntEnum, Debug, Clone, Copy, Eq, PartialEq, strum::Display, Serialize_repr, Deserialize_repr,
)]
#[repr(u64)]
pub enum ThumbnailVersion {
	V1 = 1,
	V2 = 2,
	V3 = 3,
}

impl ManagedVersion<Self> for ThumbnailVersion {
	const LATEST_VERSION: Self = Self::V3;

	const KIND: Kind = Kind::PlainText;

	type MigrationError = ThumbnailerError;

	fn from_latest_version() -> Option<Self> {
		Some(Self::LATEST_VERSION)
	}
}

pub(super) async fn init_thumbnail_dir(
	data_dir: impl AsRef<Path>,
	libraries_manager: Arc<Libraries>,
) -> Result<PathBuf, ThumbnailerError> {
	debug!("Initializing thumbnail directory");
	let thumbnails_directory = data_dir.as_ref().join(THUMBNAIL_CACHE_DIR_NAME);

	debug!("Thumbnail directory: {:?}", thumbnails_directory);

	// create thumbnails base directory
	fs::create_dir_all(&thumbnails_directory)
		.await
		.map_err(|e| FileIOError::from((&thumbnails_directory, e)))?;

	spawn({
		let thumbnails_directory = thumbnails_directory.clone();
		async move {
			let Ok(databases) = timeout(THIRTY_SECS, async move {
				loop {
					let libraries = libraries_manager.get_all().await;
					if !libraries.is_empty() {
						break libraries
							.into_iter()
							.map(|library| (library.id, Arc::clone(&library.db)))
							.collect::<HashMap<_, _>>();
					}

					sleep(ONE_SEC).await;
				}
			})
			.await
			else {
				warn!(
					"Failed to get libraries after 30 seconds, thumbnailer migration will not work; \
					Ignore this warning if you don't created libraries yet."
				);
				return;
			};

			if let Err(e) = process_migration(thumbnails_directory, databases).await {
				error!("Failed to migrate thumbnails: {e:#?}");
			}
		}
	});

	Ok(thumbnails_directory)
}

async fn process_migration(
	thumbnails_directory: impl AsRef<Path>,
	databases: HashMap<LibraryId, Arc<PrismaClient>>,
) -> Result<(), ThumbnailerError> {
	let thumbnails_directory = thumbnails_directory.as_ref();

	// create all other directories, for each library and for ephemeral thumbnails
	databases
		.keys()
		.map(|library_id| thumbnails_directory.join(library_id.to_string()))
		.chain([thumbnails_directory.join(EPHEMERAL_DIR)])
		.map(|path| async move {
			fs::create_dir_all(&path)
				.await
				.map_err(|e| FileIOError::from((&path, e)))
		})
		.collect::<Vec<_>>()
		.join()
		.await
		.into_iter()
		.collect::<Result<Vec<_>, _>>()?;

	VersionManager::<ThumbnailVersion, ThumbnailVersion>::migrate_and_load(
		thumbnails_directory.join(VERSION_FILE),
		|current, next| {
			let databases = &databases;
			async move {
				match (current, next) {
					(ThumbnailVersion::V1, ThumbnailVersion::V2) => {
						move_to_shards(thumbnails_directory).await
					}
					(ThumbnailVersion::V2, ThumbnailVersion::V3) => {
						segregate_thumbnails_by_library(thumbnails_directory, databases).await
					}

					_ => {
						error!("Thumbnail version is not handled: {:?}", current);
						Err(VersionManagerError::UnexpectedMigration {
							current_version: current.int_value(),
							next_version: next.int_value(),
						}
						.into())
					}
				}
			}
		},
	)
	.await
	.map(|_| ())
}

/// This function moves all webp files in the thumbnail directory to their respective shard folders.
/// It is used to migrate from V1 to V2.
async fn move_to_shards(thumbnails_directory: impl AsRef<Path>) -> Result<(), ThumbnailerError> {
	let thumbnails_directory = thumbnails_directory.as_ref();

	let mut dir_entries = fs::read_dir(thumbnails_directory)
		.await
		.map_err(|source| FileIOError::from((thumbnails_directory, source)))?;

	let mut count = 0;

	while let Ok(Some(entry)) = dir_entries.next_entry().await {
		if entry
			.file_type()
			.await
			.map_err(|e| FileIOError::from((entry.path(), e)))?
			.is_file()
		{
			let path = entry.path();
			if path.extension() == Some(WEBP_EXTENSION.as_ref()) {
				let file_name = entry.file_name();

				// we know they're cas_id's, so they're valid utf8
				let shard_folder = get_shard_hex(file_name.to_str().expect("Failed to parse UTF8"));

				let new_dir = thumbnails_directory.join(shard_folder);
				fs::create_dir_all(&new_dir)
					.await
					.map_err(|source| FileIOError::from((new_dir.clone(), source)))?;

				let new_path = new_dir.join(file_name);
				fs::rename(&path, &new_path)
					.await
					.map_err(|source| FileIOError::from((path.clone(), source)))?;
				count += 1;
			}
		}
	}

	info!(
		"Moved {} webp files to their respective shard folders.",
		count
	);

	Ok(())
}

async fn segregate_thumbnails_by_library(
	thumbnails_directory: impl AsRef<Path>,
	databases: &HashMap<LibraryId, Arc<PrismaClient>>,
) -> Result<(), ThumbnailerError> {
	// We already created the library folders in init_thumbnail_dir, so we can just move the files
	// to their respective folders

	let thumbnails_directory = thumbnails_directory.as_ref();

	databases
		.iter()
		.map(|(library_id, db)| (*library_id, Arc::clone(db)))
		.map(|(library_id, db)| {
			let library_thumbs_dir = thumbnails_directory.join(library_id.to_string());
			let old_thumbs_dir = thumbnails_directory.to_path_buf();
			spawn(async move {
				let mut shards_to_create = HashSet::new();

				let to_move = db
					.file_path()
					.find_many(vec![file_path::cas_id::not(None)])
					.select(file_path::select!({ cas_id }))
					.exec()
					.await?
					.into_iter()
					.filter_map(|file_path| file_path.cas_id)
					.map(|cas_id| {
						let new_shard = get_shard_hex(&cas_id).to_string();
						let new_sharded_filename = format!("{new_shard}/{cas_id}.webp");
						let old_sharded_filename = format!("{}/{cas_id}.webp", &cas_id[0..2]);

						(new_shard, new_sharded_filename, old_sharded_filename)
					})
					.map(|(new_shard, new_sharded_filename, old_sharded_filename)| {
						let old = old_thumbs_dir.join(old_sharded_filename);
						let new = library_thumbs_dir.join(new_sharded_filename);
						let new_shard_dir = library_thumbs_dir.join(new_shard);

						shards_to_create.insert(new_shard_dir);

						async move {
							trace!(
								"Moving thumbnail from old location to new location: {} -> {}",
								old.display(),
								new.display()
							);

							match fs::rename(&old, new).await {
								Ok(_) => Ok(1),
								Err(e) if e.kind() == io::ErrorKind::NotFound => {
									// Thumbnail not found, it probably wasn't processed yet
									Ok(0)
								}
								Err(e) => {
									Err(ThumbnailerError::FileIO(FileIOError::from((old, e))))
								}
							}
						}
					})
					.collect::<Vec<_>>();

				let shards_created_count = shards_to_create
					.into_iter()
					.map(|path| async move {
						fs::create_dir_all(&path)
							.await
							.map_err(|e| FileIOError::from((path, e)))
					})
					.collect::<Vec<_>>()
					.try_join()
					.await?
					.len();

				let moved_count = to_move.try_join().await?.into_iter().sum::<u64>();

				info!(
					"Created {shards_created_count} shards and moved {moved_count} \
					thumbnails to library folder {library_id}"
				);

				Ok::<_, ThumbnailerError>(())
			})
		})
		.collect::<Vec<_>>()
		.try_join()
		.await?
		.into_iter()
		.collect::<Result<_, _>>()?;

	// Now that we moved all files from all databases, everything else should be ephemeral thumbnails
	// so we can just move all of them to the ephemeral directory
	let ephemeral_thumbs_dir = thumbnails_directory.join(EPHEMERAL_DIR);

	let mut shards_to_create = HashSet::new();
	let mut to_move = vec![];

	let mut read_thumbs_dir = fs::read_dir(thumbnails_directory)
		.await
		.map_err(|e| FileIOError::from((thumbnails_directory, e)))?;

	let mut empty_shards = vec![];

	while let Some(shard_entry) = read_thumbs_dir
		.next_entry()
		.await
		.map_err(|e| FileIOError::from((thumbnails_directory, e)))?
	{
		let old_shard_path = shard_entry.path();
		if shard_entry
			.file_type()
			.await
			.map_err(|e| FileIOError::from((&old_shard_path, e)))?
			.is_dir()
		{
			let mut read_shard_dir = fs::read_dir(&old_shard_path)
				.await
				.map_err(|e| FileIOError::from((&old_shard_path, e)))?;

			while let Some(thumb_entry) = read_shard_dir
				.next_entry()
				.await
				.map_err(|e| FileIOError::from((&old_shard_path, e)))?
			{
				let thumb_path = thumb_entry.path();
				if thumb_path.extension() == Some(WEBP_EXTENSION.as_ref()) {
					let thumb_filename = thumb_entry.file_name();

					let mut new_ephemeral_shard = ephemeral_thumbs_dir.join(get_shard_hex(
						thumb_filename.to_str().expect("cas_ids are utf-8"),
					));

					shards_to_create.insert(new_ephemeral_shard.clone());

					new_ephemeral_shard.push(thumb_filename);

					to_move.push(async move {
						trace!(
							"Moving thumbnail from old location to new location: {} -> {}",
							thumb_path.display(),
							new_ephemeral_shard.display()
						);

						fs::rename(&thumb_path, &new_ephemeral_shard)
							.await
							.map_err(|e| FileIOError::from((thumb_path, e)))
					});
				}
			}

			empty_shards.push(old_shard_path);
		}
	}

	shards_to_create
		.into_iter()
		.map(|path| async move {
			fs::create_dir_all(&path)
				.await
				.map_err(|e| FileIOError::from((path, e)))
		})
		.collect::<Vec<_>>()
		.try_join()
		.await?;

	let moved_shard = to_move.try_join().await?.len();

	info!("Moved {moved_shard} shards to the ephemeral directory");

	empty_shards
		.into_iter()
		.filter_map(|path| {
			path.file_name()
				.map_or(false, |name| name.len() == 2)
				.then_some(async move {
					trace!("Removing empty shard directory: {}", path.display());
					fs::remove_dir(&path)
						.await
						.map_err(|e| FileIOError::from((path, e)))
				})
		})
		.collect::<Vec<_>>()
		.try_join()
		.await?;

	Ok(())
}



File: ./src/object/media/thumbnail/process.rs
-------------------------------------------------
use crate::{api::CoreEvent, util::error::FileIOError};

use sd_file_ext::extensions::{DocumentExtension, ImageExtension};
use sd_images::{format_image, scale_dimensions, ConvertableExtension};
use sd_media_metadata::image::Orientation;
use sd_prisma::prisma::location;

use std::{
	collections::VecDeque,
	ffi::OsString,
	ops::Deref,
	path::{Path, PathBuf},
	str::FromStr,
	sync::Arc,
};

use async_channel as chan;
use futures_concurrency::future::{Join, Race};
use image::{self, imageops, DynamicImage, GenericImageView};
use serde::{Deserialize, Serialize};
use tokio::{
	fs, io,
	sync::{broadcast, oneshot, Semaphore},
	task::{spawn, spawn_blocking},
	time::timeout,
};
use tokio_stream::StreamExt;
use tracing::{debug, error, trace, warn};
use webp::Encoder;

use super::{
	can_generate_thumbnail_for_document, can_generate_thumbnail_for_image, get_thumb_key,
	preferences::ThumbnailerPreferences, shard::get_shard_hex, ThumbnailKind, ThumbnailerError,
	EPHEMERAL_DIR, TARGET_PX, TARGET_QUALITY, THIRTY_SECS, WEBP_EXTENSION,
};

#[derive(Debug, Serialize, Deserialize)]
pub struct GenerateThumbnailArgs {
	pub extension: String,
	pub cas_id: String,
	pub path: PathBuf,
}

impl GenerateThumbnailArgs {
	pub fn new(extension: String, cas_id: String, path: PathBuf) -> Self {
		Self {
			extension,
			cas_id,
			path,
		}
	}
}

#[derive(Debug, Serialize, Deserialize)]
pub struct BatchToProcess {
	pub(super) batch: Vec<GenerateThumbnailArgs>,
	pub(super) should_regenerate: bool,
	pub(super) in_background: bool,
	pub(super) location_id: Option<location::id::Type>,
}

impl BatchToProcess {
	pub fn new(
		batch: Vec<GenerateThumbnailArgs>,
		should_regenerate: bool,
		in_background: bool,
	) -> Self {
		Self {
			batch,
			should_regenerate,
			in_background,
			location_id: None,
		}
	}
}

pub(super) struct ProcessorControlChannels {
	pub stop_rx: chan::Receiver<oneshot::Sender<()>>,
	pub done_tx: oneshot::Sender<()>,
	pub batch_report_progress_tx: chan::Sender<(location::id::Type, u32)>,
}

pub(super) async fn batch_processor(
	thumbnails_directory: Arc<PathBuf>,
	(
		BatchToProcess {
			batch,
			should_regenerate,
			in_background,
			location_id,
		},
		kind,
	): (BatchToProcess, ThumbnailKind),
	generated_ephemeral_thumbs_file_names_tx: chan::Sender<Vec<OsString>>,
	ProcessorControlChannels {
		stop_rx,
		done_tx,
		batch_report_progress_tx,
	}: ProcessorControlChannels,
	leftovers_tx: chan::Sender<(BatchToProcess, ThumbnailKind)>,
	reporter: broadcast::Sender<CoreEvent>,
	(available_parallelism, thumbnailer_preferences): (usize, ThumbnailerPreferences),
) {
	let in_parallel_count = if !in_background {
		available_parallelism
	} else {
		usize::max(
			// If the user sets the background processing percentage to 0, we still want to process at least sequentially
			thumbnailer_preferences.background_processing_percentage() as usize
				* available_parallelism
				/ 100,
			1,
		)
	};

	debug!(
		"Processing thumbnails batch of kind {kind:?} with size {} in {}, \
		at most {in_parallel_count} thumbnails at a time",
		batch.len(),
		if in_background {
			"background"
		} else {
			"foreground"
		},
	);

	let semaphore = Arc::new(Semaphore::new(in_parallel_count));

	let batch_size = batch.len();

	// Tranforming to `VecDeque` so we don't need to move anything as we consume from the beginning
	// This from is guaranteed to be O(1)
	let mut queue = VecDeque::from(batch);

	enum RaceOutputs {
		Processed,
		Stop(oneshot::Sender<()>),
	}

	let (maybe_cas_ids_tx, maybe_cas_ids_rx) = if kind == ThumbnailKind::Ephemeral {
		let (tx, rx) = chan::bounded(batch_size);
		(Some(tx), Some(rx))
	} else {
		(None, None)
	};

	let maybe_stopped_tx = if let RaceOutputs::Stop(stopped_tx) = (
		async {
			let mut join_handles = Vec::with_capacity(batch_size);

			while !queue.is_empty() {
				let permit = Arc::clone(&semaphore)
					.acquire_owned()
					.await
					.expect("this semaphore never closes");

				let GenerateThumbnailArgs {
					extension,
					cas_id,
					path,
				} = queue.pop_front().expect("queue is not empty");

				// As we got a permit, then there is available CPU to process this thumbnail
				join_handles.push(spawn({
					let reporter = reporter.clone();
					let thumbnails_directory = thumbnails_directory.as_ref().clone();
					let report_progress_tx = batch_report_progress_tx.clone();
					let maybe_cas_ids_tx = maybe_cas_ids_tx.clone();

					async move {
						let res = timeout(THIRTY_SECS, async {
							generate_thumbnail(
								thumbnails_directory,
								ThumbData {
									extension: &extension,
									cas_id,
									path: &path,
									in_background,
									should_regenerate,
									kind,
								},
								reporter,
							)
							.await
							.map(|cas_id| {
								// this send_blocking never blocks as we have a bounded channel with
								// the same capacity as the batch size, so there is always a space
								// in the queue
								if let Some(cas_ids_tx) = maybe_cas_ids_tx {
									if cas_ids_tx
										.send_blocking(OsString::from(format!("{}.webp", cas_id)))
										.is_err()
									{
										warn!("No one to listen to generated ephemeral thumbnail cas id");
									}
								}
							})
						})
						.await
						.unwrap_or_else(|_| {
							Err(ThumbnailerError::TimedOut(path.into_boxed_path()))
						});

						if let Some(location_id) = location_id {
							report_progress_tx.send((location_id, 1)).await.ok();
						}

						drop(permit);

						res
					}
				}));
			}

			for res in join_handles.join().await {
				match res {
					Ok(Ok(())) => { /* Everything is awesome! */ }
					Ok(Err(e)) => {
						error!(
							"Failed to generate thumbnail for {} location: {e:#?}",
							if let ThumbnailKind::Ephemeral = kind {
								"ephemeral"
							} else {
								"indexed"
							}
						)
					}
					Err(e) => {
						error!("Failed to join thumbnail generation task: {e:#?}");
					}
				}
			}

			if let Some(cas_ids_tx) = &maybe_cas_ids_tx {
				cas_ids_tx.close();
			}

			trace!("Processed batch with {batch_size} thumbnails");

			RaceOutputs::Processed
		},
		async {
			let tx = stop_rx
				.recv()
				.await
				.expect("Critical error on thumbnails actor");
			trace!("Received a stop signal");
			RaceOutputs::Stop(tx)
		},
	)
		.race()
		.await
	{
		// Our queue is always contiguous, so this `from` is free
		let leftovers = Vec::from(queue);

		trace!(
			"Stopped with {} thumbnails left to process",
			leftovers.len()
		);
		if !leftovers.is_empty()
			&& leftovers_tx
				.send((
					BatchToProcess {
						batch: leftovers,
						should_regenerate,
						in_background: true, // Leftovers should always be in background
						location_id,
					},
					kind,
				))
				.await
				.is_err()
		{
			error!("Thumbnail actor is dead: Failed to send leftovers")
		}

		if let Some(cas_ids_tx) = &maybe_cas_ids_tx {
			cas_ids_tx.close();
		}

		Some(stopped_tx)
	} else {
		None
	};

	if let Some(cas_ids_rx) = maybe_cas_ids_rx {
		if generated_ephemeral_thumbs_file_names_tx
			.send(cas_ids_rx.collect().await)
			.await
			.is_err()
		{
			error!("Thumbnail actor is dead: Failed to send generated cas ids")
		}
	}

	if let Some(stopped_tx) = maybe_stopped_tx {
		stopped_tx.send(()).ok();
	} else {
		trace!("Finished batch!");
	}

	done_tx.send(()).ok();
}

pub(super) struct ThumbData<'ext, P: AsRef<Path>> {
	pub extension: &'ext str,
	pub cas_id: String,
	pub path: P,
	pub in_background: bool,
	pub should_regenerate: bool,
	pub kind: ThumbnailKind,
}

pub(super) async fn generate_thumbnail(
	thumbnails_directory: PathBuf,
	ThumbData {
		extension,
		cas_id,
		path,
		in_background,
		should_regenerate,
		kind,
	}: ThumbData<'_, impl AsRef<Path>>,
	reporter: broadcast::Sender<CoreEvent>,
) -> Result<String, ThumbnailerError> {
	let path = path.as_ref();
	trace!("Generating thumbnail for {}", path.display());

	let mut output_path = thumbnails_directory;
	match kind {
		ThumbnailKind::Ephemeral => output_path.push(EPHEMERAL_DIR),
		ThumbnailKind::Indexed(library_id) => output_path.push(library_id.to_string()),
	};
	output_path.push(get_shard_hex(&cas_id));
	output_path.push(&cas_id);
	output_path.set_extension(WEBP_EXTENSION);

	if let Err(e) = fs::metadata(&output_path).await {
		if e.kind() != io::ErrorKind::NotFound {
			error!(
				"Failed to check if thumbnail exists, but we will try to generate it anyway: {e:#?}"
			);
		}
	// Otherwise we good, thumbnail doesn't exist so we can generate it
	} else if !should_regenerate {
		trace!(
			"Skipping thumbnail generation for {} because it already exists",
			path.display()
		);
		return Ok(cas_id);
	}

	if let Ok(extension) = ImageExtension::from_str(extension) {
		if can_generate_thumbnail_for_image(&extension) {
			generate_image_thumbnail(&path, &output_path).await?;
		}
	} else if let Ok(extension) = DocumentExtension::from_str(extension) {
		if can_generate_thumbnail_for_document(&extension) {
			generate_image_thumbnail(&path, &output_path).await?;
		}
	}

	#[cfg(feature = "ffmpeg")]
	{
		use crate::object::media::thumbnail::can_generate_thumbnail_for_video;
		use sd_file_ext::extensions::VideoExtension;

		if let Ok(extension) = VideoExtension::from_str(extension) {
			if can_generate_thumbnail_for_video(&extension) {
				generate_video_thumbnail(&path, &output_path).await?;
			}
		}
	}

	if !in_background {
		trace!("Emitting new thumbnail event");
		if reporter
			.send(CoreEvent::NewThumbnail {
				thumb_key: get_thumb_key(&cas_id, kind),
			})
			.is_err()
		{
			warn!("Error sending event to Node's event bus");
		}
	}

	trace!("Generated thumbnail for {}", path.display());

	Ok(cas_id)
}

async fn generate_image_thumbnail(
	file_path: impl AsRef<Path>,
	output_path: impl AsRef<Path>,
) -> Result<(), ThumbnailerError> {
	let file_path = file_path.as_ref().to_path_buf();

	let webp = spawn_blocking(move || -> Result<_, ThumbnailerError> {
		let mut img = format_image(&file_path).map_err(|e| ThumbnailerError::SdImages {
			path: file_path.clone().into_boxed_path(),
			error: e,
		})?;

		let (w, h) = img.dimensions();
		let (w_scaled, h_scaled) = scale_dimensions(w as f32, h as f32, TARGET_PX);

		// Optionally, resize the existing photo and convert back into DynamicImage
		if w != w_scaled && h != h_scaled {
			img = DynamicImage::ImageRgba8(imageops::resize(
				&img,
				w_scaled,
				h_scaled,
				imageops::FilterType::Triangle,
			));
		}

		// this corrects the rotation/flip of the image based on the *available* exif data
		// not all images have exif data, so we don't error. we also don't rotate HEIF as that's against the spec
		if let Some(orientation) = Orientation::from_path(&file_path) {
			if ConvertableExtension::try_from(file_path.as_ref())
				.expect("we already checked if the image was convertable")
				.should_rotate()
			{
				img = orientation.correct_thumbnail(img);
			}
		}

		// Create the WebP encoder for the above image
		let encoder =
			Encoder::from_image(&img).map_err(|reason| ThumbnailerError::WebPEncoding {
				path: file_path.into_boxed_path(),
				reason: reason.to_string(),
			})?;

		// Type WebPMemory is !Send, which makes the Future in this function !Send,
		// this make us `deref` to have a `&[u8]` and then `to_owned` to make a Vec<u8>
		// which implies on a unwanted clone...
		Ok(encoder.encode(TARGET_QUALITY).deref().to_owned())
	})
	.await??;

	let output_path = output_path.as_ref();

	if let Some(shard_dir) = output_path.parent() {
		fs::create_dir_all(shard_dir)
			.await
			.map_err(|e| FileIOError::from((shard_dir, e)))?;
	} else {
		error!(
			"Failed to get parent directory of '{}' for sharding parent directory",
			output_path.display()
		);
	}

	fs::write(output_path, &webp)
		.await
		.map_err(|e| FileIOError::from((output_path, e)))
		.map_err(Into::into)
}

#[cfg(feature = "ffmpeg")]
async fn generate_video_thumbnail(
	file_path: impl AsRef<Path>,
	output_path: impl AsRef<Path>,
) -> Result<(), ThumbnailerError> {
	use sd_ffmpeg::to_thumbnail;

	to_thumbnail(file_path, output_path, 256, TARGET_QUALITY)
		.await
		.map_err(Into::into)
}



File: ./src/object/media/thumbnail/shard.rs
-------------------------------------------------
/// The practice of dividing files into hex coded folders, often called "sharding,"
/// is mainly used to optimize file system performance. File systems can start to slow down
/// as the number of files in a directory increases. Thus, it's often beneficial to split
/// files into multiple directories to avoid this performance degradation.

/// `get_shard_hex` takes a cas_id (a hexadecimal hash) as input and returns the first
/// three characters of the hash as the directory name. Because we're using these first
/// three characters of a the hash, this will give us 4096 (16^3) possible directories,
/// named 000 to fff.
pub fn get_shard_hex(cas_id: &str) -> &str {
	// Use the first three characters of the hash as the directory name
	&cas_id[0..3]
}



File: ./src/object/media/mod.rs
-------------------------------------------------
pub mod media_data_extractor;
pub mod media_processor;
pub mod thumbnail;

pub use media_processor::MediaProcessorJobInit;
use sd_media_metadata::ImageMetadata;
use sd_prisma::prisma::media_data::*;

use self::media_data_extractor::MediaDataError;

pub fn media_data_image_to_query(
	mdi: ImageMetadata,
	object_id: object_id::Type,
) -> Result<CreateUnchecked, MediaDataError> {
	Ok(CreateUnchecked {
		object_id,
		_params: vec![
			camera_data::set(serde_json::to_vec(&mdi.camera_data).ok()),
			media_date::set(serde_json::to_vec(&mdi.date_taken).ok()),
			resolution::set(serde_json::to_vec(&mdi.resolution).ok()),
			media_location::set(serde_json::to_vec(&mdi.location).ok()),
			artist::set(mdi.artist),
			description::set(mdi.description),
			copyright::set(mdi.copyright),
			exif_version::set(mdi.exif_version),
			epoch_time::set(mdi.date_taken.map(|x| x.unix_timestamp())),
		],
	})
}

#[cfg(feature = "location-watcher")]
pub fn media_data_image_to_query_params(
	mdi: ImageMetadata,
) -> Result<Vec<SetParam>, MediaDataError> {
	Ok(vec![
		camera_data::set(serde_json::to_vec(&mdi.camera_data).ok()),
		media_date::set(serde_json::to_vec(&mdi.date_taken).ok()),
		resolution::set(serde_json::to_vec(&mdi.resolution).ok()),
		media_location::set(serde_json::to_vec(&mdi.location).ok()),
		artist::set(mdi.artist),
		description::set(mdi.description),
		copyright::set(mdi.copyright),
		exif_version::set(mdi.exif_version),
		epoch_time::set(mdi.date_taken.map(|x| x.unix_timestamp())),
	])
}

pub fn media_data_image_from_prisma_data(
	data: sd_prisma::prisma::media_data::Data,
) -> Result<ImageMetadata, MediaDataError> {
	Ok(ImageMetadata {
		camera_data: from_slice_option_to_option(data.camera_data).unwrap_or_default(),
		date_taken: from_slice_option_to_option(data.media_date).unwrap_or_default(),
		resolution: from_slice_option_to_option(data.resolution).unwrap_or_default(),
		location: from_slice_option_to_option(data.media_location),
		artist: data.artist,
		description: data.description,
		copyright: data.copyright,
		exif_version: data.exif_version,
	})
}

#[must_use]
fn from_slice_option_to_option<T: serde::Serialize + serde::de::DeserializeOwned>(
	value: Option<Vec<u8>>,
) -> Option<T> {
	value
		.map(|x| serde_json::from_slice(&x).ok())
		.unwrap_or_default()
}



File: ./src/object/media/media_processor/job.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{
		CurrentStep, JobError, JobInitOutput, JobReportUpdate, JobResult, JobStepOutput,
		StatefulJob, WorkerContext,
	},
	library::Library,
	location::file_path_helper::{
		ensure_file_path_exists, ensure_sub_path_is_directory, ensure_sub_path_is_in_location,
		file_path_for_media_processor, IsolatedFilePathData,
	},
	prisma::{location, PrismaClient},
	util::db::maybe_missing,
	Node,
};

use sd_file_ext::extensions::Extension;

use std::{
	hash::Hash,
	path::{Path, PathBuf},
	pin::pin,
	time::Duration,
};

use async_channel as chan;
use futures::StreamExt;
use itertools::Itertools;
use prisma_client_rust::{raw, PrismaValue};
use serde::{Deserialize, Serialize};
use serde_json::json;
use tokio::time::sleep;
use tracing::{debug, error, info, trace, warn};

use super::{
	media_data_extractor, process,
	thumbnail::{self, GenerateThumbnailArgs},
	BatchToProcess, MediaProcessorError, MediaProcessorMetadata,
};

const BATCH_SIZE: usize = 10;

#[derive(Serialize, Deserialize, Debug)]
pub struct MediaProcessorJobInit {
	pub location: location::Data,
	pub sub_path: Option<PathBuf>,
	pub regenerate_thumbnails: bool,
}

impl Hash for MediaProcessorJobInit {
	fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
		self.location.id.hash(state);
		if let Some(ref sub_path) = self.sub_path {
			sub_path.hash(state);
		}
	}
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MediaProcessorJobData {
	location_path: PathBuf,
	to_process_path: PathBuf,
	#[serde(skip, default)]
	maybe_thumbnailer_progress_rx: Option<chan::Receiver<(u32, u32)>>,
}

#[derive(Debug, Serialize, Deserialize)]
pub enum MediaProcessorJobStep {
	ExtractMediaData(Vec<file_path_for_media_processor::Data>),
	WaitThumbnails(usize),
}

#[async_trait::async_trait]
impl StatefulJob for MediaProcessorJobInit {
	type Data = MediaProcessorJobData;
	type Step = MediaProcessorJobStep;
	type RunMetadata = MediaProcessorMetadata;

	const NAME: &'static str = "media_processor";
	const IS_BATCHED: bool = true;

	fn target_location(&self) -> location::id::Type {
		self.location.id
	}

	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError> {
		let Library { db, .. } = ctx.library.as_ref();

		let location_id = self.location.id;
		let location_path =
			maybe_missing(&self.location.path, "location.path").map(PathBuf::from)?;

		let (to_process_path, iso_file_path) = match &self.sub_path {
			Some(sub_path) if sub_path != Path::new("") => {
				let full_path = ensure_sub_path_is_in_location(&location_path, sub_path)
					.await
					.map_err(MediaProcessorError::from)?;
				ensure_sub_path_is_directory(&location_path, sub_path)
					.await
					.map_err(MediaProcessorError::from)?;

				let sub_iso_file_path =
					IsolatedFilePathData::new(location_id, &location_path, &full_path, true)
						.map_err(MediaProcessorError::from)?;

				ensure_file_path_exists(
					sub_path,
					&sub_iso_file_path,
					db,
					MediaProcessorError::SubPathNotFound,
				)
				.await?;

				(full_path, sub_iso_file_path)
			}
			_ => (
				location_path.to_path_buf(),
				IsolatedFilePathData::new(location_id, &location_path, &location_path, true)
					.map_err(MediaProcessorError::from)?,
			),
		};

		debug!(
			"Searching for media files in location {location_id} at directory \"{iso_file_path}\""
		);

		let thumbs_to_process_count = dispatch_thumbnails_for_processing(
			location_id,
			&location_path,
			&iso_file_path,
			&ctx.library,
			&ctx.node,
			false,
		)
		.await?;

		let maybe_thumbnailer_progress_rx = if thumbs_to_process_count > 0 {
			let (progress_tx, progress_rx) = chan::unbounded();

			ctx.node
				.thumbnailer
				.register_reporter(location_id, progress_tx)
				.await;

			Some(progress_rx)
		} else {
			None
		};

		let file_paths = get_files_for_media_data_extraction(db, &iso_file_path).await?;

		let total_files = file_paths.len();

		let chunked_files =
			file_paths
				.into_iter()
				.chunks(BATCH_SIZE)
				.into_iter()
				.map(|chunk| chunk.collect::<Vec<_>>())
				.map(MediaProcessorJobStep::ExtractMediaData)
				.chain(
					[(thumbs_to_process_count > 0).then_some(
						MediaProcessorJobStep::WaitThumbnails(thumbs_to_process_count as usize),
					)]
					.into_iter()
					.flatten(),
				)
				.collect::<Vec<_>>();

		ctx.progress(vec![
			JobReportUpdate::TaskCount(total_files),
			JobReportUpdate::Phase("media_data".to_string()),
			JobReportUpdate::Message(format!(
				"Preparing to process {total_files} files in {} chunks",
				chunked_files.len()
			)),
		]);

		*data = Some(MediaProcessorJobData {
			location_path,
			to_process_path,
			maybe_thumbnailer_progress_rx,
		});

		Ok((
			Self::RunMetadata {
				thumbs_processed: thumbs_to_process_count,
				..Default::default()
			},
			chunked_files,
		)
			.into())
	}

	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		CurrentStep { step, step_number }: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		_: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError> {
		match step {
			MediaProcessorJobStep::ExtractMediaData(file_paths) => process(
				file_paths,
				self.location.id,
				&data.location_path,
				&ctx.library.db,
				&|completed_count| {
					ctx.progress(vec![JobReportUpdate::CompletedTaskCount(
						step_number * BATCH_SIZE + completed_count,
					)]);
				},
			)
			.await
			.map(Into::into)
			.map_err(Into::into),
			MediaProcessorJobStep::WaitThumbnails(total_thumbs) => {
				ctx.progress(vec![
					JobReportUpdate::TaskCount(*total_thumbs),
					JobReportUpdate::Phase("thumbnails".to_string()),
					JobReportUpdate::Message(format!(
						"Waiting for processing of {total_thumbs} thumbnails",
					)),
				]);

				let mut progress_rx = pin!(if let Some(progress_rx) =
					data.maybe_thumbnailer_progress_rx.clone()
				{
					progress_rx
				} else {
					let (progress_tx, progress_rx) = chan::unbounded();

					ctx.node
						.thumbnailer
						.register_reporter(self.location.id, progress_tx)
						.await;

					progress_rx
				});

				let mut total_completed = 0;

				while let Some((completed, total)) = progress_rx.next().await {
					trace!("Received progress update from thumbnailer: {completed}/{total}",);
					ctx.progress(vec![JobReportUpdate::CompletedTaskCount(
						completed as usize,
					)]);
					total_completed = completed;
				}

				if progress_rx.is_closed() && total_completed < *total_thumbs as u32 {
					warn!(
						"Thumbnailer progress reporter channel closed before all thumbnails were \
						processed, job will wait a bit waiting for a shutdown signal from manager"
					);
					sleep(Duration::from_secs(5)).await;
				}

				Ok(None.into())
			}
		}
	}

	async fn finalize(
		&self,
		ctx: &WorkerContext,
		data: &Option<Self::Data>,
		run_metadata: &Self::RunMetadata,
	) -> JobResult {
		info!(
			"Finished media processing for location {} at {}",
			self.location.id,
			data.as_ref()
				.expect("critical error: missing data on job state")
				.to_process_path
				.display()
		);

		if run_metadata.media_data.extracted > 0 {
			invalidate_query!(ctx.library, "search.paths");
		}

		Ok(Some(json!({"init: ": self, "run_metadata": run_metadata})))
	}
}

async fn dispatch_thumbnails_for_processing(
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	parent_iso_file_path: &IsolatedFilePathData<'_>,
	library: &Library,
	node: &Node,
	should_regenerate: bool,
) -> Result<u32, MediaProcessorError> {
	let Library { db, .. } = library;

	let location_path = location_path.as_ref();

	let mut file_paths = get_all_children_files_by_extensions(
		db,
		parent_iso_file_path,
		&thumbnail::ALL_THUMBNAILABLE_EXTENSIONS,
	)
	.await?;

	if file_paths.is_empty() {
		return Ok(0);
	}

	let first_materialized_path = file_paths[0].materialized_path.clone();

	// Only the first materialized_path should be processed in foreground
	let different_materialized_path_idx = file_paths
		.iter()
		.position(|file_path| file_path.materialized_path != first_materialized_path);

	let background_thumbs_args = different_materialized_path_idx
		.map(|idx| {
			file_paths
				.split_off(idx)
				.into_iter()
				.filter_map(|file_path| prepare_args(location_id, location_path, file_path))
				.collect::<Vec<_>>()
		})
		.unwrap_or_default();

	let foreground_thumbs_args = file_paths
		.into_iter()
		.filter_map(|file_path| prepare_args(location_id, location_path, file_path))
		.collect::<Vec<_>>();

	let thumbs_count = background_thumbs_args.len() + foreground_thumbs_args.len();

	debug!(
		"Dispatching {thumbs_count} thumbnails to be processed, {} in foreground and {} in background",
		foreground_thumbs_args.len(),
		background_thumbs_args.len()
	);

	if !foreground_thumbs_args.is_empty() {
		node.thumbnailer
			.new_indexed_thumbnails_tracked_batch(
				BatchToProcess::new(foreground_thumbs_args, should_regenerate, false),
				library.id,
				location_id,
			)
			.await;
	}

	if !background_thumbs_args.is_empty() {
		node.thumbnailer
			.new_indexed_thumbnails_tracked_batch(
				BatchToProcess::new(background_thumbs_args, should_regenerate, true),
				library.id,
				location_id,
			)
			.await;
	}

	Ok(thumbs_count as u32)
}

async fn get_files_for_media_data_extraction(
	db: &PrismaClient,
	parent_iso_file_path: &IsolatedFilePathData<'_>,
) -> Result<Vec<file_path_for_media_processor::Data>, MediaProcessorError> {
	get_all_children_files_by_extensions(
		db,
		parent_iso_file_path,
		&media_data_extractor::FILTERED_IMAGE_EXTENSIONS,
	)
	.await
	.map_err(Into::into)
}

async fn get_all_children_files_by_extensions(
	db: &PrismaClient,
	parent_iso_file_path: &IsolatedFilePathData<'_>,
	extensions: &[Extension],
) -> Result<Vec<file_path_for_media_processor::Data>, MediaProcessorError> {
	// FIXME: Had to use format! macro because PCR doesn't support IN with Vec for SQLite
	// We have no data coming from the user, so this is sql injection safe
	db._query_raw(raw!(
		&format!(
			"SELECT id, materialized_path, is_dir, name, extension, cas_id, object_id
			FROM file_path
			WHERE
				location_id={{}}
				AND cas_id IS NOT NULL
				AND LOWER(extension) IN ({})
				AND materialized_path LIKE {{}}
			ORDER BY materialized_path ASC",
			// Orderind by materialized_path so we can prioritize processing the first files
			// in the above part of the directories tree
			extensions
				.iter()
				.map(|ext| format!("LOWER('{ext}')"))
				.collect::<Vec<_>>()
				.join(",")
		),
		PrismaValue::Int(parent_iso_file_path.location_id() as i64),
		PrismaValue::String(format!(
			"{}%",
			parent_iso_file_path
				.materialized_path_for_children()
				.expect("sub path iso_file_path must be a directory")
		))
	))
	.exec()
	.await
	.map_err(Into::into)
}

fn prepare_args(
	location_id: location::id::Type,
	location_path: &Path, // This function is only used internally once, so we can pass &Path as a parameter
	file_path: file_path_for_media_processor::Data,
) -> Option<GenerateThumbnailArgs> {
	let file_path_id = file_path.id;

	let Ok(cas_id) = maybe_missing(&file_path.cas_id, "file_path.cas_id").cloned() else {
		error!("Missing cas_id for file_path <id='{file_path_id}'>");
		return None;
	};

	let Ok(iso_file_path) = IsolatedFilePathData::try_from((location_id, file_path)).map_err(|e| {
		error!("Failed to extract isolated file path data from file path <id='{file_path_id}'>: {e:#?}");
	}) else {
		return None;
	};

	Some(GenerateThumbnailArgs::new(
		iso_file_path.extension().to_string(),
		cas_id,
		location_path.join(&iso_file_path),
	))
}



File: ./src/object/media/media_processor/shallow.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{JobError, JobRunMetadata},
	library::Library,
	location::file_path_helper::{
		ensure_file_path_exists, ensure_sub_path_is_directory, ensure_sub_path_is_in_location,
		file_path_for_media_processor, IsolatedFilePathData,
	},
	object::media::thumbnail::GenerateThumbnailArgs,
	prisma::{location, PrismaClient},
	util::db::maybe_missing,
	Node,
};

use std::path::{Path, PathBuf};

use itertools::Itertools;
use prisma_client_rust::{raw, PrismaValue};
use sd_file_ext::extensions::Extension;
use tracing::{debug, error};

use super::{
	media_data_extractor::{self, process},
	thumbnail::{self, BatchToProcess},
	MediaProcessorError, MediaProcessorMetadata,
};

const BATCH_SIZE: usize = 10;

pub async fn shallow(
	location: &location::Data,
	sub_path: &PathBuf,
	library: &Library,
	node: &Node,
) -> Result<(), JobError> {
	let Library { db, .. } = library;

	let location_id = location.id;
	let location_path = maybe_missing(&location.path, "location.path").map(PathBuf::from)?;

	let iso_file_path = if sub_path != Path::new("") {
		let full_path = ensure_sub_path_is_in_location(&location_path, &sub_path)
			.await
			.map_err(MediaProcessorError::from)?;
		ensure_sub_path_is_directory(&location_path, &sub_path)
			.await
			.map_err(MediaProcessorError::from)?;

		let sub_iso_file_path =
			IsolatedFilePathData::new(location_id, &location_path, &full_path, true)
				.map_err(MediaProcessorError::from)?;

		ensure_file_path_exists(
			&sub_path,
			&sub_iso_file_path,
			db,
			MediaProcessorError::SubPathNotFound,
		)
		.await?;

		sub_iso_file_path
	} else {
		IsolatedFilePathData::new(location_id, &location_path, &location_path, true)
			.map_err(MediaProcessorError::from)?
	};

	debug!("Searching for images in location {location_id} at path {iso_file_path}");

	dispatch_thumbnails_for_processing(
		location.id,
		&location_path,
		&iso_file_path,
		library,
		node,
		false,
	)
	.await?;

	let file_paths = get_files_for_media_data_extraction(db, &iso_file_path).await?;

	let total_files = file_paths.len();

	let chunked_files = file_paths
		.into_iter()
		.chunks(BATCH_SIZE)
		.into_iter()
		.map(Iterator::collect)
		.collect::<Vec<Vec<_>>>();

	debug!(
		"Preparing to process {total_files} files in {} chunks",
		chunked_files.len()
	);

	let mut run_metadata = MediaProcessorMetadata::default();

	for files in chunked_files {
		let (more_run_metadata, errors) = process(&files, location.id, &location_path, db, &|_| {})
			.await
			.map_err(MediaProcessorError::from)?;

		run_metadata.update(more_run_metadata.into());

		if !errors.is_empty() {
			error!("Errors processing chunk of media data shallow extraction:\n{errors}");
		}
	}

	debug!("Media shallow processor run metadata: {run_metadata:?}");

	if run_metadata.media_data.extracted > 0 {
		invalidate_query!(library, "search.paths");
		invalidate_query!(library, "search.objects");
	}

	Ok(())
}

async fn get_files_for_media_data_extraction(
	db: &PrismaClient,
	parent_iso_file_path: &IsolatedFilePathData<'_>,
) -> Result<Vec<file_path_for_media_processor::Data>, MediaProcessorError> {
	get_files_by_extensions(
		db,
		parent_iso_file_path,
		&media_data_extractor::FILTERED_IMAGE_EXTENSIONS,
	)
	.await
	.map_err(Into::into)
}

async fn dispatch_thumbnails_for_processing(
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	parent_iso_file_path: &IsolatedFilePathData<'_>,
	library: &Library,
	node: &Node,
	should_regenerate: bool,
) -> Result<(), MediaProcessorError> {
	let Library { db, .. } = library;

	let location_path = location_path.as_ref();

	let file_paths = get_files_by_extensions(
		db,
		parent_iso_file_path,
		&thumbnail::ALL_THUMBNAILABLE_EXTENSIONS,
	)
	.await?;

	let current_batch = file_paths
		.into_iter()
		.filter_map(|file_path| {
			if let Some(cas_id) = file_path.cas_id.as_ref() {
				Some((cas_id.clone(), file_path))
			} else {
				error!("File path <id='{}'> has no cas_id, skipping", file_path.id);
				None
			}
		})
		.filter_map(|(cas_id, file_path)| {
			let file_path_id = file_path.id;
			IsolatedFilePathData::try_from((location_id, file_path))
				.map_err(|e| {
					error!("Failed to extract isolated file path data from file path <id='{file_path_id}'>: {e:#?}");
				})
				.ok()
				.map(|iso_file_path| (cas_id, iso_file_path))
		})
		.map(|(cas_id, iso_file_path)| {
			let full_path = location_path.join(&iso_file_path);

			GenerateThumbnailArgs::new(iso_file_path.extension().to_string(), cas_id, full_path)
		})
		.collect::<Vec<_>>();

	// Let's not send an empty batch lol
	if !current_batch.is_empty() {
		node.thumbnailer
			.new_indexed_thumbnails_batch(
				BatchToProcess::new(current_batch, should_regenerate, false),
				library.id,
			)
			.await;
	}

	Ok(())
}

async fn get_files_by_extensions(
	db: &PrismaClient,
	parent_iso_file_path: &IsolatedFilePathData<'_>,
	extensions: &[Extension],
) -> Result<Vec<file_path_for_media_processor::Data>, MediaProcessorError> {
	// FIXME: Had to use format! macro because PCR doesn't support IN with Vec for SQLite
	// We have no data coming from the user, so this is sql injection safe
	db._query_raw(raw!(
		&format!(
			"SELECT id, materialized_path, is_dir, name, extension, cas_id, object_id
			FROM file_path
			WHERE
				location_id={{}}
				AND cas_id IS NOT NULL
				AND LOWER(extension) IN ({})
				AND materialized_path = {{}}",
			extensions
				.iter()
				.map(|ext| format!("LOWER('{ext}')"))
				.collect::<Vec<_>>()
				.join(",")
		),
		PrismaValue::Int(parent_iso_file_path.location_id() as i64),
		PrismaValue::String(
			parent_iso_file_path
				.materialized_path_for_children()
				.expect("sub path iso_file_path must be a directory")
		)
	))
	.exec()
	.await
	.map_err(Into::into)
}



File: ./src/object/media/media_processor/mod.rs
-------------------------------------------------
use crate::{
	job::{JobRunErrors, JobRunMetadata},
	location::file_path_helper::{file_path_for_media_processor, FilePathError},
};

use sd_prisma::prisma::{location, PrismaClient};

use std::path::Path;

use serde::{Deserialize, Serialize};
use thiserror::Error;
use tracing::error;

use super::{
	media_data_extractor::{self, MediaDataError, MediaDataExtractorMetadata},
	thumbnail::{self, BatchToProcess, ThumbnailerError},
};

mod job;
mod shallow;

pub use job::MediaProcessorJobInit;
pub use shallow::shallow;

#[derive(Error, Debug)]
pub enum MediaProcessorError {
	#[error("sub path not found: <path='{}'>", .0.display())]
	SubPathNotFound(Box<Path>),

	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error(transparent)]
	FilePath(#[from] FilePathError),

	#[error(transparent)]
	Thumbnailer(#[from] ThumbnailerError),
	#[error(transparent)]
	MediaDataExtractor(#[from] MediaDataError),
}

#[derive(Debug, Serialize, Deserialize, Default)]
pub struct MediaProcessorMetadata {
	media_data: MediaDataExtractorMetadata,
	thumbs_processed: u32,
}

impl From<MediaDataExtractorMetadata> for MediaProcessorMetadata {
	fn from(media_data: MediaDataExtractorMetadata) -> Self {
		Self {
			media_data,
			thumbs_processed: 0,
		}
	}
}

impl JobRunMetadata for MediaProcessorMetadata {
	fn update(&mut self, new_data: Self) {
		self.media_data.extracted += new_data.media_data.extracted;
		self.media_data.skipped += new_data.media_data.skipped;
		self.thumbs_processed += new_data.thumbs_processed;
	}
}

pub async fn process(
	files_paths: &[file_path_for_media_processor::Data],
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	db: &PrismaClient,
	ctx_update_fn: &impl Fn(usize),
) -> Result<(MediaProcessorMetadata, JobRunErrors), MediaProcessorError> {
	// Add here new kinds of media processing if necessary in the future

	media_data_extractor::process(files_paths, location_id, location_path, db, ctx_update_fn)
		.await
		.map(|(media_data, errors)| (media_data.into(), errors))
		.map_err(Into::into)
}



File: ./src/object/media/media_data_extractor.rs
-------------------------------------------------
use crate::{
	job::JobRunErrors,
	location::file_path_helper::{file_path_for_media_processor, IsolatedFilePathData},
	prisma::{location, media_data, PrismaClient},
	util::error::FileIOError,
};

use sd_file_ext::extensions::{Extension, ImageExtension, ALL_IMAGE_EXTENSIONS};
use sd_media_metadata::ImageMetadata;

use std::{collections::HashSet, path::Path};

use futures_concurrency::future::Join;
use once_cell::sync::Lazy;
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::task::spawn_blocking;
use tracing::error;

use super::media_data_image_to_query;

#[derive(Error, Debug)]
pub enum MediaDataError {
	// Internal errors
	#[error("database error: {0}")]
	Database(#[from] prisma_client_rust::QueryError),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error(transparent)]
	MediaData(#[from] sd_media_metadata::Error),
	#[error("failed to join tokio task: {0}")]
	TokioJoinHandle(#[from] tokio::task::JoinError),
}

#[derive(Serialize, Deserialize, Default, Debug)]
pub struct MediaDataExtractorMetadata {
	pub extracted: u32,
	pub skipped: u32,
}

pub(super) static FILTERED_IMAGE_EXTENSIONS: Lazy<Vec<Extension>> = Lazy::new(|| {
	ALL_IMAGE_EXTENSIONS
		.iter()
		.cloned()
		.filter(can_extract_media_data_for_image)
		.map(Extension::Image)
		.collect()
});

pub const fn can_extract_media_data_for_image(image_extension: &ImageExtension) -> bool {
	use ImageExtension::*;
	matches!(
		image_extension,
		Tiff | Dng | Jpeg | Jpg | Heif | Heifs | Heic | Avif | Avcs | Avci | Hif | Png | Webp
	)
}

pub async fn extract_media_data(path: impl AsRef<Path>) -> Result<ImageMetadata, MediaDataError> {
	let path = path.as_ref().to_path_buf();

	// Running in a separated blocking thread due to MediaData blocking behavior (due to sync exif lib)
	spawn_blocking(|| ImageMetadata::from_path(path))
		.await?
		.map_err(Into::into)
}

pub async fn process(
	files_paths: &[file_path_for_media_processor::Data],
	location_id: location::id::Type,
	location_path: impl AsRef<Path>,
	db: &PrismaClient,
	ctx_update_fn: &impl Fn(usize),
) -> Result<(MediaDataExtractorMetadata, JobRunErrors), MediaDataError> {
	let mut run_metadata = MediaDataExtractorMetadata::default();
	if files_paths.is_empty() {
		return Ok((run_metadata, JobRunErrors::default()));
	}

	let location_path = location_path.as_ref();

	let objects_already_with_media_data = db
		.media_data()
		.find_many(vec![media_data::object_id::in_vec(
			files_paths
				.iter()
				.filter_map(|file_path| file_path.object_id)
				.collect(),
		)])
		.select(media_data::select!({ object_id }))
		.exec()
		.await?;

	if files_paths.len() == objects_already_with_media_data.len() {
		// All files already have media data, skipping
		run_metadata.skipped = files_paths.len() as u32;
		return Ok((run_metadata, JobRunErrors::default()));
	}

	let objects_already_with_media_data = objects_already_with_media_data
		.into_iter()
		.map(|media_data| media_data.object_id)
		.collect::<HashSet<_>>();

	run_metadata.skipped = objects_already_with_media_data.len() as u32;

	let (media_datas, errors) = {
		let maybe_media_data = files_paths
			.iter()
			.enumerate()
			.filter_map(|(idx, file_path)| {
				file_path.object_id.and_then(|object_id| {
					(!objects_already_with_media_data.contains(&object_id))
						.then_some((idx, file_path, object_id))
				})
			})
			.filter_map(|(idx, file_path, object_id)| {
				IsolatedFilePathData::try_from((location_id, file_path))
					.map_err(|e| error!("{e:#?}"))
					.ok()
					.map(|iso_file_path| (idx, location_path.join(iso_file_path), object_id))
			})
			.map(|(idx, path, object_id)| async move {
				let res = extract_media_data(&path).await;
				ctx_update_fn(idx + 1);
				(res, path, object_id)
			})
			.collect::<Vec<_>>()
			.join()
			.await;

		let total_media_data = maybe_media_data.len();

		maybe_media_data.into_iter().fold(
			// In the good case, all media data were extracted
			(Vec::with_capacity(total_media_data), Vec::new()),
			|(mut media_datas, mut errors), (maybe_media_data, path, object_id)| {
				match maybe_media_data {
					Ok(media_data) => media_datas.push((media_data, object_id)),
					Err(MediaDataError::MediaData(sd_media_metadata::Error::NoExifDataOnPath(
						_,
					))) => {
						// No exif data on path, skipping
						run_metadata.skipped += 1;
					}
					Err(e) => errors.push((e, path)),
				}
				(media_datas, errors)
			},
		)
	};

	let created = db
		.media_data()
		.create_many(
			media_datas
				.into_iter()
				.filter_map(|(media_data, object_id)| {
					media_data_image_to_query(media_data, object_id)
						.map_err(|e| error!("{e:#?}"))
						.ok()
				})
				.collect(),
		)
		.skip_duplicates()
		.exec()
		.await?;

	run_metadata.extracted = created as u32;
	run_metadata.skipped += errors.len() as u32;

	Ok((
		run_metadata,
		errors
			.into_iter()
			.map(|(e, path)| format!("Couldn't process file: \"{}\"; Error: {e}", path.display()))
			.collect::<Vec<_>>()
			.into(),
	))
}



File: ./src/p2p/pairing/proto.rs
-------------------------------------------------
use std::str::FromStr;

use chrono::{DateTime, Utc};
use sd_p2p::{
	proto::{decode, encode},
	spacetunnel::RemoteIdentity,
};
use tokio::io::{AsyncRead, AsyncReadExt};
use uuid::Uuid;

use crate::node::Platform;

/// Terminology:
/// Instance - DB model which represents a single `.db` file.
/// Originator - begins the pairing process and is asking to join a library that will be selected by the responder.
/// Responder - is in-charge of accepting or rejecting the originator's request and then selecting which library to "share".

/// A modified version of `prisma::instance::Data` that uses proper validated types for the fields.
#[derive(Debug, PartialEq)]
pub struct Instance {
	pub id: Uuid,
	pub identity: RemoteIdentity,
	pub node_id: Uuid,
	pub node_name: String,
	pub node_platform: Platform,
	pub last_seen: DateTime<Utc>,
	pub date_created: DateTime<Utc>,
}

/// 1. Request for pairing to a library that is owned and will be selected by the responder.
/// Sent `Originator` -> `Responder`.
#[derive(Debug, PartialEq)]
pub struct PairingRequest(/* Originator's instance */ pub Instance);

/// 2. Decision for whether pairing was accepted or rejected once a library is decided on by the user.
/// Sent `Responder` -> `Originator`.
#[derive(Debug, PartialEq)]
pub enum PairingResponse {
	/// Pairing was accepted and the responder chose the library of their we are pairing to.
	Accepted {
		// Library information
		library_id: Uuid,
		library_name: String,
		library_description: Option<String>,

		// All instances in the library
		// Copying these means we are instantly paired with everyone else that is already in the library
		// NOTE: It's super important the `identity` field is converted from a private key to a public key before sending!!!
		instances: Vec<Instance>,
	},
	// Process will terminate as the user doesn't want to pair
	Rejected,
}

/// 3. Tell the responder that the database was correctly paired.
/// Sent `Originator` -> `Responder`.
#[derive(Debug, PartialEq)]
pub enum PairingConfirmation {
	Ok,
	Error,
}

impl Instance {
	pub async fn from_stream(
		stream: &mut (impl AsyncRead + Unpin),
	) -> Result<Self, (&'static str, decode::Error)> {
		Ok(Self {
			id: decode::uuid(stream).await.map_err(|e| ("id", e))?,
			identity: RemoteIdentity::from_bytes(
				&decode::buf(stream).await.map_err(|e| ("identity", e))?,
			)
			.unwrap(), // TODO: Error handling
			node_id: decode::uuid(stream).await.map_err(|e| ("node_id", e))?,
			node_name: decode::string(stream).await.map_err(|e| ("node_name", e))?,
			node_platform: stream
				.read_u8()
				.await
				.map(|b| Platform::try_from(b).unwrap_or(Platform::Unknown))
				.map_err(|e| ("node_platform", e.into()))?,
			last_seen: DateTime::<Utc>::from_str(
				&decode::string(stream).await.map_err(|e| ("last_seen", e))?,
			)
			.unwrap(), // TODO: Error handling
			date_created: DateTime::<Utc>::from_str(
				&decode::string(stream)
					.await
					.map_err(|e| ("date_created", e))?,
			)
			.unwrap(), // TODO: Error handling
		})
	}

	pub fn to_bytes(&self) -> Vec<u8> {
		let Self {
			id,
			identity,
			node_id,
			node_name,
			node_platform,
			last_seen,
			date_created,
		} = self;

		let mut buf = Vec::new();

		encode::uuid(&mut buf, id);
		encode::buf(&mut buf, &identity.get_bytes());
		encode::uuid(&mut buf, node_id);
		encode::string(&mut buf, node_name);
		buf.push(*node_platform as u8);
		encode::string(&mut buf, &last_seen.to_string());
		encode::string(&mut buf, &date_created.to_string());

		buf
	}
}

impl PairingRequest {
	pub async fn from_stream(
		stream: &mut (impl AsyncRead + Unpin),
	) -> Result<Self, (&'static str, decode::Error)> {
		Ok(Self(Instance::from_stream(stream).await?))
	}

	pub fn to_bytes(&self) -> Vec<u8> {
		let Self(instance) = self;
		Instance::to_bytes(instance)
	}
}

impl PairingResponse {
	pub async fn from_stream(
		stream: &mut (impl AsyncRead + Unpin),
	) -> Result<Self, (&'static str, decode::Error)> {
		// TODO: Error handling
		match stream.read_u8().await.unwrap() {
			0 => Ok(Self::Accepted {
				library_id: decode::uuid(stream).await.map_err(|e| ("library_id", e))?,
				library_name: decode::string(stream)
					.await
					.map_err(|e| ("library_name", e))?,
				library_description: match decode::string(stream)
					.await
					.map_err(|e| ("library_description", e))?
				{
					s if s.is_empty() => None,
					s => Some(s),
				},
				instances: {
					let len = stream.read_u16_le().await.unwrap();
					let mut instances = Vec::with_capacity(len as usize); // TODO: Prevent DOS

					for _ in 0..len {
						instances.push(Instance::from_stream(stream).await.unwrap());
					}

					instances
				},
			}),
			1 => Ok(Self::Rejected),
			_ => todo!(),
		}
	}

	pub fn to_bytes(&self) -> Vec<u8> {
		match self {
			Self::Accepted {
				library_id,
				library_name,
				library_description,
				instances,
			} => {
				let mut buf = vec![0];

				encode::uuid(&mut buf, library_id);
				encode::string(&mut buf, library_name);
				encode::string(&mut buf, library_description.as_deref().unwrap_or(""));
				buf.extend((instances.len() as u16).to_le_bytes());
				for instance in instances {
					buf.extend(instance.to_bytes());
				}

				buf
			}
			Self::Rejected => vec![1],
		}
	}
}

#[allow(unused)] // TODO: Remove this if still unused
impl PairingConfirmation {
	pub async fn from_stream(
		stream: &mut (impl AsyncRead + Unpin),
	) -> Result<Self, (&'static str, decode::Error)> {
		// TODO: Error handling
		match stream.read_u8().await.unwrap() {
			0 => Ok(Self::Ok),
			1 => Ok(Self::Error),
			_ => todo!(), // TODO: Error handling
		}
	}

	pub fn to_bytes(&self) -> Vec<u8> {
		match self {
			Self::Ok => vec![0],
			Self::Error => vec![1],
		}
	}
}

#[cfg(test)]
mod tests {
	use sd_p2p::spacetunnel::Identity;

	use super::*;

	#[tokio::test]
	async fn test_types() {
		let identity = Identity::new();
		let instance = || Instance {
			id: Uuid::new_v4(),
			identity: identity.to_remote_identity(),
			node_id: Uuid::new_v4(),
			node_name: "Node Name".into(),
			node_platform: Platform::current(),
			last_seen: Utc::now(),
			date_created: Utc::now(),
		};

		{
			let original = PairingRequest(instance());

			let mut cursor = std::io::Cursor::new(original.to_bytes());
			let result = PairingRequest::from_stream(&mut cursor).await.unwrap();
			assert_eq!(original, result);
		}

		{
			let original = PairingResponse::Accepted {
				library_id: Uuid::new_v4(),
				library_name: "Library Name".into(),
				library_description: Some("Library Description".into()),
				instances: vec![instance(), instance(), instance()],
			};

			let mut cursor = std::io::Cursor::new(original.to_bytes());
			let result = PairingResponse::from_stream(&mut cursor).await.unwrap();
			assert_eq!(original, result);
		}

		{
			let original = PairingResponse::Accepted {
				library_id: Uuid::new_v4(),
				library_name: "Library Name".into(),
				library_description: None,
				instances: vec![],
			};

			let mut cursor = std::io::Cursor::new(original.to_bytes());
			let result = PairingResponse::from_stream(&mut cursor).await.unwrap();
			assert_eq!(original, result);
		}

		{
			let original = PairingResponse::Rejected;

			let mut cursor = std::io::Cursor::new(original.to_bytes());
			let result = PairingResponse::from_stream(&mut cursor).await.unwrap();
			assert_eq!(original, result);
		}

		{
			let original = PairingConfirmation::Ok;

			let mut cursor = std::io::Cursor::new(original.to_bytes());
			let result = PairingConfirmation::from_stream(&mut cursor).await.unwrap();
			assert_eq!(original, result);
		}

		{
			let original = PairingConfirmation::Error;

			let mut cursor = std::io::Cursor::new(original.to_bytes());
			let result = PairingConfirmation::from_stream(&mut cursor).await.unwrap();
			assert_eq!(original, result);
		}
	}
}



File: ./src/p2p/pairing/mod.rs
-------------------------------------------------
#![allow(clippy::panic, clippy::unwrap_used)] // TODO: Finish this

use std::{
	collections::HashMap,
	sync::{
		atomic::{AtomicU16, Ordering},
		Arc, RwLock,
	},
};

use chrono::Utc;
use futures::channel::oneshot;
use sd_p2p::{
	spacetunnel::{Identity, RemoteIdentity},
	Manager,
};

use sd_prisma::prisma::instance;
use serde::{Deserialize, Serialize};
use specta::Type;
use tokio::{
	io::{AsyncRead, AsyncWrite, AsyncWriteExt},
	sync::broadcast,
};
use tracing::{info, warn};
use uuid::Uuid;

mod proto;

use proto::*;

use crate::{
	library::{Libraries, LibraryName},
	node::Platform,
	p2p::{Header, IdentityOrRemoteIdentity},
	Node,
};

use super::P2PEvent;

pub struct PairingManager {
	id: AtomicU16,
	events_tx: broadcast::Sender<P2PEvent>,
	pairing_response: RwLock<HashMap<u16, oneshot::Sender<PairingDecision>>>,
	manager: Arc<Manager>,
}

impl PairingManager {
	pub fn new(manager: Arc<Manager>, events_tx: broadcast::Sender<P2PEvent>) -> Arc<Self> {
		Arc::new(Self {
			id: AtomicU16::new(0),
			events_tx,
			pairing_response: RwLock::new(HashMap::new()),
			manager,
		})
	}

	fn emit_progress(&self, id: u16, status: PairingStatus) {
		self.events_tx
			.send(P2PEvent::PairingProgress { id, status })
			.ok();
	}

	pub fn decision(&self, id: u16, decision: PairingDecision) {
		if let Some(tx) = self.pairing_response.write().unwrap().remove(&id) {
			tx.send(decision).ok();
		}
	}

	// TODO: Error handling

	pub async fn originator(self: Arc<Self>, identity: RemoteIdentity, node: Arc<Node>) -> u16 {
		// TODO: Timeout for max number of pairings in a time period

		let pairing_id = self.id.fetch_add(1, Ordering::SeqCst);
		self.emit_progress(pairing_id, PairingStatus::EstablishingConnection);

		info!("Beginning pairing '{pairing_id}' as originator to remote peer '{identity}'");

		tokio::spawn(async move {
			let mut stream = self.manager.stream(identity).await.unwrap();
			stream.write_all(&Header::Pair.to_bytes()).await.unwrap();

			// TODO: Ensure both clients are on a compatible version cause Prisma model changes will cause issues

			// 1. Create new instance for originator and send it to the responder
			self.emit_progress(pairing_id, PairingStatus::PairingRequested);
			let node_config = node.config.get().await;
			let now = Utc::now();
			let identity = Identity::new();
			let self_instance_id = Uuid::new_v4();
			let req = PairingRequest(Instance {
				id: self_instance_id,
				identity: identity.to_remote_identity(),
				node_id: node_config.id,
				node_name: node_config.name.clone(),
				node_platform: Platform::current(),
				last_seen: now,
				date_created: now,
			});
			stream.write_all(&req.to_bytes()).await.unwrap();

			// 2.
			match PairingResponse::from_stream(&mut stream).await.unwrap() {
				PairingResponse::Accepted {
					library_id,
					library_name,
					library_description,
					instances,
				} => {
					info!("Pairing '{pairing_id}' accepted by remote into library '{library_id}'");
					// TODO: Log all instances and library info
					self.emit_progress(
						pairing_id,
						PairingStatus::PairingInProgress {
							library_name: library_name.clone(),
							library_description: library_description.clone(),
						},
					);

					// TODO: Future - Library in pairing state
					// TODO: Create library

					if node
						.libraries
						.get_all()
						.await
						.into_iter()
						.any(|i| i.id == library_id)
					{
						self.emit_progress(pairing_id, PairingStatus::LibraryAlreadyExists);

						// TODO: Properly handle this at a protocol level so the error is on both sides

						return;
					}

					let (this, instances): (Vec<_>, Vec<_>) = instances
						.into_iter()
						.partition(|i| i.id == self_instance_id);

					if this.len() != 1 {
						todo!("error handling");
					}
					let this = this.first().expect("unreachable");
					if this.identity != identity.to_remote_identity() {
						todo!("error handling. Something went really wrong!");
					}

					let library = node
						.libraries
						.create_with_uuid(
							library_id,
							LibraryName::new(library_name).unwrap(),
							library_description,
							false, // We will sync everything which will conflict with the seeded stuff
							Some(instance::Create {
								pub_id: this.id.as_bytes().to_vec(),
								identity: IdentityOrRemoteIdentity::Identity(identity).to_bytes(),
								node_id: this.node_id.as_bytes().to_vec(),
								node_name: this.node_name.clone(), // TODO: Remove `clone`
								node_platform: this.node_platform as i32,
								last_seen: this.last_seen.into(),
								date_created: this.date_created.into(),
								_params: vec![],
							}),
							&node,
						)
						.await
						.unwrap();

					let library = node.libraries.get_library(&library.id).await.unwrap();

					library
						.db
						.instance()
						.create_many(
							instances
								.into_iter()
								.map(|i| {
									instance::CreateUnchecked {
										pub_id: i.id.as_bytes().to_vec(),
										identity: IdentityOrRemoteIdentity::RemoteIdentity(
											i.identity,
										)
										.to_bytes(),
										node_id: i.node_id.as_bytes().to_vec(),
										node_name: i.node_name,
										node_platform: i.node_platform as i32,
										last_seen: i.last_seen.into(),
										date_created: i.date_created.into(),
										// timestamp: Default::default(), // TODO: Source this properly!
										_params: vec![],
									}
								})
								.collect(),
						)
						.exec()
						.await
						.unwrap();

					// Called again so the new instances are picked up
					node.libraries.update_instances(library.clone()).await;

					// TODO: Done message to frontend
					self.emit_progress(pairing_id, PairingStatus::PairingComplete(library_id));
					stream.flush().await.unwrap();

					// Remember, originator creates a new stream internally so the handler for this doesn't have to do anything.
					super::sync::originator(library_id, &library.sync, &node.p2p).await;
				}
				PairingResponse::Rejected => {
					info!("Pairing '{pairing_id}' rejected by remote");
					self.emit_progress(pairing_id, PairingStatus::PairingRejected);
				}
			}
		});

		pairing_id
	}

	pub async fn responder(
		self: Arc<Self>,
		identity: RemoteIdentity,
		mut stream: impl AsyncRead + AsyncWrite + Unpin,
		library_manager: &Libraries,
		node: Arc<Node>,
	) -> Result<(), ()> {
		let pairing_id = self.id.fetch_add(1, Ordering::SeqCst);
		self.emit_progress(pairing_id, PairingStatus::EstablishingConnection);

		info!("Beginning pairing '{pairing_id}' as responder to remote peer '{identity}'");

		let remote_instance = match PairingRequest::from_stream(&mut stream).await {
			Ok(v) => v,
			Err((field_name, err)) => {
				warn!("Error reading field '{field_name}' of pairing request from remote: {err}");
				self.emit_progress(pairing_id, PairingStatus::PairingRejected);

				// TODO: Attempt to send error to remote and reset connection
				return Ok(());
			}
		}
		.0;
		self.emit_progress(pairing_id, PairingStatus::PairingDecisionRequest);
		self.events_tx
			.send(P2PEvent::PairingRequest {
				id: pairing_id,
				name: remote_instance.node_name.clone(),
				os: remote_instance.node_platform.into(),
			})
			.ok();

		// Prompt the user and wait
		// TODO: After 1 minute remove channel from map and assume it was rejected
		let (tx, rx) = oneshot::channel();
		self.pairing_response
			.write()
			.unwrap()
			.insert(pairing_id, tx);
		let PairingDecision::Accept(library_id) = rx.await.unwrap() else {
			info!("The user rejected pairing '{pairing_id}'!");
			// self.emit_progress(pairing_id, PairingStatus::PairingRejected); // TODO: Event to remove from frontend index
			stream
				.write_all(&PairingResponse::Rejected.to_bytes())
				.await
				.unwrap();
			return Ok(());
		};
		info!("The user accepted pairing '{pairing_id}' for library '{library_id}'!");

		let library = library_manager.get_library(&library_id).await.unwrap();

		// TODO: Rollback this on pairing failure
		instance::Create {
			pub_id: remote_instance.id.as_bytes().to_vec(),
			identity: IdentityOrRemoteIdentity::RemoteIdentity(remote_instance.identity).to_bytes(),
			node_id: remote_instance.node_id.as_bytes().to_vec(),
			node_name: remote_instance.node_name,
			node_platform: remote_instance.node_platform as i32,
			last_seen: remote_instance.last_seen.into(),
			date_created: remote_instance.date_created.into(),
			// timestamp: Default::default(), // TODO: Source this properly!
			_params: vec![],
		}
		.to_query(&library.db)
		.exec()
		.await
		.unwrap();
		library_manager.update_instances(library.clone()).await;

		let library_config = library.config().await;

		stream
			.write_all(
				&PairingResponse::Accepted {
					library_id: library.id,
					library_name: library_config.name.into(),
					library_description: library_config.description,
					instances: library
						.db
						.instance()
						.find_many(vec![])
						.exec()
						.await
						.unwrap()
						.into_iter()
						.filter_map(|i| {
							let Ok(id) = Uuid::from_slice(&i.pub_id) else {
								warn!("Invalid instance pub_id in database: {:?}", i.pub_id);
								return None;
							};

							let Ok(node_id) = Uuid::from_slice(&i.node_id) else {
								warn!("Invalid instance node_id in database: {:?}", i.node_id);
								return None;
							};

							Some(Instance {
								id,
								identity: IdentityOrRemoteIdentity::from_bytes(&i.identity)
									.unwrap()
									.remote_identity(),
								node_id,
								node_name: i.node_name,
								node_platform: Platform::try_from(i.node_platform as u8)
									.unwrap_or(Platform::Unknown),
								last_seen: i.last_seen.into(),
								date_created: i.date_created.into(),
							})
						})
						.collect(),
				}
				.to_bytes(),
			)
			.await
			.unwrap();

		// TODO: Pairing confirmation + rollback

		self.emit_progress(pairing_id, PairingStatus::PairingComplete(library_id));
		stream.flush().await.unwrap();

		// Remember, originator creates a new stream internally so the handler for this doesn't have to do anything.
		super::sync::originator(library_id, &library.sync, &node.p2p).await;

		Ok(())
	}
}

#[derive(Debug, Type, Serialize, Deserialize)]
#[serde(tag = "decision", content = "libraryId", rename_all = "camelCase")]
pub enum PairingDecision {
	Accept(Uuid),
	Reject,
}

#[derive(Debug, Hash, Clone, Serialize, Type)]
#[serde(tag = "type", content = "data")]
pub enum PairingStatus {
	EstablishingConnection,
	PairingRequested,
	LibraryAlreadyExists,
	PairingDecisionRequest,
	PairingInProgress {
		library_name: String,
		library_description: Option<String>,
	},
	InitialSyncProgress(u8),
	PairingComplete(Uuid),
	PairingRejected,
}

// TODO: Unit tests



File: ./src/p2p/libraries.rs
-------------------------------------------------
use std::{
	collections::HashMap,
	fmt,
	sync::{Arc, PoisonError, RwLock},
};

use sd_p2p::Service;
use tokio::sync::mpsc;
use tracing::{error, warn};
use uuid::Uuid;

use crate::library::{Libraries, Library, LibraryManagerEvent};

use super::{IdentityOrRemoteIdentity, LibraryMetadata, P2PManager};

pub struct LibraryServices {
	services: RwLock<HashMap<Uuid, Arc<Service<LibraryMetadata>>>>,
	register_service_tx: mpsc::Sender<Arc<Service<LibraryMetadata>>>,
}

impl fmt::Debug for LibraryServices {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		f.debug_struct("LibraryServices")
			.field(
				"services",
				&self
					.services
					.read()
					.unwrap_or_else(PoisonError::into_inner)
					.keys(),
			)
			.finish()
	}
}

impl LibraryServices {
	pub fn new(register_service_tx: mpsc::Sender<Arc<Service<LibraryMetadata>>>) -> Self {
		Self {
			services: Default::default(),
			register_service_tx,
		}
	}

	pub(crate) async fn start(manager: Arc<P2PManager>, libraries: Arc<Libraries>) {
		if let Err(err) = libraries
			.rx
			.clone()
			.subscribe(|msg| {
				let manager = manager.clone();
				async move {
					match msg {
						LibraryManagerEvent::InstancesModified(library)
						| LibraryManagerEvent::Load(library) => {
							manager
								.clone()
								.libraries
								.load_library(manager, &library)
								.await
						}
						LibraryManagerEvent::Edit(library) => {
							manager.libraries.edit_library(&library).await
						}
						LibraryManagerEvent::Delete(library) => {
							manager.libraries.delete_library(&library).await
						}
					}
				}
			})
			.await
		{
			error!("Core may become unstable! `LibraryServices::start` manager aborted with error: {err:?}");
		}
	}

	pub fn get(&self, id: &Uuid) -> Option<Arc<Service<LibraryMetadata>>> {
		self.services
			.read()
			.unwrap_or_else(PoisonError::into_inner)
			.get(id)
			.cloned()
	}

	pub fn libraries(&self) -> Vec<(Uuid, Arc<Service<LibraryMetadata>>)> {
		self.services
			.read()
			.unwrap_or_else(PoisonError::into_inner)
			.iter()
			.map(|(k, v)| (*k, v.clone()))
			.collect::<Vec<_>>()
	}

	pub(crate) async fn load_library(&self, manager: Arc<P2PManager>, library: &Library) {
		let identities = match library.db.instance().find_many(vec![]).exec().await {
			Ok(library) => library
				.into_iter()
				.filter_map(
					// TODO: Error handling
					|i| match IdentityOrRemoteIdentity::from_bytes(&i.identity) {
						Err(err) => {
							warn!("error parsing identity: {err:?}");
							None
						}
						Ok(IdentityOrRemoteIdentity::Identity(_)) => None,
						Ok(IdentityOrRemoteIdentity::RemoteIdentity(identity)) => Some(identity),
					},
				)
				.collect(),
			Err(err) => {
				warn!("error loading library '{}': {err:?}", library.id);
				return;
			}
		};

		let mut inserted = false;

		let service = {
			let mut service = self
				.services
				.write()
				.unwrap_or_else(PoisonError::into_inner);
			let service = service.entry(library.id).or_insert_with(|| {
				inserted = true;
				Arc::new(
					Service::new(library.id.to_string(), manager.manager.clone())
						.expect("error creating service with duplicate service name"),
				)
			});
			service.add_known(identities);
			service.clone()
		};

		if inserted {
			service.update(LibraryMetadata {});
			if self.register_service_tx.send(service).await.is_err() {
				warn!("error sending on 'register_service_tx'. This indicates a bug!");
			}
		}
	}

	pub(crate) async fn edit_library(&self, _library: &Library) {
		// TODO: Send changes to all connected nodes!
		// TODO: Update mdns
	}

	pub(crate) async fn delete_library(&self, library: &Library) {
		drop(
			self.services
				.write()
				.unwrap_or_else(PoisonError::into_inner)
				.remove(&library.id),
		);
	}
}



File: ./src/p2p/protocol.rs
-------------------------------------------------
use thiserror::Error;
use tokio::io::{AsyncRead, AsyncReadExt};
use uuid::Uuid;

use sd_p2p::{
	proto::{decode, encode},
	spaceblock::{Range, SpaceblockRequests, SpaceblockRequestsError},
};

#[derive(Debug, PartialEq, Eq)]
pub struct HeaderFile {
	// Request ID
	pub(crate) id: Uuid,
	pub(crate) library_id: Uuid,
	pub(crate) file_path_id: Uuid,
	pub(crate) range: Range,
}

/// TODO
#[derive(Debug, PartialEq, Eq)]
pub enum Header {
	// TODO: Split out cause this is a broadcast
	Ping,
	Spacedrop(SpaceblockRequests),
	Pair,
	Sync(Uuid),
	File(HeaderFile),
}

#[derive(Debug, Error)]
pub enum HeaderError {
	#[error("io error reading discriminator: {0}")]
	DiscriminatorIo(std::io::Error),
	#[error("invalid discriminator '{0}'")]
	DiscriminatorInvalid(u8),
	#[error("error reading spacedrop request: {0}")]
	SpacedropRequest(#[from] SpaceblockRequestsError),
	#[error("error reading sync request: {0}")]
	SyncRequest(decode::Error),
	#[error("error reading header file: {0}")]
	HeaderFile(decode::Error),
	#[error("error invalid header file discriminator '{0}'")]
	HeaderFileDiscriminatorInvalid(u8),
}

impl Header {
	pub async fn from_stream(stream: &mut (impl AsyncRead + Unpin)) -> Result<Self, HeaderError> {
		let discriminator = stream
			.read_u8()
			.await
			.map_err(HeaderError::DiscriminatorIo)?;

		match discriminator {
			0 => Ok(Self::Spacedrop(
				SpaceblockRequests::from_stream(stream).await?,
			)),
			1 => Ok(Self::Ping),
			2 => Ok(Self::Pair),
			3 => Ok(Self::Sync(
				decode::uuid(stream)
					.await
					.map_err(HeaderError::SyncRequest)?,
			)),
			4 => Ok(Self::File(HeaderFile {
				id: decode::uuid(stream)
					.await
					.map_err(HeaderError::HeaderFile)?,
				library_id: decode::uuid(stream)
					.await
					.map_err(HeaderError::HeaderFile)?,
				file_path_id: decode::uuid(stream)
					.await
					.map_err(HeaderError::HeaderFile)?,
				range: match stream
					.read_u8()
					.await
					.map_err(|err| HeaderError::HeaderFile(err.into()))?
				{
					0 => Range::Full,
					1 => {
						let start = stream
							.read_u64_le()
							.await
							.map_err(|err| HeaderError::HeaderFile(err.into()))?;
						let end = stream
							.read_u64_le()
							.await
							.map_err(|err| HeaderError::HeaderFile(err.into()))?;
						Range::Partial(start..end)
					}
					i => return Err(HeaderError::HeaderFileDiscriminatorInvalid(i)),
				},
			})),
			d => Err(HeaderError::DiscriminatorInvalid(d)),
		}
	}

	pub fn to_bytes(&self) -> Vec<u8> {
		match self {
			Self::Spacedrop(transfer_request) => {
				let mut bytes = vec![0];
				bytes.extend_from_slice(&transfer_request.to_bytes());
				bytes
			}
			Self::Ping => vec![1],
			Self::Pair => vec![2],
			Self::Sync(uuid) => {
				let mut bytes = vec![3];
				encode::uuid(&mut bytes, uuid);
				bytes
			}
			Self::File(HeaderFile {
				id,
				library_id,
				file_path_id,
				range,
			}) => {
				let mut buf = vec![4];
				encode::uuid(&mut buf, id);
				encode::uuid(&mut buf, library_id);
				encode::uuid(&mut buf, file_path_id);
				buf.extend_from_slice(&range.to_bytes());
				buf
			}
		}
	}
}

#[cfg(test)]
mod tests {
	// use super::*;

	#[test]
	fn test_header() {
		// TODO: Finish this

		// 	assert_eq!(
		// 		Header::from_bytes(&Header::Ping.to_bytes()),
		// 		Ok(Header::Ping)
		// 	);

		// 	assert_eq!(
		// 		Header::from_bytes(&Header::Spacedrop.to_bytes()),
		// 		Ok(Header::Spacedrop)
		// 	);

		// 	let uuid = Uuid::new_v4();
		// 	assert_eq!(
		// 		Header::from_bytes(&Header::Sync(uuid).to_bytes()),
		// 		Ok(Header::Sync(uuid))
		// 	);
	}
}



File: ./src/p2p/p2p_events.rs
-------------------------------------------------
use sd_p2p::spacetunnel::RemoteIdentity;
use serde::Serialize;
use specta::Type;
use uuid::Uuid;

use super::{OperatingSystem, PairingStatus, PeerMetadata};

/// TODO: P2P event for the frontend
#[derive(Debug, Clone, Serialize, Type)]
#[serde(tag = "type")]
pub enum P2PEvent {
	DiscoveredPeer {
		identity: RemoteIdentity,
		metadata: PeerMetadata,
	},
	ExpiredPeer {
		identity: RemoteIdentity,
	},
	ConnectedPeer {
		identity: RemoteIdentity,
	},
	DisconnectedPeer {
		identity: RemoteIdentity,
	},
	SpacedropRequest {
		id: Uuid,
		identity: RemoteIdentity,
		peer_name: String,
		files: Vec<String>,
	},
	SpacedropProgress {
		id: Uuid,
		percent: u8,
	},
	SpacedropTimedout {
		id: Uuid,
	},
	SpacedropRejected {
		id: Uuid,
	},
	// Pairing was reuqest has come in.
	// This will fire on the responder only.
	PairingRequest {
		id: u16,
		name: String,
		os: OperatingSystem,
	},
	PairingProgress {
		id: u16,
		status: PairingStatus,
	}, // TODO: Expire peer + connection/disconnect
}



File: ./src/p2p/p2p_manager.rs
-------------------------------------------------
use std::{
	collections::{HashMap, HashSet},
	net::SocketAddr,
	sync::{atomic::AtomicBool, Arc},
};

use sd_p2p::{
	spacetunnel::RemoteIdentity, Manager, ManagerConfig, ManagerError, PeerStatus, Service,
};
use serde::Serialize;
use specta::Type;
use tokio::sync::{broadcast, mpsc, oneshot, Mutex};
use tracing::info;
use uuid::Uuid;

use crate::{
	node::config,
	p2p::{OperatingSystem, SPACEDRIVE_APP_ID},
};

use super::{
	LibraryMetadata, LibraryServices, P2PEvent, P2PManagerActor, PairingManager, PeerMetadata,
};

pub struct P2PManager {
	pub(crate) node: Service<PeerMetadata>,
	pub(crate) libraries: LibraryServices,

	pub events: (broadcast::Sender<P2PEvent>, broadcast::Receiver<P2PEvent>),
	pub manager: Arc<Manager>,
	pub(super) spacedrop_pairing_reqs: Arc<Mutex<HashMap<Uuid, oneshot::Sender<Option<String>>>>>,
	pub(super) spacedrop_cancelations: Arc<Mutex<HashMap<Uuid, Arc<AtomicBool>>>>,
	pub pairing: Arc<PairingManager>,
	node_config_manager: Arc<config::Manager>,
}

impl P2PManager {
	pub async fn new(
		node_config: Arc<config::Manager>,
		libraries: Arc<crate::library::Libraries>,
	) -> Result<(Arc<P2PManager>, P2PManagerActor), ManagerError> {
		let (keypair, manager_config) = {
			let config = node_config.get().await;
			(config.keypair, config.p2p.clone())
		};

		let (manager, stream) =
			sd_p2p::Manager::new(SPACEDRIVE_APP_ID, &keypair, manager_config).await?;

		info!(
			"Node RemoteIdentity('{}') libp2p::PeerId('{}') is now online listening at addresses: {:?}",
			manager.identity(),
			manager.libp2p_peer_id(),
			stream.listen_addrs()
		);

		// need to keep 'rx' around so that the channel isn't dropped
		let (tx, rx) = broadcast::channel(100);
		let pairing = PairingManager::new(manager.clone(), tx.clone());

		let (register_service_tx, register_service_rx) = mpsc::channel(10);
		let this = Arc::new(Self {
			node: Service::new("node", manager.clone())
				.expect("Hardcoded service name will never be a duplicate!"),
			libraries: LibraryServices::new(register_service_tx),
			pairing,
			events: (tx, rx),
			manager,
			spacedrop_pairing_reqs: Default::default(),
			spacedrop_cancelations: Default::default(),
			node_config_manager: node_config,
		});
		this.update_metadata().await;

		tokio::spawn(LibraryServices::start(this.clone(), libraries));

		Ok((
			this.clone(),
			P2PManagerActor {
				manager: this,
				stream,
				register_service_rx,
			},
		))
	}

	pub fn get_library_service(&self, library_id: &Uuid) -> Option<Arc<Service<LibraryMetadata>>> {
		self.libraries.get(library_id)
	}

	pub async fn update_metadata(&self) {
		self.node.update({
			let config = self.node_config_manager.get().await;
			PeerMetadata {
				name: config.name.clone(),
				operating_system: Some(OperatingSystem::get_os()),
				version: Some(env!("CARGO_PKG_VERSION").to_string()),
			}
		});
	}

	pub fn subscribe(&self) -> broadcast::Receiver<P2PEvent> {
		self.events.0.subscribe()
	}

	// TODO: Replace this with a better system that is more built into `sd-p2p` crate
	pub fn state(&self) -> P2PState {
		let (
			self_peer_id,
			self_identity,
			config,
			manager_connected,
			manager_connections,
			dicovery_services,
			discovery_discovered,
			discovery_known,
		) = self.manager.get_debug_state();

		P2PState {
			node: self.node.get_state(),
			libraries: self
				.libraries
				.libraries()
				.into_iter()
				.map(|(id, lib)| (id, lib.get_state()))
				.collect(),
			self_peer_id: PeerId(self_peer_id),
			self_identity,
			config,
			manager_connected: manager_connected
				.into_iter()
				.map(|(k, v)| (PeerId(k), v))
				.collect(),
			manager_connections: manager_connections.into_iter().map(PeerId).collect(),
			dicovery_services,
			discovery_discovered: discovery_discovered
				.into_iter()
				.map(|(k, v)| {
					(
						k,
						v.into_iter()
							.map(|(k, (k1, v, b))| (k, (PeerId(k1), v, b)))
							.collect(),
					)
				})
				.collect(),
			discovery_known,
		}
	}

	pub async fn shutdown(&self) {
		self.manager.shutdown().await;
	}
}

#[derive(Debug, Serialize, Type)]
#[allow(clippy::type_complexity)]
pub struct P2PState {
	node: HashMap<RemoteIdentity, PeerStatus>,
	libraries: Vec<(Uuid, HashMap<RemoteIdentity, PeerStatus>)>,
	self_peer_id: PeerId,
	self_identity: RemoteIdentity,
	config: ManagerConfig,
	manager_connected: HashMap<PeerId, RemoteIdentity>,
	manager_connections: HashSet<PeerId>,
	dicovery_services: HashMap<String, Option<HashMap<String, String>>>,
	discovery_discovered: HashMap<
		String,
		HashMap<RemoteIdentity, (PeerId, HashMap<String, String>, Vec<SocketAddr>)>,
	>,
	discovery_known: HashMap<String, HashSet<RemoteIdentity>>,
}

// TODO: Get this back into `sd-p2p` but keep it private
#[derive(Debug, Serialize, Type, Hash, Eq, PartialEq, Ord, PartialOrd, Clone)]
pub struct PeerId(#[specta(type = String)] sd_p2p::internal::PeerId);



File: ./src/p2p/operations/mod.rs
-------------------------------------------------
pub mod ping;
pub mod request_file;
pub mod spacedrop;

pub use request_file::request_file;
pub use spacedrop::spacedrop;



File: ./src/p2p/operations/ping.rs
-------------------------------------------------
use std::sync::Arc;

use sd_p2p::PeerMessageEvent;
use tracing::debug;

use crate::p2p::P2PManager;

/// Send a ping to all peers we are connected to
#[allow(unused)]
pub async fn ping(_p2p: Arc<P2PManager>) {
	todo!();
}

pub(crate) async fn reciever(event: PeerMessageEvent) {
	debug!("Received ping from peer '{}'", event.identity);
}



File: ./src/p2p/operations/spacedrop.rs
-------------------------------------------------
use std::{
	borrow::Cow,
	path::PathBuf,
	sync::{
		atomic::{AtomicBool, Ordering},
		Arc,
	},
	time::Duration,
};

use futures::future::join_all;
use sd_p2p::{
	spaceblock::{BlockSize, Range, SpaceblockRequest, SpaceblockRequests, Transfer},
	spacetunnel::RemoteIdentity,
	PeerMessageEvent,
};
use tokio::{
	fs::{create_dir_all, File},
	io::{AsyncReadExt, AsyncWriteExt, BufReader, BufWriter},
	sync::oneshot,
	time::{sleep, Instant},
};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::p2p::{Header, P2PEvent, P2PManager};

/// The amount of time to wait for a Spacedrop request to be accepted or rejected before it's automatically rejected
pub(crate) const SPACEDROP_TIMEOUT: Duration = Duration::from_secs(60);

// TODO: Proper error handling
pub async fn spacedrop(
	p2p: Arc<P2PManager>,
	// TODO: Stop using `PeerId`
	identity: RemoteIdentity,
	paths: Vec<PathBuf>,
) -> Result<Uuid, ()> {
	if paths.is_empty() {
		return Err(());
	}

	let (files, requests): (Vec<_>, Vec<_>) = join_all(paths.into_iter().map(|path| async move {
		let file = File::open(&path).await?;
		let metadata = file.metadata().await?;
		let name = path
			.file_name()
			.map(|v| v.to_string_lossy())
			.unwrap_or(Cow::Borrowed(""))
			.to_string();

		Ok((
			(path, file),
			SpaceblockRequest {
				name,
				size: metadata.len(),
				range: Range::Full,
			},
		))
	}))
	.await
	.into_iter()
	.collect::<Result<Vec<_>, std::io::Error>>()
	.map_err(|_| ())? // TODO: Error handling
	.into_iter()
	.unzip();

	let total_length: u64 = requests.iter().map(|req| req.size).sum();

	let id = Uuid::new_v4();
	debug!("({id}): starting Spacedrop with peer '{identity}");
	let mut stream = p2p.manager.stream(identity).await.map_err(|err| {
		debug!("({id}): failed to connect: {err:?}");
		// TODO: Proper error
	})?;

	tokio::spawn(async move {
		debug!("({id}): connected, sending header");
		let header = Header::Spacedrop(SpaceblockRequests {
			id,
			block_size: BlockSize::from_size(total_length),
			requests,
		});
		if let Err(err) = stream.write_all(&header.to_bytes()).await {
			debug!("({id}): failed to send header: {err}");
			return;
		}
		let Header::Spacedrop(requests) = header else {
			unreachable!();
		};

		debug!("({id}): waiting for response");
		let result = tokio::select! {
		  result = stream.read_u8() => result,
		  // Add 5 seconds incase the user responded on the deadline and slow network
		   _ = sleep(SPACEDROP_TIMEOUT + Duration::from_secs(5)) => {
				debug!("({id}): timed out, cancelling");
				p2p.events.0.send(P2PEvent::SpacedropTimedout { id }).ok();
				return;
			},
		};

		match result {
			Ok(0) => {
				debug!("({id}): Spacedrop was rejected from peer '{identity}'");
				p2p.events.0.send(P2PEvent::SpacedropRejected { id }).ok();
				return;
			}
			Ok(1) => {}        // Okay
			Ok(_) => todo!(),  // TODO: Proper error
			Err(_) => todo!(), // TODO: Proper error
		}

		let cancelled = Arc::new(AtomicBool::new(false));
		p2p.spacedrop_cancelations
			.lock()
			.await
			.insert(id, cancelled.clone());

		debug!("({id}): starting transfer");
		let i = Instant::now();

		let mut transfer = Transfer::new(
			&requests,
			|percent| {
				p2p.events
					.0
					.send(P2PEvent::SpacedropProgress { id, percent })
					.ok();
			},
			&cancelled,
		);

		for (file_id, (path, file)) in files.into_iter().enumerate() {
			debug!("({id}): transmitting '{file_id}' from '{path:?}'");
			let file = BufReader::new(file);
			if let Err(err) = transfer.send(&mut stream, file).await {
				debug!("({id}): failed to send file '{file_id}': {err}");
				// TODO: Error to frontend
				// p2p.events
				// 	.0
				// 	.send(P2PEvent::SpacedropFailed { id, file_id })
				// 	.ok();
				return;
			}
		}

		debug!("({id}): finished; took '{:?}", i.elapsed());
	});

	Ok(id)
}

// TODO: Move these off the manager
impl P2PManager {
	pub async fn accept_spacedrop(&self, id: Uuid, path: String) {
		if let Some(chan) = self.spacedrop_pairing_reqs.lock().await.remove(&id) {
			chan.send(Some(path))
				.map_err(|err| {
					warn!("error accepting Spacedrop '{id:?}': '{err:?}'");
				})
				.ok();
		}
	}

	pub async fn reject_spacedrop(&self, id: Uuid) {
		if let Some(chan) = self.spacedrop_pairing_reqs.lock().await.remove(&id) {
			chan.send(None)
				.map_err(|err| {
					warn!("error rejecting Spacedrop '{id:?}': '{err:?}'");
				})
				.ok();
		}
	}

	pub async fn cancel_spacedrop(&self, id: Uuid) {
		if let Some(cancelled) = self.spacedrop_cancelations.lock().await.remove(&id) {
			cancelled.store(true, Ordering::Relaxed);
		}
	}
}

pub(crate) async fn reciever(
	this: &Arc<P2PManager>,
	req: SpaceblockRequests,
	event: PeerMessageEvent,
) -> Result<(), ()> {
	let id = req.id;
	let mut stream = event.stream;
	let (tx, rx) = oneshot::channel();

	info!(
		"({id}): received '{}' files from peer '{}' with block size '{:?}'",
		req.requests.len(),
		event.identity,
		req.block_size
	);
	this.spacedrop_pairing_reqs.lock().await.insert(id, tx);

	if this
		.events
		.0
		.send(P2PEvent::SpacedropRequest {
			id,
			identity: event.identity,
			peer_name: "Unknown".into(),
			// TODO: A better solution to this
			// manager
			// 	.get_discovered_peers()
			// 	.await
			// 	.into_iter()
			// 	.find(|p| p.peer_id == event.peer_id)
			// 	.map(|p| p.metadata.name)
			// 	.unwrap_or_else(|| "Unknown".to_string()),
			files: req
				.requests
				.iter()
				.map(|req| req.name.clone())
				.collect::<Vec<_>>(),
		})
		.is_err()
	{
		// No frontend's are active

		// TODO: Implement this
		error!("TODO: Outright reject Spacedrop");
	}

	tokio::select! {
		_ = sleep(SPACEDROP_TIMEOUT) => {
			info!("({id}): timeout, rejecting!");

			stream.write_all(&[0]).await.map_err(|err| {
				error!("({id}): error reject bit: '{err:?}'");
			})?;
			stream.flush().await.map_err(|err| {
				error!("({id}): error flushing reject bit: '{err:?}'");
			})?;
		}
		file_path = rx => {
			match file_path {
				Ok(Some(file_path)) => {
					info!("({id}): accepted saving to '{:?}'", file_path);

					let cancelled = Arc::new(AtomicBool::new(false));
					this.spacedrop_cancelations
						.lock()
						.await
						.insert(id, cancelled.clone());

					stream.write_all(&[1]).await.map_err(|err| {
						error!("({id}): error sending continuation bit: '{err:?}'");

						// TODO: Send error to the frontend

						// TODO: make sure the other peer times out or we retry???
					})?;

					let names = req.requests.iter().map(|req| req.name.clone()).collect::<Vec<_>>();
					let mut transfer = Transfer::new(&req, |percent| {
						this.events.0.send(P2PEvent::SpacedropProgress { id, percent }).ok();
					}, &cancelled);

					let file_path = PathBuf::from(file_path);
					let names_len = names.len();
					for file_name in names {
						 // When transferring more than 1 file we wanna join the incoming file name to the directory provided by the user
						 let mut path = file_path.clone();
						 if names_len != 1 {
							// We know the `file_path` will be a directory so we can just push the file name to it
							path.push(&file_name);
						}

						debug!("({id}): accepting '{file_name}' and saving to '{:?}'", path);

						if let Some(parent) = path.parent() {
						  create_dir_all(&parent).await.map_err(|err| {
								error!("({id}): error creating parent directory '{parent:?}': '{err:?}'");

								// TODO: Send error to the frontend

								// TODO: Send error to remote peer
							})?;
						}

						let f = File::create(&path).await.map_err(|err| {
							error!("({id}): error creating file at '{path:?}': '{err:?}'");

							// TODO: Send error to the frontend

							// TODO: Send error to remote peer
						})?;
						let f = BufWriter::new(f);
						if let Err(err) = transfer.receive(&mut stream, f).await {
							error!("({id}): error receiving file '{file_name}': '{err:?}'");

							// TODO: Send error to frontend

							break;
						}
					}

					info!("({id}): complete");
				}
				Ok(None) => {
					info!("({id}): rejected");

					stream.write_all(&[0]).await.map_err(|err| {
					   error!("({id}): error sending rejection: '{err:?}'");
					})?;
					stream.flush().await.map_err(|err| {
					   error!("({id}): error flushing rejection: '{err:?}'");
					})?;
				}
				Err(_) => {
					warn!("({id}): error with Spacedrop pairing request receiver!");
				}
			}
		}
	};

	Ok(())
}



File: ./src/p2p/operations/request_file.rs
-------------------------------------------------
use std::{
	path::Path,
	sync::{
		atomic::{AtomicBool, Ordering},
		Arc,
	},
};

use sd_p2p::{
	spaceblock::{BlockSize, Range, SpaceblockRequest, SpaceblockRequests, Transfer},
	spacetime::UnicastStream,
	PeerMessageEvent,
};
use sd_prisma::prisma::file_path;
use tokio::{
	fs::File,
	io::{AsyncReadExt, AsyncWrite, AsyncWriteExt, BufReader},
};
use tracing::{debug, warn};
use uuid::Uuid;

use crate::{
	library::Library,
	location::file_path_helper::{file_path_to_handle_p2p_serve_file, IsolatedFilePathData},
	p2p::{Header, HeaderFile},
	Node,
};

/// Request a file from the remote machine over P2P. This is used for preview media and quick preview.
///
/// DO NOT USE THIS WITHOUT `node.files_over_p2p_flag == true`
pub async fn request_file(
	mut stream: UnicastStream,
	library: &Library,
	file_path_id: Uuid,
	range: Range,
	output: impl AsyncWrite + Unpin,
) -> Result<(), ()> {
	let id = Uuid::new_v4();
	// TODO: Tunnel for encryption + authentication

	stream
		.write_all(
			&Header::File(HeaderFile {
				id,
				library_id: library.id,
				file_path_id,
				range: range.clone(),
			})
			.to_bytes(),
		)
		.await
		.map_err(|err| {
			warn!("({id}): failed to read `Header::File`: {err:?}");

			// TODO: UI error
			// TODO: Error sent to remote peer
		})?;

	let block_size = BlockSize::from_stream(&mut stream).await.map_err(|err| {
		warn!("({id}): failed to read block size: {err:?}");

		// TODO: UI error
		// TODO: Error sent to remote peer
	})?;
	let size = stream.read_u64_le().await.map_err(|err| {
		warn!("({id}): failed to read file size: {err:?}");

		// TODO: UI error
		// TODO: Error sent to remote peer
	})?;

	Transfer::new(
		&SpaceblockRequests {
			id,
			block_size,
			requests: vec![SpaceblockRequest {
				// TODO: Removing need for this field in this case
				name: "todo".to_string(),
				// TODO: Maybe removing need for `size` from this side
				size,
				range,
			}],
		},
		|percent| {
			debug!(
				"P2P receiving file path '{}' - progress {}%",
				file_path_id, percent
			);
		},
		&Arc::new(AtomicBool::new(false)),
	)
	.receive(&mut stream, output)
	.await
	.map_err(|err| {
		warn!("({id}): transfer failed: {err:?}");

		// TODO: Error in UI
		// TODO: Send error to remote peer???
	})?;

	Ok(())
}

pub(crate) async fn receiver(
	node: &Arc<Node>,
	HeaderFile {
		id,
		library_id,
		file_path_id,
		range,
	}: HeaderFile,
	event: PeerMessageEvent,
) -> Result<(), ()> {
	let mut stream = event.stream;
	#[allow(clippy::panic)] // If you've made it this far that's on you.
	if !node.files_over_p2p_flag.load(Ordering::Relaxed) {
		panic!("Files over P2P is disabled!");
	}

	// TODO: Tunnel and authentication
	// TODO: Use BufReader

	let library = node
		.libraries
		.get_library(&library_id)
		.await
		.ok_or_else(|| {
			warn!("({id}): library not found'{library_id:?}'");

			// TODO: Error in UI
			// TODO: Send error to remote peer??? -> Can we avoid constructing connection until this is done so it's only an error on one side?
		})?;

	let file_path = library
		.db
		.file_path()
		.find_unique(file_path::pub_id::equals(file_path_id.as_bytes().to_vec()))
		.select(file_path_to_handle_p2p_serve_file::select())
		.exec()
		.await
		.map_err(|err| {
			warn!("({id}): error querying for file_path '{file_path_id:?}': {err:?}",);

			// TODO: Error in UI
			// TODO: Send error to remote peer??? -> Can we avoid constructing connection until this is done so it's only an error on one side?
		})?
		.ok_or_else(|| {
			warn!("({id}): file_path not found '{file_path_id:?}'");

			// TODO: Error in UI
			// TODO: Send error to remote peer??? -> Can we avoid constructing connection until this is done so it's only an error on one side?
		})?;

	let location = file_path.location.as_ref().ok_or_else(|| {
		warn!("({id}): file_path '{file_path_id:?} is missing 'location' property");

		// TODO: Error in UI
		// TODO: Send error to remote peer???
	})?;
	let location_path = location.path.as_ref().ok_or_else(|| {
		warn!(
			"({id}): location '{:?} is missing 'path' property",
			location.id
		);

		// TODO: Error in UI
		// TODO: Send error to remote peer???
	})?;
	let path = Path::new(location_path)
		.join(IsolatedFilePathData::try_from((location.id, &file_path)).map_err(|err| {
			warn!("({id}): failed to construct 'IsolatedFilePathData' for location '{:?} '{file_path:?}': {err:?}", location.id);

			// TODO: Error in UI
			// TODO: Send error to remote peer???
		})?);

	debug!("Serving path '{:?}' over P2P", path);

	let file = File::open(&path).await.map_err(|err| {
		warn!("({id}): failed to open file '{path:?}': {err:?}");

		// TODO: Error in UI
		// TODO: Send error to remote peer???
	})?;

	let metadata = file.metadata().await.map_err(|err| {
		warn!("({id}): failed to get metadata for file '{path:?}': {err:?}");

		// TODO: Error in UI
		// TODO: Send error to remote peer???
	})?;
	let block_size = BlockSize::from_size(metadata.len());

	stream
		.write_all(&block_size.to_bytes())
		.await
		.map_err(|err| {
			warn!("({id}): failed to write block size: {err:?}");

			// TODO: Error in UI
			// TODO: Send error to remote peer???
		})?;
	stream
		.write_all(&metadata.len().to_le_bytes())
		.await
		.map_err(|err| {
			warn!("({id}): failed to write length: {err:?}");

			// TODO: Error in UI
			// TODO: Send error to remote peer???
		})?;

	let file = BufReader::new(file);
	Transfer::new(
		&SpaceblockRequests {
			id,
			block_size,
			requests: vec![SpaceblockRequest {
				// TODO: Removing need for this field in this case
				name: "todo".to_string(),
				size: metadata.len(),
				range,
			}],
		},
		|percent| {
			debug!(
				"P2P loading file path '{}' - progress {}%",
				file_path_id, percent
			);
		},
		&Arc::new(AtomicBool::new(false)),
	)
	.send(&mut stream, file)
	.await
	.map_err(|err| {
		warn!("({id}): transfer failed: {err:?}");

		// TODO: Error in UI
		// TODO: Send error to remote peer???
	})?;

	Ok(())
}

// TODO: Unit tests



File: ./src/p2p/library_metadata.rs
-------------------------------------------------
use std::collections::HashMap;

use sd_p2p::Metadata;
use serde::{Deserialize, Serialize};
use specta::Type;

#[derive(Debug, Clone, Type, Serialize, Deserialize)]
pub struct LibraryMetadata {}

impl Metadata for LibraryMetadata {
	fn to_hashmap(self) -> HashMap<String, String> {
		HashMap::with_capacity(0)
	}

	fn from_hashmap(_: &HashMap<String, String>) -> Result<Self, String>
	where
		Self: Sized,
	{
		Ok(Self {})
	}
}



File: ./src/p2p/identity_or_remote_identity.rs
-------------------------------------------------
use sd_p2p::spacetunnel::{Identity, IdentityErr, RemoteIdentity};
use thiserror::Error;

#[derive(Debug, Error)]
pub enum IdentityOrRemoteIdentityErr {
	#[error("IdentityErr({0})")]
	IdentityErr(#[from] IdentityErr),
	#[error("InvalidFormat")]
	InvalidFormat,
}

/// TODO
#[derive(Debug, PartialEq)]

pub enum IdentityOrRemoteIdentity {
	Identity(Identity),
	RemoteIdentity(RemoteIdentity),
}

impl IdentityOrRemoteIdentity {
	pub fn remote_identity(&self) -> RemoteIdentity {
		match self {
			Self::Identity(identity) => identity.to_remote_identity(),
			Self::RemoteIdentity(identity) => {
				RemoteIdentity::from_bytes(identity.get_bytes().as_slice()).expect("unreachable")
			}
		}
	}
}

impl IdentityOrRemoteIdentity {
	pub fn from_bytes(bytes: &[u8]) -> Result<Self, IdentityOrRemoteIdentityErr> {
		match bytes[0] {
			b'I' => Ok(Self::Identity(Identity::from_bytes(&bytes[1..])?)),
			b'R' => Ok(Self::RemoteIdentity(RemoteIdentity::from_bytes(
				&bytes[1..],
			)?)),
			_ => Err(IdentityOrRemoteIdentityErr::InvalidFormat),
		}
	}

	pub fn to_bytes(&self) -> Vec<u8> {
		match self {
			Self::Identity(identity) => [&[b'I'], &*identity.to_bytes()].concat(),
			Self::RemoteIdentity(identity) => [[b'R'].as_slice(), &identity.get_bytes()].concat(),
		}
	}
}



File: ./src/p2p/mod.rs
-------------------------------------------------
#![warn(clippy::all, clippy::unwrap_used, clippy::panic)]
#![allow(clippy::unnecessary_cast)] // Yeah they aren't necessary on this arch, but they are on others

mod identity_or_remote_identity;
mod libraries;
mod library_metadata;
pub mod operations;
mod p2p_events;
mod p2p_manager;
mod p2p_manager_actor;
mod pairing;
mod peer_metadata;
mod protocol;
pub mod sync;

pub use identity_or_remote_identity::*;
pub use libraries::*;
pub use library_metadata::*;
pub use p2p_events::*;
pub use p2p_manager::*;
pub use p2p_manager_actor::*;
pub use pairing::*;
pub use peer_metadata::*;
pub use protocol::*;

pub(super) const SPACEDRIVE_APP_ID: &str = "spacedrive";



File: ./src/p2p/peer_metadata.rs
-------------------------------------------------
use std::{collections::HashMap, env, str::FromStr};

use sd_p2p::Metadata;
use serde::{Deserialize, Serialize};
use specta::Type;

use crate::node::Platform;

#[derive(Debug, Clone, Type, Serialize, Deserialize)]
pub struct PeerMetadata {
	pub(super) name: String,
	pub(super) operating_system: Option<OperatingSystem>,
	pub(super) version: Option<String>,
}

impl Metadata for PeerMetadata {
	fn to_hashmap(self) -> HashMap<String, String> {
		let mut map = HashMap::with_capacity(5);
		map.insert("name".to_owned(), self.name);
		if let Some(os) = self.operating_system {
			map.insert("os".to_owned(), os.to_string());
		}
		if let Some(version) = self.version {
			map.insert("version".to_owned(), version);
		}
		map
	}

	fn from_hashmap(data: &HashMap<String, String>) -> Result<Self, String>
	where
		Self: Sized,
	{
		Ok(Self {
			name: data
				.get("name")
				.ok_or_else(|| {
					"DNS record for field 'name' missing. Unable to decode 'PeerMetadata'!"
						.to_owned()
				})?
				.to_owned(),
			operating_system: data
				.get("os")
				.map(|os| os.parse().map_err(|_| "Unable to parse 'OperationSystem'!"))
				.transpose()?,
			version: data.get("version").map(|v| v.to_owned()),
		})
	}
}

/// Represents the operating system which the remote peer is running.
/// This is not used internally and predominantly is designed to be used for display purposes by the embedding application.
#[derive(Debug, Clone, Type, Serialize, Deserialize)]
pub enum OperatingSystem {
	Windows,
	Linux,
	MacOS,
	Ios,
	Android,
	Other(String),
}

// TODO: Should `Platform` and `OperatingSystem` be merged into one?
impl From<Platform> for OperatingSystem {
	fn from(platform: Platform) -> Self {
		match platform {
			Platform::Unknown => OperatingSystem::Other("Unknown".into()),
			Platform::Windows => OperatingSystem::Windows,
			Platform::Linux => OperatingSystem::Linux,
			Platform::MacOS => OperatingSystem::MacOS,
			Platform::IOS => OperatingSystem::Ios,
			Platform::Android => OperatingSystem::Android,
		}
	}
}

impl OperatingSystem {
	pub fn get_os() -> Self {
		match env::consts::OS {
			"windows" => OperatingSystem::Windows,
			"macos" => OperatingSystem::MacOS,
			"linux" => OperatingSystem::Linux,
			"ios" => OperatingSystem::Ios,
			"android" => OperatingSystem::Android,
			platform => OperatingSystem::Other(platform.into()),
		}
	}
}

impl ToString for OperatingSystem {
	fn to_string(&self) -> String {
		match self {
			OperatingSystem::Windows => "Windows".into(),
			OperatingSystem::Linux => "Linux".into(),
			OperatingSystem::MacOS => "MacOS".into(),
			OperatingSystem::Ios => "IOS".into(),
			OperatingSystem::Android => "Android".into(),
			OperatingSystem::Other(s) => {
				let mut chars = s.chars();
				chars.next();
				chars.as_str().to_string()
			}
		}
	}
}

impl FromStr for OperatingSystem {
	type Err = ();

	fn from_str(s: &str) -> Result<Self, Self::Err> {
		let mut chars = s.chars();
		match chars.next() {
			Some('W') => Ok(OperatingSystem::Windows),
			Some('L') => Ok(OperatingSystem::Linux),
			Some('M') => Ok(OperatingSystem::MacOS),
			Some('I') => Ok(OperatingSystem::Ios),
			Some('A') => Ok(OperatingSystem::Android),
			_ => Ok(OperatingSystem::Other(s.to_owned())),
		}
	}
}



File: ./src/p2p/p2p_manager_actor.rs
-------------------------------------------------
use std::sync::Arc;

use futures::StreamExt;
use sd_p2p::{spacetunnel::Tunnel, Event, ManagerStream, Service, ServiceEvent};
use tokio::sync::mpsc;
use tracing::error;

use crate::Node;

use super::{operations, sync::SyncMessage, Header, LibraryMetadata, P2PEvent, P2PManager};

pub struct P2PManagerActor {
	pub(super) manager: Arc<P2PManager>,
	pub(super) stream: ManagerStream,
	pub(super) register_service_rx: mpsc::Receiver<Arc<Service<LibraryMetadata>>>,
}

impl P2PManagerActor {
	pub fn start(self, node: Arc<Node>) {
		let Self {
			manager: this,
			mut stream,
			mut register_service_rx,
		} = self;

		tokio::spawn({
			async move {
				let mut node_rx = this.node.listen();

				loop {
					tokio::select! {
					   // TODO: We ignore the response of this but I suspect it will be useful in the future so it stays for now.
					   Some(_event) = register_service_rx.recv() => {},
					   // TODO: We should subscribe to library-level events too but frontend isn't cut out for them right now.
					   Some(Ok(event)) = node_rx.next() => {
								this.events.0
										.send(match event {
											   ServiceEvent::Discovered { identity, metadata } =>
														P2PEvent::DiscoveredPeer {
															   identity,
															   metadata,
														},
											   ServiceEvent::Expired { identity } =>
														P2PEvent::ExpiredPeer {
															   identity,
														},
										})
										.map_err(|_| error!("Failed to send event to p2p event stream!"))
										.ok();
						}
						Some(event) = stream.next() => {
							match event {
								Event::PeerConnected(event) => {
									this.events
										.0
										.send(P2PEvent::ConnectedPeer {
											identity: event.identity,
										})
										.map_err(|_| error!("Failed to send event to p2p event stream!"))
										.ok();
								}
								Event::PeerDisconnected(identity) => {
									this.events
										.0
										.send(P2PEvent::DisconnectedPeer { identity })
										.map_err(|_| error!("Failed to send event to p2p event stream!"))
										.ok();
								}
								Event::PeerMessage(mut event) => {
									let this = this.clone();
									let node = node.clone();

									tokio::spawn(async move {
										let header = Header::from_stream(&mut event.stream)
											.await
											.map_err(|err| {
												error!("Failed to read header from stream: {}", err);
											})?;

										match header {
											Header::Ping => operations::ping::reciever(event).await,
											Header::Spacedrop(req) => {
												operations::spacedrop::reciever(&this, req, event).await?
											}
											Header::Pair => {
												this.pairing
													.clone()
													.responder(
														event.identity,
														event.stream,
														&node.libraries,
														node.clone(),
													)
													.await?
											}
											Header::Sync(library_id) => {
												let mut tunnel =
													Tunnel::responder(event.stream).await.map_err(|err| {
														error!("Failed `Tunnel::responder`: {}", err);
													})?;

												let msg =
													SyncMessage::from_stream(&mut tunnel).await.map_err(|err| {
														error!("Failed `SyncMessage::from_stream`: {}", err);
													})?;

												let library =
													node.libraries.get_library(&library_id).await.ok_or_else(|| {
														error!("Failed to get library '{library_id}'");

														// TODO: Respond to remote client with warning!
													})?;

												match msg {
													SyncMessage::NewOperations => {
														super::sync::responder(&mut tunnel, library).await?;
													}
												};
											}
											Header::File(req) => {
												operations::request_file::receiver(&node, req, event).await?;
											}
										}

										Ok::<_, ()>(())
									});
								}
								Event::Shutdown => break,
								_ => {}
							}
						}
					}
				}

				error!(
					"Manager event stream closed! The core is unstable from this point forward!"
				);
			}
		});
	}
}



File: ./src/p2p/sync/proto.rs
-------------------------------------------------
use tokio::io::{AsyncRead, AsyncReadExt};

// will probs have more variants in future
#[derive(Debug, PartialEq, Eq)]
pub enum SyncMessage {
	NewOperations,
}

impl SyncMessage {
	// TODO: Per field errors for better error handling
	// TODO: Using `decode::Error` instead of `io::Result`
	pub async fn from_stream(stream: &mut (impl AsyncRead + Unpin)) -> std::io::Result<Self> {
		match stream.read_u8().await? {
			b'N' => Ok(Self::NewOperations),
			header => Err(std::io::Error::new(
				std::io::ErrorKind::InvalidData,
				format!("Invalid sync message header: {}", (header as char)),
			)),
		}
	}

	pub fn to_bytes(&self) -> Vec<u8> {
		match self {
			Self::NewOperations => vec![b'N'],
		}
	}
}

#[cfg(test)]
mod tests {
	use super::*;

	#[tokio::test]
	async fn test_types() {
		{
			let original = SyncMessage::NewOperations;

			let mut cursor = std::io::Cursor::new(original.to_bytes());
			let result = SyncMessage::from_stream(&mut cursor).await.unwrap();
			assert_eq!(original, result);
		}
	}
}



File: ./src/p2p/sync/mod.rs
-------------------------------------------------
#![allow(clippy::panic, clippy::unwrap_used)] // TODO: Finish this

use std::sync::Arc;

use sd_p2p::{
	proto::{decode, encode},
	spacetunnel::Tunnel,
};
use sd_sync::CRDTOperation;
use sync::GetOpsArgs;

use tokio::io::{AsyncRead, AsyncWrite, AsyncWriteExt};
use tracing::*;
use uuid::Uuid;

use crate::{library::Library, sync};

use super::{Header, P2PManager};

mod proto;
pub use proto::*;

pub use originator::run as originator;
mod originator {
	use super::*;
	use responder::tx as rx;
	use sd_p2p::PeerStatus;

	pub mod tx {
		use super::*;

		#[derive(Debug, PartialEq)]
		pub struct Operations(pub Vec<CRDTOperation>);

		impl Operations {
			// TODO: Per field errors for better error handling
			pub async fn from_stream(
				stream: &mut (impl AsyncRead + Unpin),
			) -> std::io::Result<Self> {
				Ok(Self(
					rmp_serde::from_slice(&decode::buf(stream).await.unwrap()).unwrap(),
				))
			}

			pub fn to_bytes(&self) -> Vec<u8> {
				let Self(args) = self;
				let mut buf = vec![];

				// TODO: Error handling
				encode::buf(&mut buf, &rmp_serde::to_vec_named(&args).unwrap());
				buf
			}
		}

		#[cfg(test)]
		#[tokio::test]
		async fn test() {
			{
				let original = Operations(vec![]);

				let mut cursor = std::io::Cursor::new(original.to_bytes());
				let result = Operations::from_stream(&mut cursor).await.unwrap();
				assert_eq!(original, result);
			}

			{
				let original = Operations(vec![CRDTOperation {
					instance: Uuid::new_v4(),
					timestamp: sync::NTP64(0),
					id: Uuid::new_v4(),
					typ: sd_sync::CRDTOperationType::Shared(sd_sync::SharedOperation {
						record_id: serde_json::Value::Null,
						model: "name".to_string(),
						data: sd_sync::SharedOperationData::Create,
					}),
				}]);

				let mut cursor = std::io::Cursor::new(original.to_bytes());
				let result = Operations::from_stream(&mut cursor).await.unwrap();
				assert_eq!(original, result);
			}
		}
	}

	/// REMEMBER: This only syncs one direction!
	pub async fn run(library_id: Uuid, sync: &Arc<sync::Manager>, p2p: &Arc<super::P2PManager>) {
		let service = p2p.get_library_service(&library_id).unwrap();

		// TODO: Deduplicate any duplicate peer ids -> This is an edge case but still
		for (remote_identity, status) in service.get_state() {
			let PeerStatus::Connected = status else {
				continue;
			};

			let sync = sync.clone();
			let p2p = p2p.clone();
			let service = service.clone();

			tokio::spawn(async move {
				debug!(
					"Alerting peer '{remote_identity:?}' of new sync events for library '{library_id:?}'"
				);

				let mut stream = service
					.connect(p2p.manager.clone(), &remote_identity)
					.await
					.map_err(|_| ())
					.unwrap(); // TODO: handle providing incorrect peer id

				stream
					.write_all(&Header::Sync(library_id).to_bytes())
					.await
					.unwrap();

				let mut tunnel = Tunnel::initiator(stream).await.unwrap();

				tunnel
					.write_all(&SyncMessage::NewOperations.to_bytes())
					.await
					.unwrap();
				tunnel.flush().await.unwrap();

				while let Ok(rx::MainRequest::GetOperations(args)) =
					rx::MainRequest::from_stream(&mut tunnel).await
				{
					let ops = sync.get_ops(args).await.unwrap();

					tunnel
						.write_all(&tx::Operations(ops).to_bytes())
						.await
						.unwrap();
					tunnel.flush().await.unwrap();
				}
			});
		}
	}
}

pub use responder::run as responder;
mod responder {
	use super::*;
	use originator::tx as rx;

	pub mod tx {
		use serde::{Deserialize, Serialize};

		use super::*;

		#[derive(Serialize, Deserialize, PartialEq, Debug)]
		pub enum MainRequest {
			GetOperations(GetOpsArgs),
			Done,
		}

		impl MainRequest {
			// TODO: Per field errors for better error handling
			pub async fn from_stream(
				stream: &mut (impl AsyncRead + Unpin),
			) -> std::io::Result<Self> {
				Ok(
					// TODO: Error handling
					rmp_serde::from_slice(&decode::buf(stream).await.unwrap()).unwrap(),
				)
			}

			pub fn to_bytes(&self) -> Vec<u8> {
				let mut buf = vec![];
				// TODO: Error handling
				encode::buf(&mut buf, &rmp_serde::to_vec_named(&self).unwrap());
				buf
			}
		}

		#[cfg(test)]
		#[tokio::test]
		async fn test() {
			{
				let original = MainRequest::GetOperations(GetOpsArgs {
					clocks: vec![],
					count: 0,
				});

				let mut cursor = std::io::Cursor::new(original.to_bytes());
				let result = MainRequest::from_stream(&mut cursor).await.unwrap();
				assert_eq!(original, result);
			}

			{
				let original = MainRequest::Done;

				let mut cursor = std::io::Cursor::new(original.to_bytes());
				let result = MainRequest::from_stream(&mut cursor).await.unwrap();
				assert_eq!(original, result);
			}
		}
	}

	pub async fn run(
		stream: &mut (impl AsyncRead + AsyncWrite + Unpin),
		library: Arc<Library>,
	) -> Result<(), ()> {
		let ingest = &library.sync.ingest;

		async fn early_return(stream: &mut (impl AsyncRead + AsyncWrite + Unpin)) {
			// TODO: Proper error returned to remote instead of this.
			// TODO: We can't just abort the connection when the remote is expecting data.
			stream
				.write_all(&tx::MainRequest::Done.to_bytes())
				.await
				.unwrap();
			stream.flush().await.unwrap();
		}

		let Ok(mut rx) = ingest.req_rx.try_lock() else {
			warn!("Rejected sync due to libraries lock being held!");

			early_return(stream).await;
			return Ok(());
		};

		use sync::ingest::*;

		ingest.event_tx.send(Event::Notification).await.unwrap();

		while let Some(req) = rx.recv().await {
			const OPS_PER_REQUEST: u32 = 1000;

			let timestamps = match req {
				Request::FinishedIngesting => break,
				Request::Messages { timestamps } => timestamps,
				_ => continue,
			};

			debug!("Getting ops for timestamps {timestamps:?}");

			stream
				.write_all(
					&tx::MainRequest::GetOperations(sync::GetOpsArgs {
						clocks: timestamps,
						count: OPS_PER_REQUEST,
					})
					.to_bytes(),
				)
				.await
				.unwrap();
			stream.flush().await.unwrap();

			let rx::Operations(ops) = rx::Operations::from_stream(stream).await.unwrap();

			ingest
				.event_tx
				.send(Event::Messages(MessagesEvent {
					instance_id: library.sync.instance,
					has_more: ops.len() == OPS_PER_REQUEST as usize,
					messages: ops,
				}))
				.await
				.expect("TODO: Handle ingest channel closed, so we don't loose ops");
		}

		debug!("Sync responder done");

		stream
			.write_all(&tx::MainRequest::Done.to_bytes())
			.await
			.unwrap();
		stream.flush().await.unwrap();

		Ok(())
	}
}



File: ./src/job/error.rs
-------------------------------------------------
use crate::{
	location::{indexer::IndexerError, LocationError},
	object::{
		file_identifier::FileIdentifierJobError, fs::error::FileSystemJobsError,
		media::media_processor::MediaProcessorError, validation::ValidatorError,
	},
	util::{db::MissingFieldError, error::FileIOError},
};

use sd_crypto::Error as CryptoError;

use std::time::Duration;

use prisma_client_rust::QueryError;
use rmp_serde::{decode::Error as DecodeError, encode::Error as EncodeError};
use thiserror::Error;
use tokio::sync::oneshot;
use uuid::Uuid;

#[derive(Error, Debug)]
pub enum JobError {
	// General errors
	#[error("database error: {0}")]
	Database(#[from] QueryError),
	#[error("Failed to join Tokio spawn blocking: {0}")]
	JoinTask(#[from] tokio::task::JoinError),
	#[error("job state encode error: {0}")]
	StateEncode(#[from] EncodeError),
	#[error("job state decode error: {0}")]
	StateDecode(#[from] DecodeError),
	#[error("job metadata serialization error: {0}")]
	MetadataSerialization(#[from] serde_json::Error),
	#[error("tried to resume a job with unknown name: job <name='{1}', uuid='{0}'>")]
	UnknownJobName(Uuid, String),
	#[error(
		"Tried to resume a job that doesn't have saved state data: job <name='{1}', uuid='{0}'>"
	)]
	MissingJobDataState(Uuid, String),
	#[error("missing report field: job <uuid='{id}', name='{name}'>")]
	MissingReport { id: Uuid, name: String },
	#[error("missing some job data: '{value}'")]
	MissingData { value: String },
	#[error("invalid job status integer: {0}")]
	InvalidJobStatusInt(i32),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
	#[error("Location error: {0}")]
	Location(#[from] LocationError),
	#[error("missing-field: {0}")]
	MissingField(#[from] MissingFieldError),
	#[error("item of type '{0}' with id '{1}' is missing from the db")]
	MissingFromDb(&'static str, String),
	#[error("job timed out after {0:?} without updates")]
	Timeout(Duration),
	#[error("critical job error: {0}")]
	Critical(&'static str),

	// Specific job errors
	#[error(transparent)]
	Indexer(#[from] IndexerError),
	#[error(transparent)]
	MediaProcessor(#[from] MediaProcessorError),
	#[error(transparent)]
	FileIdentifier(#[from] FileIdentifierJobError),
	#[error(transparent)]
	Validator(#[from] ValidatorError),
	#[error(transparent)]
	FileSystemJobsError(#[from] FileSystemJobsError),
	#[error(transparent)]
	CryptoError(#[from] CryptoError),

	// Not errors
	#[error("job had a early finish: <name='{name}', reason='{reason}'>")]
	EarlyFinish { name: String, reason: String },
	#[error("data needed for job execution not found: job <name='{0}'>")]
	JobDataNotFound(String),
	#[error("job paused")]
	Paused(Vec<u8>, oneshot::Sender<()>),
	#[error("job canceled")]
	Canceled(oneshot::Sender<()>),
}

#[derive(Error, Debug)]
pub enum JobManagerError {
	#[error("Tried to dispatch a job that is already running: Job <name='{name}', hash='{hash}'>")]
	AlreadyRunningJob { name: &'static str, hash: u64 },

	#[error("Failed to fetch job data from database: {0}")]
	Database(#[from] prisma_client_rust::QueryError),

	#[error("job not found: {0}")]
	NotFound(Uuid),

	#[error("missing-field: {0}")]
	MissingField(#[from] MissingFieldError),
}

impl From<JobManagerError> for rspc::Error {
	fn from(value: JobManagerError) -> Self {
		match value {
			JobManagerError::AlreadyRunningJob { .. } => Self::with_cause(
				rspc::ErrorCode::BadRequest,
				"Tried to spawn a job that is already running!".to_string(),
				value,
			),
			JobManagerError::Database(_) => Self::with_cause(
				rspc::ErrorCode::InternalServerError,
				"Error accessing the database".to_string(),
				value,
			),
			JobManagerError::NotFound(_) => Self::with_cause(
				rspc::ErrorCode::NotFound,
				"Job not found".to_string(),
				value,
			),
			JobManagerError::MissingField(_) => Self::with_cause(
				rspc::ErrorCode::InternalServerError,
				"Missing field".to_string(),
				value,
			),
		}
	}
}



File: ./src/job/worker.rs
-------------------------------------------------
use crate::{api::CoreEvent, invalidate_query, library::Library, Node};

use std::{
	fmt,
	pin::pin,
	sync::{
		atomic::{AtomicBool, Ordering},
		Arc,
	},
	time::Duration,
};

use async_channel as chan;
use chrono::{DateTime, Utc};
use futures::stream::{self, StreamExt};
use futures_concurrency::stream::Merge;
use serde::Serialize;
use serde_json::json;
use specta::Type;
use tokio::{
	spawn,
	sync::{oneshot, watch},
	task::JoinError,
	time::{interval, timeout, Instant, MissedTickBehavior},
};
use tokio_stream::wrappers::IntervalStream;
use tracing::{debug, error, info, trace, warn};
use uuid::Uuid;

use super::{
	DynJob, JobError, JobIdentity, JobReport, JobReportUpdate, JobRunErrors, JobRunOutput,
	JobStatus, Jobs,
};

const FIVE_SECS: Duration = Duration::from_secs(5);
const FIVE_MINUTES: Duration = Duration::from_secs(10 * 60);

#[derive(Debug, Clone, Serialize, Type)]
pub struct JobProgressEvent {
	pub id: Uuid,
	pub library_id: Uuid,
	pub task_count: i32,
	pub completed_task_count: i32,
	pub phase: String,
	pub message: String,
	pub estimated_completion: DateTime<Utc>,
}

// used to update the worker state from inside the worker thread
#[derive(Debug)]
pub enum WorkerEvent {
	Progressed(Vec<JobReportUpdate>),
	Paused,
	Stop,
}

// used to send commands to the worker thread from the manager
#[derive(Debug)]
pub enum WorkerCommand {
	Pause(Instant),
	Resume(Instant),
	IdentifyYourself(oneshot::Sender<JobIdentity>),
	Cancel(Instant, oneshot::Sender<()>),
	Shutdown(Instant, oneshot::Sender<()>),
	Timeout(Duration, oneshot::Sender<()>),
}

pub struct WorkerContext {
	pub library: Arc<Library>,
	pub node: Arc<Node>,
	pub(super) events_tx: chan::Sender<WorkerEvent>,
}

impl fmt::Debug for WorkerContext {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		f.debug_struct("WorkerContext").finish()
	}
}

impl Drop for WorkerContext {
	fn drop(&mut self) {
		// This send blocking is fine as this sender is unbounded
		if !self.events_tx.is_closed() && self.events_tx.send_blocking(WorkerEvent::Stop).is_err() {
			error!("Error sending worker context stop event");
		}
	}
}
impl WorkerContext {
	pub fn pause(&self) {
		if self.events_tx.send_blocking(WorkerEvent::Paused).is_err() {
			error!("Error sending worker context pause event");
		}
	}

	pub fn progress_msg(&self, msg: String) {
		self.progress(vec![JobReportUpdate::Message(msg)]);
	}

	pub fn progress(&self, updates: Vec<JobReportUpdate>) {
		if !self.events_tx.is_closed()
			&& self
				.events_tx
				// This send blocking is fine as this sender is unbounded
				.send_blocking(WorkerEvent::Progressed(updates))
				.is_err()
		{
			error!("Error sending worker context progress event");
		}
	}
}

// a worker is a dedicated task that runs a single job
// once the job is complete the worker will exit
pub struct Worker {
	pub(super) library_id: Uuid,
	commands_tx: chan::Sender<WorkerCommand>,
	report_watch_tx: Arc<watch::Sender<JobReport>>,
	report_watch_rx: watch::Receiver<JobReport>,
	paused: AtomicBool,
}

impl Worker {
	pub async fn new(
		id: Uuid,
		mut job: Box<dyn DynJob>,
		mut report: JobReport,
		library: Arc<Library>,
		node: Arc<Node>,
		job_manager: Arc<Jobs>,
	) -> Result<Self, JobError> {
		let (commands_tx, commands_rx) = chan::bounded(8);

		let job_hash = job.hash();

		let start_time = Utc::now();

		report.status = JobStatus::Running;
		if report.started_at.is_none() {
			report.started_at = Some(start_time);
		}

		// If the report doesn't have a created_at date, it's a new report
		if report.created_at.is_none() {
			report.create(&library).await?;
		} else {
			// Otherwise it can be a job being resumed or a children job that was already been created
			report.update(&library).await?;
		}

		job.register_children(&library).await?;

		invalidate_queries(&library);

		let (report_watch_tx, report_watch_rx) = watch::channel(report.clone());
		let report_watch_tx = Arc::new(report_watch_tx);
		let library_id = library.id;

		// spawn task to handle running the job
		spawn(Self::do_work(
			id,
			JobWorkTable {
				job,
				manager: job_manager,
				hash: job_hash,
				report,
			},
			Arc::clone(&report_watch_tx),
			start_time,
			(commands_tx.clone(), commands_rx),
			library,
			node,
		));

		Ok(Self {
			library_id,
			commands_tx,
			report_watch_tx,
			report_watch_rx,
			paused: AtomicBool::new(false),
		})
	}

	pub async fn pause(&self) {
		if self.report_watch_rx.borrow().status == JobStatus::Running {
			self.paused.store(true, Ordering::Relaxed);
			if self
				.commands_tx
				.send(WorkerCommand::Pause(Instant::now()))
				.await
				.is_ok()
			{
				self.report_watch_tx
					.send_modify(|report| report.status = JobStatus::Paused);
			}
		}
	}

	pub async fn who_am_i(&self) -> Option<JobIdentity> {
		let (tx, rx) = oneshot::channel();
		if self
			.commands_tx
			.send(WorkerCommand::IdentifyYourself(tx))
			.await
			.is_err()
		{
			warn!("Failed to send identify yourself command to a job worker");
			return None;
		}

		rx.await
			.map_err(|_| warn!("Failed to receive identify yourself answer from a job worker"))
			.ok()
	}

	pub async fn resume(&self) {
		if self.report_watch_rx.borrow().status == JobStatus::Paused {
			self.paused.store(false, Ordering::Relaxed);
			if self
				.commands_tx
				.send(WorkerCommand::Resume(Instant::now()))
				.await
				.is_ok()
			{
				self.report_watch_tx
					.send_modify(|report| report.status = JobStatus::Running);
			}
		}
	}

	pub async fn cancel(&self) {
		if self.report_watch_rx.borrow().status != JobStatus::Canceled {
			let (tx, rx) = oneshot::channel();
			if self
				.commands_tx
				.send(WorkerCommand::Cancel(Instant::now(), tx))
				.await
				.is_ok()
			{
				self.report_watch_tx
					.send_modify(|report| report.status = JobStatus::Canceled);
				rx.await.ok();
			}
		}
	}

	pub async fn shutdown(&self) {
		let (tx, rx) = oneshot::channel();
		if self
			.commands_tx
			.send(WorkerCommand::Shutdown(Instant::now(), tx))
			.await
			.is_ok()
		{
			rx.await.ok();
		}
	}

	pub fn report(&self) -> JobReport {
		self.report_watch_rx.borrow().clone()
	}

	pub fn is_paused(&self) -> bool {
		self.paused.load(Ordering::Relaxed)
	}

	fn track_progress(
		report: &mut JobReport,
		last_report_watch_update: &mut Instant,
		report_watch_tx: &watch::Sender<JobReport>,
		start_time: DateTime<Utc>,
		updates: Vec<JobReportUpdate>,
		library: &Library,
	) {
		// protect against updates if job is not running
		if report.status != JobStatus::Running {
			return;
		};

		for update in updates {
			match update {
				JobReportUpdate::TaskCount(task_count) => {
					report.task_count = task_count as i32;
				}
				JobReportUpdate::CompletedTaskCount(completed_task_count) => {
					report.completed_task_count = completed_task_count as i32;
				}

				JobReportUpdate::Message(message) => {
					trace!("job {} message: {}", report.id, message);
					report.message = message;
				}
				JobReportUpdate::Phase(phase) => {
					trace!(
						"changing Job <id='{}'> phase: {} -> {phase}",
						report.id,
						report.phase
					);
					report.phase = phase;
				}
			}
		}

		// Calculate elapsed time
		let elapsed = Utc::now() - start_time;

		// Calculate remaining time
		let task_count = report.task_count as usize;
		let completed_task_count = report.completed_task_count as usize;
		let remaining_task_count = task_count.saturating_sub(completed_task_count);
		let remaining_time_per_task = elapsed / (completed_task_count + 1) as i32; // Adding 1 to avoid division by zero
		let remaining_time = remaining_time_per_task * remaining_task_count as i32;

		// Update the report with estimated remaining time
		report.estimated_completion = Utc::now()
			.checked_add_signed(remaining_time)
			.unwrap_or(Utc::now());

		// updated the report watcher
		if last_report_watch_update.elapsed() > Duration::from_millis(500) {
			report_watch_tx.send_modify(|old| {
				old.task_count = report.task_count;
				old.completed_task_count = report.completed_task_count;
				old.estimated_completion = report.estimated_completion;
				old.message = report.message.clone();
			});
			*last_report_watch_update = Instant::now();
		}

		// emit a CoreEvent
		library.emit(CoreEvent::JobProgress(JobProgressEvent {
			id: report.id,
			library_id: library.id,
			task_count: report.task_count,
			completed_task_count: report.completed_task_count,
			estimated_completion: report.estimated_completion,
			phase: report.phase.clone(),
			message: report.message.clone(),
		}));
	}

	async fn do_work(
		worker_id: Uuid,
		JobWorkTable {
			mut job,
			manager,
			hash,
			mut report,
		}: JobWorkTable,
		report_watch_tx: Arc<watch::Sender<JobReport>>,
		start_time: DateTime<Utc>,
		(commands_tx, commands_rx): (chan::Sender<WorkerCommand>, chan::Receiver<WorkerCommand>),
		library: Arc<Library>,
		node: Arc<Node>,
	) {
		let (events_tx, events_rx) = chan::unbounded();

		let mut timeout_checker = interval(FIVE_SECS);
		timeout_checker.set_missed_tick_behavior(MissedTickBehavior::Skip);

		let mut last_update_received_at = Instant::now();

		let mut last_reporter_watch_update = Instant::now();
		invalidate_query!(library, "jobs.reports");

		let mut finalized_events_rx = pin!(events_rx.clone());

		let mut is_paused = false;

		let mut run_task = {
			let library = Arc::clone(&library);
			spawn(async move {
				let job_result = job
					.run(
						WorkerContext {
							library,
							node,
							events_tx,
						},
						commands_rx,
					)
					.await;

				(job, job_result)
			})
		};

		type RunOutput = (Box<dyn DynJob>, Result<JobRunOutput, JobError>);

		enum StreamMessage {
			JobResult(Result<RunOutput, JoinError>),
			NewEvent(WorkerEvent),
			Tick,
		}

		let mut msg_stream = pin!((
			stream::once(&mut run_task).map(StreamMessage::JobResult),
			events_rx.map(StreamMessage::NewEvent),
			IntervalStream::new(timeout_checker).map(|_| StreamMessage::Tick),
		)
			.merge());

		let mut events_ended = false;

		while let Some(msg) = msg_stream.next().await {
			match msg {
				StreamMessage::JobResult(Err(join_error)) => {
					error!("Worker<id='{worker_id}'> had a critical error: {join_error:#?}");
					break;
				}
				StreamMessage::JobResult(Ok((job, job_result))) => {
					if !events_ended {
						finalized_events_rx.close();
						// There are still some progress events to be processed so we postpone the job result
						while let Some(WorkerEvent::Progressed(updates)) =
							finalized_events_rx.next().await
						{
							Self::track_progress(
								&mut report,
								&mut last_reporter_watch_update,
								&report_watch_tx,
								start_time,
								updates,
								&library,
							);
						}
					}

					let next_job =
						Self::process_job_output(job, job_result, &mut report, &library).await;

					report_watch_tx.send(report.clone()).ok();

					debug!(
						"Worker<id='{worker_id}'> completed Job<id='{}', name='{}'>",
						report.id, report.name
					);

					return manager.complete(&library, worker_id, hash, next_job).await;
				}
				StreamMessage::NewEvent(WorkerEvent::Progressed(updates)) => {
					is_paused = false;
					last_update_received_at = Instant::now();
					Self::track_progress(
						&mut report,
						&mut last_reporter_watch_update,
						&report_watch_tx,
						start_time,
						updates,
						&library,
					);
				}
				StreamMessage::NewEvent(WorkerEvent::Paused) => {
					is_paused = true;
				}
				StreamMessage::NewEvent(WorkerEvent::Stop) => {
					events_ended = true;
				}
				StreamMessage::Tick => {
					if !is_paused {
						let elapsed = last_update_received_at.elapsed();
						if elapsed > FIVE_MINUTES {
							error!(
							"Worker<id='{worker_id}'> has not received any updates for {elapsed:?}"
						);

							let (tx, rx) = oneshot::channel();
							if commands_tx
								.send(WorkerCommand::Timeout(elapsed, tx))
								.await
								.is_err()
							{
								error!("Worker<id='{worker_id}'> failed to send timeout step command to a running job");
							} else if timeout(FIVE_SECS, rx).await.is_err() {
								error!("Worker<id='{worker_id}'> failed to receive timeout step answer from a running job");
							}

							// As we already sent a timeout command, we can safely join as the job is over
							let Ok((job, job_result)) = run_task.await.map_err(|join_error| {
								error!("Worker<id='{worker_id}'> had a critical error: {join_error:#?}")
							}) else {
								break;
							};

							Self::process_job_output(job, job_result, &mut report, &library).await;

							report_watch_tx.send(report.clone()).ok();

							error!(
								"Worker<id='{worker_id}'> timed out Job<id='{}', name='{}'>",
								report.id, report.name
							);

							break;
						}
					}
				}
			}
		}

		manager.complete(&library, worker_id, hash, None).await
	}

	async fn process_job_output(
		mut job: Box<dyn DynJob>,
		job_result: Result<JobRunOutput, JobError>,
		report: &mut JobReport,
		library: &Library,
	) -> Option<Box<dyn DynJob>> {
		// Run the job and handle the result
		match job_result {
			// -> Job completed successfully
			Ok(JobRunOutput {
				metadata,
				errors: JobRunErrors(errors),
				next_job,
			}) if errors.is_empty() => {
				report.status = JobStatus::Completed;
				report.data = None;
				report.metadata = match (report.metadata.take(), metadata) {
					(Some(mut current_metadata), Some(new_metadata)) => {
						current_metadata["output"] = new_metadata;
						Some(current_metadata)
					}
					(None, Some(new_metadata)) => Some(json!({ "output": new_metadata })),
					(Some(current_metadata), None) => Some(current_metadata),
					_ => None,
				};
				report.completed_at = Some(Utc::now());
				if let Err(e) = report.update(library).await {
					error!("failed to update job report: {:#?}", e);
				}

				debug!("{report}");

				invalidate_queries(library);

				return next_job;
			}
			// -> Job completed with errors
			Ok(JobRunOutput {
				metadata,
				errors: JobRunErrors(errors),
				next_job,
			}) => {
				warn!(
					"Job<id='{}', name='{}'> completed with errors",
					report.id, report.name
				);
				report.status = JobStatus::CompletedWithErrors;
				report.errors_text = errors;
				report.data = None;
				report.metadata = match (report.metadata.take(), metadata) {
					(Some(mut current_metadata), Some(new_metadata)) => {
						current_metadata["output"] = new_metadata;
						Some(current_metadata)
					}
					(None, Some(new_metadata)) => Some(json!({ "output": new_metadata })),
					(Some(current_metadata), None) => Some(current_metadata),
					_ => None,
				};
				report.completed_at = Some(Utc::now());
				if let Err(e) = report.update(library).await {
					error!("failed to update job report: {:#?}", e);
				}

				debug!("{report}");

				invalidate_queries(library);

				return next_job;
			}
			// -> Job paused
			Err(JobError::Paused(state, signal_tx)) => {
				info!(
					"Job<id='{}', name='{}'> paused, we will pause all children jobs",
					report.id, report.name
				);
				if let Err(e) = job.pause_children(library).await {
					error!("Failed to pause children jobs: {e:#?}");
				}

				debug!("Setting worker status to paused");

				report.status = JobStatus::Paused;
				report.data = Some(state);

				if let Err(e) = report.update(library).await {
					error!("failed to update job report: {:#?}", e);
				}

				debug!("{report}");

				invalidate_queries(library);

				signal_tx.send(()).ok();
			}
			// -> Job paused
			Err(JobError::Canceled(signal_tx)) => {
				info!(
					"Job<id='{}', name='{}'> canceled, we will cancel all children jobs",
					report.id, report.name
				);
				if let Err(e) = job.cancel_children(library).await {
					error!("Failed to pause children jobs: {e:#?}");
				}

				debug!("Setting worker status to paused");

				report.status = JobStatus::Canceled;
				report.data = None;

				if let Err(e) = report.update(library).await {
					error!("failed to update job report: {:#?}", e);
				}

				debug!("{report}");

				invalidate_queries(library);

				signal_tx.send(()).ok();
			}
			// -> Job failed
			Err(e) => {
				error!(
					"Job<id='{}', name='{}'> failed with error: {e:#?};",
					report.id, report.name
				);
				if let Err(e) = job.cancel_children(library).await {
					error!("Failed to cancel children jobs: {e:#?}");
				}

				report.status = JobStatus::Failed;
				report.data = None;
				if let Err(e) = report.update(library).await {
					error!("failed to update job report: {:#?}", e);
				}

				warn!("{report}");

				invalidate_queries(library);
			}
		}

		None
	}
}

struct JobWorkTable {
	job: Box<dyn DynJob>,
	manager: Arc<Jobs>,
	hash: u64,
	report: JobReport,
}

fn invalidate_queries(library: &Library) {
	invalidate_query!(library, "jobs.isActive");
	invalidate_query!(library, "jobs.reports");
}



File: ./src/job/report.rs
-------------------------------------------------
use crate::{
	library::Library,
	prisma::job,
	util::db::{maybe_missing, MissingFieldError},
};

use std::{
	collections::HashMap,
	fmt::{Display, Formatter},
};

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use specta::Type;
use tracing::error;
use uuid::Uuid;

use super::JobError;

#[derive(Debug)]
pub enum JobReportUpdate {
	TaskCount(usize),
	CompletedTaskCount(usize),
	Message(String),
	Phase(String),
}

job::select!(job_without_data {
	id
	name
	action
	status
	parent_id
	errors_text
	metadata
	date_created
	date_started
	date_completed
	task_count
	completed_task_count
	date_estimated_completion
});

#[derive(Debug, Serialize, Deserialize, Type, Clone)]
pub struct JobReport {
	pub id: Uuid,
	pub name: String,
	pub action: Option<String>,
	pub data: Option<Vec<u8>>,
	// In Typescript `any | null` is just `any` so we don't get prompted for null checks
	// TODO(@Oscar): This will be fixed
	#[specta(type = Option<HashMap<String, serde_json::Value>>)]
	pub metadata: Option<serde_json::Value>,
	pub is_background: bool,
	pub errors_text: Vec<String>,

	pub created_at: Option<DateTime<Utc>>,
	pub started_at: Option<DateTime<Utc>>,
	pub completed_at: Option<DateTime<Utc>>,

	pub parent_id: Option<Uuid>,

	pub status: JobStatus,
	pub task_count: i32,
	pub completed_task_count: i32,

	pub phase: String,
	pub message: String,
	pub estimated_completion: DateTime<Utc>,
}

impl Display for JobReport {
	fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
		write!(
			f,
			"Job <name='{}', uuid='{}'> {:#?}",
			self.name, self.id, self.status
		)
	}
}

// convert database struct into a resource struct
impl TryFrom<job::Data> for JobReport {
	type Error = MissingFieldError;

	fn try_from(data: job::Data) -> Result<Self, Self::Error> {
		Ok(Self {
			id: Uuid::from_slice(&data.id).expect("corrupted database"),
			is_background: false, // deprecated
			name: maybe_missing(data.name, "job.name")?,
			action: data.action,
			data: data.data,
			metadata: data.metadata.and_then(|m| {
				serde_json::from_slice(&m).unwrap_or_else(|e| -> Option<serde_json::Value> {
					error!("Failed to deserialize job metadata: {}", e);
					None
				})
			}),
			errors_text: data
				.errors_text
				.map(|errors_str| errors_str.split("\n\n").map(str::to_string).collect())
				.unwrap_or_default(),
			created_at: data.date_created.map(DateTime::into),
			started_at: data.date_started.map(DateTime::into),
			completed_at: data.date_completed.map(DateTime::into),
			parent_id: data
				.parent_id
				.map(|id| Uuid::from_slice(&id).expect("corrupted database")),
			status: JobStatus::try_from(maybe_missing(data.status, "job.status")?)
				.expect("corrupted database"),
			task_count: data.task_count.unwrap_or(0),
			completed_task_count: data.completed_task_count.unwrap_or(0),
			phase: String::new(),
			message: String::new(),
			estimated_completion: data
				.date_estimated_completion
				.map_or(Utc::now(), DateTime::into),
		})
	}
}

// I despise having to write this twice, but it seems to be the only way to
// remove the data field from the struct
// would love to get this DRY'd up
impl TryFrom<job_without_data::Data> for JobReport {
	type Error = MissingFieldError;

	fn try_from(data: job_without_data::Data) -> Result<Self, Self::Error> {
		Ok(Self {
			id: Uuid::from_slice(&data.id).expect("corrupted database"),
			is_background: false, // deprecated
			name: maybe_missing(data.name, "job.name")?,
			action: data.action,
			data: None,
			metadata: data.metadata.and_then(|m| {
				serde_json::from_slice(&m).unwrap_or_else(|e| -> Option<serde_json::Value> {
					error!("Failed to deserialize job metadata: {}", e);
					None
				})
			}),
			errors_text: data
				.errors_text
				.map(|errors_str| errors_str.split("\n\n").map(str::to_string).collect())
				.unwrap_or_default(),
			created_at: data.date_created.map(DateTime::into),
			started_at: data.date_started.map(DateTime::into),
			completed_at: data.date_completed.map(DateTime::into),
			parent_id: data
				.parent_id
				.map(|id| Uuid::from_slice(&id).expect("corrupted database")),
			status: JobStatus::try_from(maybe_missing(data.status, "job.status")?)
				.expect("corrupted database"),
			task_count: data.task_count.unwrap_or(0),
			completed_task_count: data.completed_task_count.unwrap_or(0),

			phase: String::new(),
			message: String::new(),
			estimated_completion: data
				.date_estimated_completion
				.map_or(Utc::now(), DateTime::into),
		})
	}
}

impl JobReport {
	pub fn new(uuid: Uuid, name: String) -> Self {
		Self {
			id: uuid,
			is_background: false, // deprecated
			name,
			action: None,
			created_at: None,
			started_at: None,
			completed_at: None,
			status: JobStatus::Queued,
			errors_text: vec![],
			task_count: 0,
			data: None,
			metadata: None,
			parent_id: None,
			completed_task_count: 0,
			phase: String::new(),
			message: String::new(),
			estimated_completion: Utc::now(),
		}
	}

	pub fn get_meta(&self) -> (String, Option<String>) {
		// actions are formatted like "added_location" or "added_location-1"
		let Some(action_name) = self.action.as_ref().map(|action| {
			action
				.split('-')
				.next()
				.map(str::to_string)
				.unwrap_or_default()
		}) else {
			return (self.id.to_string(), None);
		};
		// create a unique group_key, EG: "added_location-<location_id>"
		let group_key = self.parent_id.map_or_else(
			|| format!("{}-{}", action_name, &self.id),
			|parent_id| format!("{}-{}", action_name, parent_id),
		);

		(action_name, Some(group_key))
	}

	pub async fn create(&mut self, library: &Library) -> Result<(), JobError> {
		let now = Utc::now();

		library
			.db
			.job()
			.create(
				self.id.as_bytes().to_vec(),
				sd_utils::chain_optional_iter(
					[
						job::name::set(Some(self.name.clone())),
						job::action::set(self.action.clone()),
						job::data::set(self.data.clone()),
						job::date_created::set(Some(now.into())),
						job::status::set(Some(self.status as i32)),
						job::date_started::set(self.started_at.map(|d| d.into())),
						job::task_count::set(Some(1)),
						job::completed_task_count::set(Some(0)),
					],
					[self
						.parent_id
						.map(|id| job::parent::connect(job::id::equals(id.as_bytes().to_vec())))],
				),
			)
			.exec()
			.await?;

		// Only setting created_at after we successfully created the job in DB
		self.created_at = Some(now);

		Ok(())
	}

	pub async fn update(&mut self, library: &Library) -> Result<(), JobError> {
		library
			.db
			.job()
			.update(
				job::id::equals(self.id.as_bytes().to_vec()),
				vec![
					job::status::set(Some(self.status as i32)),
					job::errors_text::set(
						(!self.errors_text.is_empty()).then(|| self.errors_text.join("\n\n")),
					),
					job::data::set(self.data.clone()),
					job::metadata::set(serde_json::to_vec(&self.metadata).ok()),
					job::task_count::set(Some(self.task_count)),
					job::completed_task_count::set(Some(self.completed_task_count)),
					job::date_started::set(self.started_at.map(Into::into)),
					job::date_completed::set(self.completed_at.map(Into::into)),
				],
			)
			.exec()
			.await?;
		Ok(())
	}
}

#[repr(i32)]
#[derive(Debug, Clone, Copy, Serialize, Deserialize, Type, Eq, PartialEq)]
pub enum JobStatus {
	Queued = 0,
	Running = 1,
	Completed = 2,
	Canceled = 3,
	Failed = 4,
	Paused = 5,
	CompletedWithErrors = 6,
}

impl JobStatus {
	pub fn is_finished(self) -> bool {
		matches!(
			self,
			Self::Completed
				| Self::Canceled | Self::Paused
				| Self::Failed | Self::CompletedWithErrors
		)
	}
}

impl TryFrom<i32> for JobStatus {
	type Error = JobError;

	fn try_from(value: i32) -> Result<Self, Self::Error> {
		let s = match value {
			0 => Self::Queued,
			1 => Self::Running,
			2 => Self::Completed,
			3 => Self::Canceled,
			4 => Self::Failed,
			5 => Self::Paused,
			6 => Self::CompletedWithErrors,
			_ => return Err(JobError::InvalidJobStatusInt(value)),
		};

		Ok(s)
	}
}

pub struct JobReportBuilder {
	pub id: Uuid,
	pub name: String,
	pub action: Option<String>,
	pub metadata: Option<serde_json::Value>,
	pub parent_id: Option<Uuid>,
}

impl JobReportBuilder {
	pub fn build(self) -> JobReport {
		JobReport {
			id: self.id,
			is_background: false, // deprecated
			name: self.name,
			action: self.action,
			created_at: None,
			started_at: None,
			completed_at: None,
			status: JobStatus::Queued,
			errors_text: vec![],
			task_count: 0,
			data: None,
			metadata: self.metadata,
			parent_id: self.parent_id,
			completed_task_count: 0,
			phase: String::new(),
			message: String::new(),
			estimated_completion: Utc::now(),
		}
	}

	pub fn new(id: Uuid, name: String) -> Self {
		Self {
			id,
			name,
			action: None,
			metadata: None,
			parent_id: None,
		}
	}

	pub fn with_action(mut self, action: impl AsRef<str>) -> Self {
		self.action = Some(action.as_ref().to_string());
		self
	}

	pub fn with_metadata(mut self, metadata: serde_json::Value) -> Self {
		self.metadata = Some(metadata);
		self
	}

	pub fn with_parent_id(mut self, parent_id: Uuid) -> Self {
		self.parent_id = Some(parent_id);
		self
	}
}



File: ./src/job/manager.rs
-------------------------------------------------
use crate::{
	job::{worker::Worker, DynJob, Job, JobError},
	library::Library,
	location::indexer::indexer_job::IndexerJobInit,
	object::{
		file_identifier::file_identifier_job::FileIdentifierJobInit,
		fs::{
			copy::FileCopierJobInit, cut::FileCutterJobInit, delete::FileDeleterJobInit,
			erase::FileEraserJobInit,
		},
		media::media_processor::MediaProcessorJobInit,
		validation::validator_job::ObjectValidatorJobInit,
	},
	prisma::job,
	Node,
};

use std::{
	collections::{HashMap, HashSet, VecDeque},
	sync::Arc,
};

use futures::future::join_all;
use prisma_client_rust::operator::or;
use tokio::sync::{mpsc, oneshot, RwLock};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use super::{JobIdentity, JobManagerError, JobReport, JobStatus, StatefulJob};

const MAX_WORKERS: usize = 5;

pub enum JobManagerEvent {
	IngestJob(Arc<Library>, Box<dyn DynJob>),
	Shutdown(oneshot::Sender<()>, Arc<Jobs>),
}

#[must_use = "'job::manager::Actor::start' must be called to start the actor"]
pub struct Actor {
	jobs: Arc<Jobs>,
	internal_receiver: mpsc::UnboundedReceiver<JobManagerEvent>,
}

impl Actor {
	pub fn start(mut self, node: Arc<Node>) {
		tokio::spawn(async move {
			// FIXME: if this task crashes, the entire application is unusable
			while let Some(event) = self.internal_receiver.recv().await {
				match event {
					JobManagerEvent::IngestJob(library, job) => {
						self.jobs.clone().dispatch(&node, &library, job).await
					}
					// When the app shuts down, we need to gracefully shutdown all
					// active workers and preserve their state
					JobManagerEvent::Shutdown(signal_tx, this) => {
						info!("Shutting down job manager");
						let running_workers = this.running_workers.read().await;
						join_all(running_workers.values().map(|worker| worker.shutdown())).await;

						signal_tx.send(()).ok();
					}
				}
			}
		});
	}
}

/// JobManager handles queueing and executing jobs using the `DynJob`
/// Handling persisting JobReports to the database, pause/resuming, and
///
pub struct Jobs {
	current_jobs_hashes: RwLock<HashSet<u64>>,
	job_queue: RwLock<VecDeque<Box<dyn DynJob>>>,
	running_workers: RwLock<HashMap<Uuid, Worker>>,
	internal_sender: mpsc::UnboundedSender<JobManagerEvent>,
}

impl Jobs {
	/// Initializes the JobManager and spawns the internal event loop to listen for ingest.
	pub fn new() -> (Arc<Self>, Actor) {
		// allow the job manager to control its workers
		let (internal_sender, internal_receiver) = mpsc::unbounded_channel();
		let this = Arc::new(Self {
			current_jobs_hashes: RwLock::new(HashSet::new()),
			job_queue: RwLock::new(VecDeque::new()),
			running_workers: RwLock::new(HashMap::new()),
			internal_sender,
		});

		(
			this.clone(),
			Actor {
				jobs: this,
				internal_receiver,
			},
		)
	}

	/// Ingests a new job and dispatches it if possible, queues it otherwise.
	pub async fn ingest(
		self: Arc<Self>,
		node: &Arc<Node>,
		library: &Arc<Library>,
		job: Box<Job<impl StatefulJob>>,
	) -> Result<(), JobManagerError> {
		let job_hash = job.hash();

		if self.current_jobs_hashes.read().await.contains(&job_hash) {
			return Err(JobManagerError::AlreadyRunningJob {
				name: job.name(),
				hash: job_hash,
			});
		}

		debug!(
			"Ingesting job: <name='{}', hash='{}'>",
			job.name(),
			job_hash
		);

		self.current_jobs_hashes.write().await.insert(job_hash);
		self.dispatch(node, library, job).await;
		Ok(())
	}

	/// Dispatches a job to a worker if under MAX_WORKERS limit, queues it otherwise.
	async fn dispatch(
		self: Arc<Self>,
		node: &Arc<Node>,
		library: &Arc<Library>,
		mut job: Box<dyn DynJob>,
	) {
		let mut running_workers = self.running_workers.write().await;
		let mut job_report = job
			.report_mut()
			.take()
			.expect("critical error: missing job on worker");

		if running_workers.len() < MAX_WORKERS {
			info!("Running job: {:?}", job.name());

			let worker_id = job_report.parent_id.unwrap_or(job_report.id);

			Worker::new(
				worker_id,
				job,
				job_report,
				library.clone(),
				node.clone(),
				self.clone(),
			)
			.await
			.map_or_else(
				|e| {
					error!("Error spawning worker: {:#?}", e);
				},
				|worker| {
					running_workers.insert(worker_id, worker);
				},
			);
		} else {
			debug!(
				"Queueing job: <name='{}', hash='{}'>",
				job.name(),
				job.hash()
			);
			if let Err(e) = job_report.create(library).await {
				// It's alright to just log here, as will try to create the report on run if it wasn't created before
				error!("Error creating job report: {:#?}", e);
			}

			// Put the report back, or it will be lost forever
			*job.report_mut() = Some(job_report);

			self.job_queue.write().await.push_back(job);
		}
	}

	pub async fn complete(
		self: Arc<Self>,
		library: &Arc<Library>,
		worker_id: Uuid,
		job_hash: u64,
		next_job: Option<Box<dyn DynJob>>,
	) {
		// remove worker from running workers and from current jobs hashes
		self.current_jobs_hashes.write().await.remove(&job_hash);
		self.running_workers.write().await.remove(&worker_id);
		// continue queue
		let job = if next_job.is_some() {
			next_job
		} else {
			self.job_queue.write().await.pop_front()
		};

		if let Some(job) = job {
			// We can't directly execute `self.ingest` here because it would cause an async cycle.
			self.internal_sender
				.send(JobManagerEvent::IngestJob(library.clone(), job))
				.unwrap_or_else(|_| {
					error!("Failed to ingest job!");
				});
		}
	}

	/// Shutdown the job manager, signaled by core on shutdown.
	pub async fn shutdown(self: &Arc<Self>) {
		let (tx, rx) = oneshot::channel();
		self.internal_sender
			.send(JobManagerEvent::Shutdown(tx, self.clone()))
			.unwrap_or_else(|_| {
				error!("Failed to send shutdown event to job manager!");
			});

		rx.await.unwrap_or_else(|_| {
			error!("Failed to receive shutdown event response from job manager!");
		});
	}

	/// Pause a specific job.
	pub async fn pause(&self, job_id: Uuid) -> Result<(), JobManagerError> {
		// Look up the worker for the given job ID.
		if let Some(worker) = self.running_workers.read().await.get(&job_id) {
			debug!("Pausing job: {:#?}", worker.report());

			// Set the pause signal in the worker.
			worker.pause().await;

			Ok(())
		} else {
			Err(JobManagerError::NotFound(job_id))
		}
	}
	/// Resume a specific job.
	pub async fn resume(&self, job_id: Uuid) -> Result<(), JobManagerError> {
		// Look up the worker for the given job ID.
		if let Some(worker) = self.running_workers.read().await.get(&job_id) {
			debug!("Resuming job: {:?}", worker.report());

			// Set the pause signal in the worker.
			worker.resume().await;

			Ok(())
		} else {
			Err(JobManagerError::NotFound(job_id))
		}
	}

	/// Cancel a specific job.
	pub async fn cancel(&self, job_id: Uuid) -> Result<(), JobManagerError> {
		// Look up the worker for the given job ID.
		if let Some(worker) = self.running_workers.read().await.get(&job_id) {
			debug!("Canceling job: {:#?}", worker.report());

			// Set the cancel signal in the worker.
			worker.cancel().await;

			Ok(())
		} else {
			Err(JobManagerError::NotFound(job_id))
		}
	}

	/// This is called at startup to resume all paused jobs or jobs that were running
	/// when the core was shut down.
	/// - It will resume jobs that contain data and cancel jobs that do not.
	/// - Prevents jobs from being stuck in a paused/running state
	pub async fn cold_resume(
		self: Arc<Self>,
		node: &Arc<Node>,
		library: &Arc<Library>,
	) -> Result<(), JobManagerError> {
		// Include the Queued status in the initial find condition
		let find_condition = vec![or(vec![
			job::status::equals(Some(JobStatus::Paused as i32)),
			job::status::equals(Some(JobStatus::Running as i32)),
			job::status::equals(Some(JobStatus::Queued as i32)),
		])];

		let all_jobs = library
			.db
			.job()
			.find_many(find_condition)
			.exec()
			.await?
			.into_iter()
			.map(JobReport::try_from);

		for job in all_jobs {
			let job = job?;

			match initialize_resumable_job(job.clone(), None) {
				Ok(resumable_job) => {
					info!("Resuming job: {} with uuid {}", job.name, job.id);
					Arc::clone(&self)
						.dispatch(node, library, resumable_job)
						.await;
				}
				Err(err) => {
					warn!(
						"Failed to initialize job: {} with uuid {}, error: {:?}",
						job.name, job.id, err
					);
					info!("Cancelling job: {} with uuid {}", job.name, job.id);
					library
						.db
						.job()
						.update(
							job::id::equals(job.id.as_bytes().to_vec()),
							vec![job::status::set(Some(JobStatus::Canceled as i32))],
						)
						.exec()
						.await?;
				}
			}
		}
		Ok(())
	}

	// get all active jobs, including paused jobs organized by job id
	pub async fn get_active_reports_with_id(&self) -> HashMap<Uuid, JobReport> {
		self.running_workers
			.read()
			.await
			.values()
			.map(|worker| {
				let report = worker.report();
				(report.id, report)
			})
			.collect()
	}

	// get all running jobs, excluding paused jobs organized by action
	pub async fn get_running_reports(&self) -> HashMap<String, JobReport> {
		self.running_workers
			.read()
			.await
			.values()
			.filter(|&worker| !worker.is_paused())
			.map(|worker| {
				let report = worker.report();
				(report.get_meta().0, report)
			})
			.collect()
	}

	/// Check if the manager currently has some active workers.
	pub async fn has_active_workers(&self, library_id: Uuid) -> bool {
		for worker in self.running_workers.read().await.values() {
			if worker.library_id == library_id && !worker.is_paused() {
				return true;
			}
		}

		false
	}

	pub async fn has_job_running(&self, predicate: impl Fn(JobIdentity) -> bool) -> bool {
		for worker in self.running_workers.read().await.values() {
			if worker.who_am_i().await.map(&predicate).unwrap_or(false) {
				return true;
			}
		}
		false
	}
}

#[macro_use]
mod macros {
	macro_rules! dispatch_call_to_job_by_name {
        ($job_name:expr, T -> $call:expr, default = $default:block, jobs = [ $($job:ty),+ $(,)?]) => {{
            match $job_name {
                $(<$job as $crate::job::StatefulJob>::NAME => {
                    type T = $job;
                    $call
                },)+
                _ => $default
            }
        }};
    }
}
/// This function is used to initialize a  DynJob from a job report.
fn initialize_resumable_job(
	job_report: JobReport,
	next_jobs: Option<VecDeque<Box<dyn DynJob>>>,
) -> Result<Box<dyn DynJob>, JobError> {
	dispatch_call_to_job_by_name!(
		job_report.name.as_str(),
		T -> Job::<T>::new_from_report(job_report, next_jobs),
		default = {
			error!(
				"Unknown job type: {}, id: {}",
				job_report.name, job_report.id
			);
			Err(JobError::UnknownJobName(job_report.id, job_report.name))
		},
		jobs = [
			MediaProcessorJobInit,
			IndexerJobInit,
			FileIdentifierJobInit,
			ObjectValidatorJobInit,
			FileCutterJobInit,
			FileCopierJobInit,
			FileDeleterJobInit,
			FileEraserJobInit,
		]
	)
}



File: ./src/job/mod.rs
-------------------------------------------------
use crate::{library::Library, Node};

use std::{
	collections::{hash_map::DefaultHasher, VecDeque},
	fmt,
	hash::{Hash, Hasher},
	mem,
	pin::pin,
	sync::Arc,
	time::Instant,
};

use sd_prisma::prisma::location;

use async_channel as chan;
use futures::stream::{self, StreamExt};
use futures_concurrency::stream::Merge;
use serde::{de::DeserializeOwned, Deserialize, Serialize};
use tokio::{
	spawn,
	task::{JoinError, JoinHandle},
};
use tracing::{debug, error, info, trace, warn};
use uuid::Uuid;

mod error;
mod manager;
mod report;
mod worker;

pub use error::*;
pub use manager::*;
pub use report::*;
pub use worker::*;

pub type JobResult = Result<JobMetadata, JobError>;
pub type JobMetadata = Option<serde_json::Value>;

#[derive(Debug)]
pub struct JobIdentity {
	pub id: Uuid,
	pub name: &'static str,
	pub target_location: location::id::Type,
	pub status: JobStatus,
}

#[derive(Debug, Default)]
pub struct JobRunErrors(pub Vec<String>);

impl JobRunErrors {
	pub fn is_empty(&self) -> bool {
		self.0.is_empty()
	}
}

impl<I: IntoIterator<Item = String>> From<I> for JobRunErrors {
	fn from(errors: I) -> Self {
		Self(errors.into_iter().collect())
	}
}

impl fmt::Display for JobRunErrors {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		write!(f, "{}", self.0.join("\n"))
	}
}

pub struct JobRunOutput {
	pub metadata: JobMetadata,
	pub errors: JobRunErrors,
	pub next_job: Option<Box<dyn DynJob>>,
}

pub trait JobRunMetadata:
	Default + Serialize + DeserializeOwned + Send + Sync + fmt::Debug
{
	fn update(&mut self, new_data: Self);
}

impl JobRunMetadata for () {
	fn update(&mut self, _new_data: Self) {}
}

#[async_trait::async_trait]
pub trait StatefulJob:
	Serialize + DeserializeOwned + Hash + fmt::Debug + Send + Sync + Sized + 'static
{
	type Data: Serialize + DeserializeOwned + Send + Sync + fmt::Debug;
	type Step: Serialize + DeserializeOwned + Send + Sync + fmt::Debug;
	type RunMetadata: JobRunMetadata;

	/// The name of the job is a unique human readable identifier for the job.
	const NAME: &'static str;
	const IS_BACKGROUND: bool = false;
	const IS_BATCHED: bool = false;

	/// initialize the steps for the job
	async fn init(
		&self,
		ctx: &WorkerContext,
		data: &mut Option<Self::Data>,
	) -> Result<JobInitOutput<Self::RunMetadata, Self::Step>, JobError>;

	/// The location id where this job will act upon
	fn target_location(&self) -> location::id::Type;

	/// is called for each step in the job. These steps are created in the `Self::init` method.
	async fn execute_step(
		&self,
		ctx: &WorkerContext,
		step: CurrentStep<'_, Self::Step>,
		data: &Self::Data,
		run_metadata: &Self::RunMetadata,
	) -> Result<JobStepOutput<Self::Step, Self::RunMetadata>, JobError>;

	/// is called after all steps have been executed
	async fn finalize(
		&self,
		ctx: &WorkerContext,
		data: &Option<Self::Data>,
		run_metadata: &Self::RunMetadata,
	) -> JobResult;

	fn hash(&self) -> u64 {
		let mut s = DefaultHasher::new();
		Self::NAME.hash(&mut s);
		<Self as Hash>::hash(self, &mut s);
		s.finish()
	}
}

#[async_trait::async_trait]
pub trait DynJob: Send + Sync {
	fn id(&self) -> Uuid;
	fn parent_id(&self) -> Option<Uuid>;
	fn report(&self) -> &Option<JobReport>;
	fn report_mut(&mut self) -> &mut Option<JobReport>;
	fn name(&self) -> &'static str;
	async fn run(
		&mut self,
		ctx: WorkerContext,
		commands_rx: chan::Receiver<WorkerCommand>,
	) -> Result<JobRunOutput, JobError>;
	fn hash(&self) -> u64;
	fn set_next_jobs(&mut self, next_jobs: VecDeque<Box<dyn DynJob>>);
	fn serialize_state(&self) -> Result<Vec<u8>, JobError>;
	async fn register_children(&mut self, library: &Library) -> Result<(), JobError>;
	async fn pause_children(&mut self, library: &Library) -> Result<(), JobError>;
	async fn cancel_children(&mut self, library: &Library) -> Result<(), JobError>;
}

pub struct JobBuilder<SJob: StatefulJob> {
	id: Uuid,
	init: SJob,
	report_builder: JobReportBuilder,
}

impl<SJob: StatefulJob> JobBuilder<SJob> {
	pub fn build(self) -> Box<Job<SJob>> {
		Box::new(Job::<SJob> {
			id: self.id,
			hash: <SJob as StatefulJob>::hash(&self.init),
			report: Some(self.report_builder.build()),
			state: Some(JobState {
				init: self.init,
				data: None,
				steps: VecDeque::new(),
				step_number: 0,
				run_metadata: Default::default(),
			}),
			next_jobs: VecDeque::new(),
		})
	}

	pub fn new(init: SJob) -> Self {
		let id = Uuid::new_v4();
		Self {
			id,
			init,
			report_builder: JobReportBuilder::new(id, SJob::NAME.to_string()),
		}
	}

	pub fn with_action(mut self, action: impl AsRef<str>) -> Self {
		self.report_builder = self.report_builder.with_action(action);
		self
	}

	pub fn with_parent_id(mut self, parent_id: Uuid) -> Self {
		self.report_builder = self.report_builder.with_parent_id(parent_id);
		self
	}

	pub fn with_metadata(mut self, metadata: serde_json::Value) -> Self {
		self.report_builder = self.report_builder.with_metadata(metadata);
		self
	}
}

pub struct Job<SJob: StatefulJob> {
	id: Uuid,
	hash: u64,
	report: Option<JobReport>,
	state: Option<JobState<SJob>>,
	next_jobs: VecDeque<Box<dyn DynJob>>,
}

impl<SJob: StatefulJob> Job<SJob> {
	pub fn new(init: SJob) -> Box<Self> {
		JobBuilder::new(init).build()
	}

	pub fn queue_next<NextSJob>(mut self: Box<Self>, init: NextSJob) -> Box<Self>
	where
		NextSJob: StatefulJob + 'static,
	{
		let next_job_order = self.next_jobs.len() + 1;

		let mut child_job_builder = JobBuilder::new(init).with_parent_id(self.id);

		if let Some(parent_report) = self.report() {
			if let Some(parent_action) = &parent_report.action {
				child_job_builder =
					child_job_builder.with_action(format!("{parent_action}-{next_job_order}"));
			}
		}

		self.next_jobs.push_back(child_job_builder.build());

		self
	}

	// this function returns an ingestible job instance from a job report
	pub fn new_from_report(
		mut report: JobReport,
		next_jobs: Option<VecDeque<Box<dyn DynJob>>>,
	) -> Result<Box<dyn DynJob>, JobError> {
		let state = rmp_serde::from_slice::<JobState<SJob>>(
			&report
				.data
				.take()
				.ok_or_else(|| JobError::MissingJobDataState(report.id, report.name.clone()))?,
		)?;

		Ok(Box::new(Self {
			id: report.id,
			hash: <SJob as StatefulJob>::hash(&state.init),
			state: Some(state),
			report: Some(report),
			next_jobs: next_jobs.unwrap_or_default(),
		}))
	}

	pub async fn spawn(
		self,
		node: &Arc<Node>,
		library: &Arc<Library>,
	) -> Result<(), JobManagerError> {
		node.jobs
			.clone()
			.ingest(node, library, Box::new(self))
			.await
	}
}

#[derive(Serialize)]
pub struct JobState<Job: StatefulJob> {
	pub init: Job,
	pub data: Option<Job::Data>,
	pub steps: VecDeque<Job::Step>,
	pub step_number: usize,
	pub run_metadata: Job::RunMetadata,
}

impl<'de, Job: StatefulJob> Deserialize<'de> for JobState<Job> {
	fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
	where
		D: serde::Deserializer<'de>,
	{
		<JobStateRaw<Job, Job> as Deserialize<'de>>::deserialize::<D>(deserializer).map(|raw| {
			JobState {
				init: raw.init,
				data: raw.data,
				steps: raw.steps,
				step_number: raw.step_number,
				run_metadata: raw.run_metadata,
			}
		})
	}
}

/// This is a workaround for a serde bug.
/// Both these generics on this type should point to the same type.
///
/// https://github.com/serde-rs/serde/issues/2418
/// https://github.com/rust-lang/rust/issues/34979
#[derive(Serialize, Deserialize)]
struct JobStateRaw<Job, JobInit>
where
	Job: StatefulJob,
{
	pub init: JobInit,
	pub data: Option<Job::Data>,
	pub steps: VecDeque<Job::Step>,
	pub step_number: usize,
	pub run_metadata: Job::RunMetadata,
}

pub struct JobInitOutput<RunMetadata, Step> {
	run_metadata: RunMetadata,
	steps: VecDeque<Step>,
	errors: JobRunErrors,
}

impl<RunMetadata, Step> From<(RunMetadata, Vec<Step>)> for JobInitOutput<RunMetadata, Step> {
	fn from((run_metadata, steps): (RunMetadata, Vec<Step>)) -> Self {
		Self {
			run_metadata,
			steps: VecDeque::from(steps),
			errors: Default::default(),
		}
	}
}

impl<RunMetadata, Step> From<Vec<Step>> for JobInitOutput<RunMetadata, Step>
where
	RunMetadata: Default,
{
	fn from(steps: Vec<Step>) -> Self {
		Self {
			run_metadata: RunMetadata::default(),
			steps: VecDeque::from(steps),
			errors: Default::default(),
		}
	}
}

impl<RunMetadata, Step> From<(RunMetadata, Vec<Step>, JobRunErrors)>
	for JobInitOutput<RunMetadata, Step>
{
	fn from((run_metadata, steps, errors): (RunMetadata, Vec<Step>, JobRunErrors)) -> Self {
		Self {
			run_metadata,
			steps: VecDeque::from(steps),
			errors,
		}
	}
}

pub struct CurrentStep<'step, Step> {
	pub step: &'step Step,
	pub step_number: usize,
}

pub struct JobStepOutput<Step, RunMetadata> {
	maybe_more_steps: Option<Vec<Step>>,
	maybe_more_metadata: Option<RunMetadata>,
	errors: JobRunErrors,
}

impl<Step, RunMetadata: JobRunMetadata> From<Vec<Step>> for JobStepOutput<Step, RunMetadata> {
	fn from(more_steps: Vec<Step>) -> Self {
		Self {
			maybe_more_steps: Some(more_steps),
			maybe_more_metadata: None,
			errors: Default::default(),
		}
	}
}

impl<Step, RunMetadata: JobRunMetadata> From<RunMetadata> for JobStepOutput<Step, RunMetadata> {
	fn from(more_metadata: RunMetadata) -> Self {
		Self {
			maybe_more_steps: None,
			maybe_more_metadata: Some(more_metadata),
			errors: Default::default(),
		}
	}
}

impl<Step, RunMetadata: JobRunMetadata> From<JobRunErrors> for JobStepOutput<Step, RunMetadata> {
	fn from(errors: JobRunErrors) -> Self {
		Self {
			maybe_more_steps: None,
			maybe_more_metadata: None,
			errors,
		}
	}
}

impl<Step, RunMetadata: JobRunMetadata> From<(Vec<Step>, RunMetadata)>
	for JobStepOutput<Step, RunMetadata>
{
	fn from((more_steps, more_metadata): (Vec<Step>, RunMetadata)) -> Self {
		Self {
			maybe_more_steps: Some(more_steps),
			maybe_more_metadata: Some(more_metadata),
			errors: Default::default(),
		}
	}
}

impl<Step, RunMetadata: JobRunMetadata> From<(RunMetadata, JobRunErrors)>
	for JobStepOutput<Step, RunMetadata>
{
	fn from((more_metadata, errors): (RunMetadata, JobRunErrors)) -> Self {
		Self {
			maybe_more_steps: None,
			maybe_more_metadata: Some(more_metadata),
			errors,
		}
	}
}

impl<Step, RunMetadata: JobRunMetadata> From<(Vec<Step>, RunMetadata, JobRunErrors)>
	for JobStepOutput<Step, RunMetadata>
{
	fn from((more_steps, more_metadata, errors): (Vec<Step>, RunMetadata, JobRunErrors)) -> Self {
		Self {
			maybe_more_steps: Some(more_steps),
			maybe_more_metadata: Some(more_metadata),
			errors,
		}
	}
}

impl<Step, RunMetadata: JobRunMetadata> From<Option<()>> for JobStepOutput<Step, RunMetadata> {
	fn from(_: Option<()>) -> Self {
		Self {
			maybe_more_steps: None,
			maybe_more_metadata: None,
			errors: Vec::new().into(),
		}
	}
}

#[async_trait::async_trait]
impl<SJob: StatefulJob> DynJob for Job<SJob> {
	fn id(&self) -> Uuid {
		// SAFETY: This method is using during queueing, so we still have a report
		self.report()
			.as_ref()
			.expect("This method is using during queueing, so we still have a report")
			.id
	}

	fn parent_id(&self) -> Option<Uuid> {
		self.report.as_ref().and_then(|r| r.parent_id)
	}

	fn report(&self) -> &Option<JobReport> {
		&self.report
	}

	fn report_mut(&mut self) -> &mut Option<JobReport> {
		&mut self.report
	}

	fn name(&self) -> &'static str {
		<SJob as StatefulJob>::NAME
	}

	async fn run(
		&mut self,
		ctx: WorkerContext,
		commands_rx: chan::Receiver<WorkerCommand>,
	) -> Result<JobRunOutput, JobError> {
		let job_name = self.name();
		let job_id = self.id;
		let mut errors = vec![];
		info!("Starting Job <id='{job_id}', name='{job_name}'>");

		let JobState {
			init,
			data,
			mut steps,
			mut step_number,
			mut run_metadata,
		} = self
			.state
			.take()
			.expect("critical error: missing job state");

		let target_location = init.target_location();

		let mut stateful_job = Arc::new(init);

		let mut ctx = Arc::new(ctx);

		let mut job_should_run = true;
		let job_init_time = Instant::now();

		// Checking if we have a brand new job, or if we are resuming an old one.
		let working_data = if let Some(data) = data {
			Some(data)
		} else {
			// Job init phase
			let init_time = Instant::now();
			let init_task = {
				let ctx = Arc::clone(&ctx);
				spawn(async move {
					let mut new_data = None;
					let res = stateful_job.init(&ctx, &mut new_data).await;

					if let Ok(res) = res.as_ref() {
						if !<SJob as StatefulJob>::IS_BATCHED {
							ctx.progress(vec![JobReportUpdate::TaskCount(res.steps.len())]);
						}
					}

					(stateful_job, new_data, res)
				})
			};

			let InitPhaseOutput {
				stateful_job: returned_stateful_job,
				maybe_data,
				output,
			} = handle_init_phase::<SJob>(
				JobRunWorkTable {
					id: job_id,
					name: job_name,
					init_time,
					target_location,
				},
				Arc::clone(&ctx),
				init_task,
				commands_rx.clone(),
			)
			.await?;

			stateful_job = returned_stateful_job;

			match output {
				Ok(JobInitOutput {
					run_metadata: new_run_metadata,
					steps: new_steps,
					errors: JobRunErrors(new_errors),
				}) => {
					steps = new_steps;
					errors.extend(new_errors);
					run_metadata.update(new_run_metadata);
				}
				Err(e @ JobError::EarlyFinish { .. }) => {
					info!("{e}");
					job_should_run = false;
				}
				Err(e) => return Err(e),
			}

			maybe_data
		};

		// Run the job until it's done or we get a command
		let data = if let Some(working_data) = working_data {
			let mut working_data_arc = Arc::new(working_data);

			// Job run phase
			while job_should_run && !steps.is_empty() {
				let steps_len: usize = steps.len();

				let mut run_metadata_arc = Arc::new(run_metadata);
				let step = Arc::new(steps.pop_front().expect("just checked that we have steps"));

				let init_time = Instant::now();

				// JoinHandle<Result<JobStepOutput<SJob::Step, SJob::RunMetadata>, JobError>>
				let step_task = {
					// Need these bunch of Arcs to be able to move them into the async block of tokio::spawn
					let ctx = Arc::clone(&ctx);
					let run_metadata = Arc::clone(&run_metadata_arc);
					let working_data = Arc::clone(&working_data_arc);
					let step = Arc::clone(&step);
					let stateful_job = Arc::clone(&stateful_job);
					spawn(async move {
						stateful_job
							.execute_step(
								&ctx,
								CurrentStep {
									step: &step,
									step_number,
								},
								&working_data,
								&run_metadata,
							)
							.await
					})
				};

				let JobStepsPhaseOutput {
					steps: returned_steps,
					output,
					step_arcs:
						(
							returned_ctx,
							returned_run_metadata_arc,
							returned_working_data_arc,
							returned_stateful_job,
						),
				} = handle_single_step::<SJob>(
					JobRunWorkTable {
						id: job_id,
						name: job_name,
						init_time,
						target_location,
					},
					&job_init_time,
					(
						// Must not hold extra references here; moving and getting back on function completion
						ctx,
						run_metadata_arc,
						working_data_arc,
						stateful_job,
					),
					JobStepDataWorkTable {
						step_number,
						steps,
						step,
						step_task,
					},
					commands_rx.clone(),
				)
				.await?;

				steps = returned_steps;
				ctx = returned_ctx;
				run_metadata_arc = returned_run_metadata_arc;
				working_data_arc = returned_working_data_arc;
				stateful_job = returned_stateful_job;

				run_metadata =
					Arc::try_unwrap(run_metadata_arc).expect("step already ran, no more refs");

				match output {
					Ok(JobStepOutput {
						maybe_more_steps,
						maybe_more_metadata,
						errors: JobRunErrors(new_errors),
					}) => {
						let mut events = vec![JobReportUpdate::CompletedTaskCount(step_number + 1)];

						if let Some(more_steps) = maybe_more_steps {
							events.push(JobReportUpdate::TaskCount(steps_len + more_steps.len()));

							steps.extend(more_steps);
						}

						if let Some(more_metadata) = maybe_more_metadata {
							run_metadata.update(more_metadata);
						}

						if !<SJob as StatefulJob>::IS_BATCHED {
							ctx.progress(events);
						}

						if !new_errors.is_empty() {
							warn!("Job<id='{job_id}', name='{job_name}'> had a step with errors");
							new_errors.iter().for_each(|err| {
								warn!("Job<id='{job_id}', name='{job_name}'> error: {:?}", err);
							});

							errors.extend(new_errors);
						}
					}
					Err(e @ JobError::EarlyFinish { .. }) => {
						info!("{e}");
						break;
					}
					Err(e) => return Err(e),
				}
				// remove the step from the queue
				step_number += 1;
			}

			debug!(
				"Total job run time {:?} Job <id='{job_id}', name='{job_name}'>",
				job_init_time.elapsed()
			);

			Some(Arc::try_unwrap(working_data_arc).expect("job already ran, no more refs"))
		} else {
			warn!("Tried to run a job without data Job <id='{job_id}', name='{job_name}'>");
			None
		};

		let metadata = stateful_job.finalize(&ctx, &data, &run_metadata).await?;

		let mut next_jobs = mem::take(&mut self.next_jobs);

		Ok(JobRunOutput {
			metadata,
			errors: errors.into(),
			next_job: next_jobs.pop_front().map(|mut next_job| {
				debug!(
					"Job<id='{job_id}', name='{job_name}'> requesting to spawn '{}' now that it's complete!",
					next_job.name()
				);
				next_job.set_next_jobs(next_jobs);

				next_job
			}),
		})
	}

	fn hash(&self) -> u64 {
		self.hash
	}

	fn set_next_jobs(&mut self, next_jobs: VecDeque<Box<dyn DynJob>>) {
		self.next_jobs = next_jobs;
	}

	fn serialize_state(&self) -> Result<Vec<u8>, JobError> {
		rmp_serde::to_vec_named(&self.state).map_err(Into::into)
	}

	async fn register_children(&mut self, library: &Library) -> Result<(), JobError> {
		for next_job in self.next_jobs.iter_mut() {
			if let Some(next_job_report) = next_job.report_mut() {
				if next_job_report.created_at.is_none() {
					next_job_report.create(library).await?
				}
			} else {
				return Err(JobError::MissingReport {
					id: next_job.id(),
					name: next_job.name().to_string(),
				});
			}
		}

		Ok(())
	}

	async fn pause_children(&mut self, library: &Library) -> Result<(), JobError> {
		for next_job in self.next_jobs.iter_mut() {
			let state = next_job.serialize_state()?;
			if let Some(next_job_report) = next_job.report_mut() {
				next_job_report.status = JobStatus::Paused;
				next_job_report.data = Some(state);
				next_job_report.update(library).await?;
			} else {
				return Err(JobError::MissingReport {
					id: next_job.id(),
					name: next_job.name().to_string(),
				});
			}
		}

		Ok(())
	}

	async fn cancel_children(&mut self, library: &Library) -> Result<(), JobError> {
		for next_job in self.next_jobs.iter_mut() {
			let state = next_job.serialize_state()?;
			if let Some(next_job_report) = next_job.report_mut() {
				next_job_report.status = JobStatus::Canceled;
				next_job_report.data = Some(state);
				next_job_report.update(library).await?;
			} else {
				return Err(JobError::MissingReport {
					id: next_job.id(),
					name: next_job.name().to_string(),
				});
			}
		}

		Ok(())
	}
}

struct InitPhaseOutput<SJob: StatefulJob> {
	stateful_job: Arc<SJob>,
	maybe_data: Option<SJob::Data>,
	output: Result<JobInitOutput<SJob::RunMetadata, SJob::Step>, JobError>,
}

struct JobRunWorkTable {
	id: Uuid,
	name: &'static str,
	init_time: Instant,
	target_location: location::id::Type,
}

type InitTaskOutput<SJob> = (
	Arc<SJob>,
	Option<<SJob as StatefulJob>::Data>,
	Result<
		JobInitOutput<<SJob as StatefulJob>::RunMetadata, <SJob as StatefulJob>::Step>,
		JobError,
	>,
);

#[inline]
async fn handle_init_phase<SJob: StatefulJob>(
	JobRunWorkTable {
		id,
		name,
		init_time,
		target_location,
	}: JobRunWorkTable,
	worker_ctx: Arc<WorkerContext>,
	init_task: JoinHandle<InitTaskOutput<SJob>>,
	mut commands_rx: chan::Receiver<WorkerCommand>,
) -> Result<InitPhaseOutput<SJob>, JobError> {
	enum StreamMessage<SJob: StatefulJob> {
		NewCommand(WorkerCommand),
		InitResult(Result<InitTaskOutput<SJob>, JoinError>),
	}

	let mut status = JobStatus::Running;

	let init_abort_handle = init_task.abort_handle();

	let mut msg_stream = pin!((
		stream::once(init_task).map(StreamMessage::<SJob>::InitResult),
		commands_rx.clone().map(StreamMessage::<SJob>::NewCommand),
	)
		.merge());

	let mut commands_rx = pin!(commands_rx);

	'messages: while let Some(msg) = msg_stream.next().await {
		match msg {
			StreamMessage::InitResult(Err(join_error)) => {
				error!(
					"Job <id='{id}', name='{name}'> \
							 failed to initialize due to an internal error: {join_error:#?}",
				);
				return Err(join_error.into());
			}
			StreamMessage::InitResult(Ok((stateful_job, maybe_data, output))) => {
				debug!(
					"Init phase took {:?} Job <id='{id}', name='{name}'>",
					init_time.elapsed()
				);

				return Ok(InitPhaseOutput {
					stateful_job,
					maybe_data,
					output,
				});
			}
			StreamMessage::NewCommand(WorkerCommand::IdentifyYourself(tx)) => {
				if tx
					.send(JobIdentity {
						id,
						name,
						target_location,
						status,
					})
					.is_err()
				{
					warn!("Failed to send IdentifyYourself event reply");
				}
			}
			StreamMessage::NewCommand(WorkerCommand::Pause(when)) => {
				debug!(
					"Pausing Job at init phase <id='{id}', name='{name}'> took {:?}",
					when.elapsed()
				);

				// Notify the worker's work task that now we're paused
				worker_ctx.pause();

				status = JobStatus::Paused;

				// In case of a Pause command, we keep waiting for the next command
				let paused_time = Instant::now();
				while let Some(command) = commands_rx.next().await {
					match command {
						WorkerCommand::IdentifyYourself(tx) => {
							if tx
								.send(JobIdentity {
									id,
									name,
									target_location,
									status,
								})
								.is_err()
							{
								warn!("Failed to send IdentifyYourself event reply");
							}
						}
						WorkerCommand::Resume(when) => {
							debug!(
								"Resuming Job at init phase <id='{id}', name='{name}'> took {:?}",
								when.elapsed()
							);
							debug!(
								"Total paused time {:?} Job <id='{id}', name='{name}'>",
								paused_time.elapsed()
							);
							status = JobStatus::Running;

							continue 'messages;
						}
						// The job can also be shutdown or canceled while paused
						WorkerCommand::Shutdown(when, signal_tx) => {
							init_abort_handle.abort();

							debug!(
								"Shuting down Job at init phase <id='{id}', name='{name}'> \
									took {:?} after running for {:?}",
								when.elapsed(),
								init_time.elapsed(),
							);
							debug!("Total paused time {:?}", paused_time.elapsed());

							// Shutting down at init phase will abort the job
							return Err(JobError::Canceled(signal_tx));
						}
						WorkerCommand::Cancel(when, signal_tx) => {
							init_abort_handle.abort();
							debug!(
								"Canceling Job at init phase <id='{id}', name='{name}'> \
									took {:?} after running for {:?}",
								when.elapsed(),
								init_time.elapsed(),
							);
							debug!(
								"Total paused time {:?} Job <id='{id}', name='{name}'>",
								paused_time.elapsed()
							);
							return Err(JobError::Canceled(signal_tx));
						}
						WorkerCommand::Pause(_) => {
							// We continue paused lol
						}
						WorkerCommand::Timeout(elapsed, tx) => {
							error!(
								"Job <id='{id}', name='{name}'> \
								timed out at init phase after {elapsed:?} without updates"
							);
							tx.send(()).ok();
							return Err(JobError::Timeout(elapsed));
						}
					}
				}

				if commands_rx.is_closed() {
					error!(
						"Job <id='{id}', name='{name}'> \
						closed the command channel while paused"
					);
					return Err(JobError::Critical(
						"worker command channel closed while job was paused",
					));
				}
			}
			StreamMessage::NewCommand(WorkerCommand::Resume(_)) => {
				// We're already running so we just ignore this command
			}
			StreamMessage::NewCommand(WorkerCommand::Shutdown(when, signal_tx)) => {
				init_abort_handle.abort();

				debug!(
					"Shuting down at init phase Job <id='{id}', name='{name}'> took {:?} \
					after running for {:?}",
					when.elapsed(),
					init_time.elapsed(),
				);

				// Shutting down at init phase will abort the job
				return Err(JobError::Canceled(signal_tx));
			}
			StreamMessage::NewCommand(WorkerCommand::Cancel(when, signal_tx)) => {
				init_abort_handle.abort();

				debug!(
					"Canceling at init phase Job <id='{id}', name='{name}'> took {:?} \
					after running for {:?}",
					when.elapsed(),
					init_time.elapsed()
				);

				return Err(JobError::Canceled(signal_tx));
			}
			StreamMessage::NewCommand(WorkerCommand::Timeout(elapsed, tx)) => {
				error!(
					"Job <id='{id}', name='{name}'> \
					timed out at init phase after {elapsed:?} without updates"
				);
				tx.send(()).ok();
				return Err(JobError::Timeout(elapsed));
			}
		}
	}

	Err(JobError::Critical("unexpect job init end without result"))
}

type StepTaskOutput<SJob> = Result<
	JobStepOutput<<SJob as StatefulJob>::Step, <SJob as StatefulJob>::RunMetadata>,
	JobError,
>;

struct JobStepDataWorkTable<SJob: StatefulJob> {
	step_number: usize,
	steps: VecDeque<SJob::Step>,
	step: Arc<SJob::Step>,
	step_task: JoinHandle<StepTaskOutput<SJob>>,
}

struct JobStepsPhaseOutput<SJob: StatefulJob> {
	steps: VecDeque<SJob::Step>,
	output: StepTaskOutput<SJob>,
	step_arcs: StepArcs<SJob>,
}

type StepArcs<SJob> = (
	Arc<WorkerContext>,
	Arc<<SJob as StatefulJob>::RunMetadata>,
	Arc<<SJob as StatefulJob>::Data>,
	Arc<SJob>,
);

#[inline]
async fn handle_single_step<SJob: StatefulJob>(
	JobRunWorkTable {
		id,
		name,
		init_time,
		target_location,
	}: JobRunWorkTable,
	job_init_time: &Instant,
	(worker_ctx, run_metadata, working_data, stateful_job): StepArcs<SJob>,
	JobStepDataWorkTable {
		step_number,
		mut steps,
		step,
		mut step_task,
	}: JobStepDataWorkTable<SJob>,
	mut commands_rx: chan::Receiver<WorkerCommand>,
) -> Result<JobStepsPhaseOutput<SJob>, JobError> {
	enum StreamMessage<SJob: StatefulJob> {
		NewCommand(WorkerCommand),
		StepResult(Result<StepTaskOutput<SJob>, JoinError>),
	}

	let mut status = JobStatus::Running;

	let mut msg_stream = pin!((
		stream::once(&mut step_task).map(StreamMessage::<SJob>::StepResult),
		commands_rx.clone().map(StreamMessage::<SJob>::NewCommand),
	)
		.merge());

	let mut commands_rx = pin!(commands_rx);

	'messages: while let Some(msg) = msg_stream.next().await {
		match msg {
			StreamMessage::StepResult(Err(join_error)) => {
				error!(
					"Job <id='{id}', name='{name}'> \
					failed to run step #{step_number} due to an internal error: {join_error:#?}",
				);
				return Err(join_error.into());
			}
			StreamMessage::StepResult(Ok(output)) => {
				trace!(
					"Step finished in {:?} Job <id='{id}', name='{name}'>",
					init_time.elapsed(),
				);

				return Ok(JobStepsPhaseOutput {
					steps,
					output,
					step_arcs: (worker_ctx, run_metadata, working_data, stateful_job),
				});
			}
			StreamMessage::NewCommand(WorkerCommand::IdentifyYourself(tx)) => {
				if tx
					.send(JobIdentity {
						id,
						name,
						target_location,
						status,
					})
					.is_err()
				{
					warn!("Failed to send IdentifyYourself event reply");
				}
			}
			StreamMessage::NewCommand(WorkerCommand::Pause(when)) => {
				debug!(
					"Pausing Job <id='{id}', name='{name}'> took {:?}",
					when.elapsed()
				);

				worker_ctx.pause();

				status = JobStatus::Paused;

				// In case of a Pause command, we keep waiting for the next command
				let paused_time = Instant::now();
				while let Some(command) = commands_rx.next().await {
					match command {
						WorkerCommand::IdentifyYourself(tx) => {
							if tx
								.send(JobIdentity {
									id,
									name,
									target_location,
									status,
								})
								.is_err()
							{
								warn!("Failed to send IdentifyYourself event reply");
							}
						}
						WorkerCommand::Resume(when) => {
							debug!(
								"Resuming Job <id='{id}', name='{name}'> took {:?}",
								when.elapsed(),
							);
							debug!(
								"Total paused time {:?} Job <id='{id}', name='{name}'>",
								paused_time.elapsed(),
							);
							status = JobStatus::Running;

							continue 'messages;
						}
						// The job can also be shutdown or canceled while paused
						WorkerCommand::Shutdown(when, signal_tx) => {
							step_task.abort();
							let _ = step_task.await;

							debug!(
								"Shuting down Job <id='{id}', name='{name}'> took {:?} \
								after running for {:?}",
								when.elapsed(),
								job_init_time.elapsed(),
							);
							debug!(
								"Total paused time {:?} Job <id='{id}', name='{name}'>",
								paused_time.elapsed(),
							);

							// Taking back the last step, so it can run to completion later
							steps.push_front(
								Arc::try_unwrap(step).expect("step already ran, no more refs"),
							);

							return Err(JobError::Paused(
								rmp_serde::to_vec_named(&JobState::<SJob> {
									init: Arc::try_unwrap(stateful_job)
										.expect("handle abort already ran, no more refs"),
									data: Some(
										Arc::try_unwrap(working_data)
											.expect("handle abort already ran, no more refs"),
									),
									steps,
									step_number,
									run_metadata: Arc::try_unwrap(run_metadata)
										.expect("handle abort already ran, no more refs"),
								})?,
								signal_tx,
							));
						}
						WorkerCommand::Cancel(when, signal_tx) => {
							step_task.abort();
							let _ = step_task.await;
							debug!(
								"Canceling Job <id='{id}', name='{name}'> \
								took {:?} after running for {:?}",
								when.elapsed(),
								job_init_time.elapsed(),
							);
							debug!(
								"Total paused time {:?} Job <id='{id}', name='{name}'>",
								paused_time.elapsed(),
							);
							return Err(JobError::Canceled(signal_tx));
						}
						WorkerCommand::Pause(_) => {
							// We continue paused lol
						}

						WorkerCommand::Timeout(elapsed, tx) => {
							error!(
								"Job <id='{id}', name='{name}'> \
								timed out at step #{step_number} after {elapsed:?} without updates"
							);
							tx.send(()).ok();
							return Err(JobError::Timeout(elapsed));
						}
					}
				}

				if commands_rx.is_closed() {
					error!(
						"Job <id='{id}', name='{name}'> \
						closed the command channel while paused"
					);
					return Err(JobError::Critical(
						"worker command channel closed while job was paused",
					));
				}
			}
			StreamMessage::NewCommand(WorkerCommand::Resume(_)) => {
				// We're already running so we just ignore this command
			}
			StreamMessage::NewCommand(WorkerCommand::Shutdown(when, signal_tx)) => {
				step_task.abort();
				let _ = step_task.await;

				debug!(
					"Shuting down Job <id='{id}', name='{name}'> took {:?} \
					after running for {:?}",
					when.elapsed(),
					job_init_time.elapsed(),
				);

				// Taking back the last step, so it can run to completion later
				steps.push_front(
					Arc::try_unwrap(step).expect("handle abort already ran, no more refs"),
				);

				return Err(JobError::Paused(
					rmp_serde::to_vec_named(&JobState::<SJob> {
						init: Arc::try_unwrap(stateful_job)
							.expect("handle abort already ran, no more refs"),
						data: Some(
							Arc::try_unwrap(working_data)
								.expect("handle abort already ran, no more refs"),
						),
						steps,
						step_number,
						run_metadata: Arc::try_unwrap(run_metadata)
							.expect("step already ran, no more refs"),
					})?,
					signal_tx,
				));
			}
			StreamMessage::NewCommand(WorkerCommand::Cancel(when, signal_tx)) => {
				step_task.abort();
				let _ = step_task.await;
				debug!(
					"Canceling Job <id='{id}', name='{name}'> took {:?} \
										 after running for {:?}",
					when.elapsed(),
					job_init_time.elapsed(),
				);
				return Err(JobError::Canceled(signal_tx));
			}
			StreamMessage::NewCommand(WorkerCommand::Timeout(elapsed, tx)) => {
				error!(
					"Job <id='{id}', name='{name}'> \
					timed out at step #{step_number} after {elapsed:?} without updates"
				);
				tx.send(()).ok();
				return Err(JobError::Timeout(elapsed));
			}
		}
	}

	Err(JobError::Critical("unexpect job step end without result"))
}



File: ./src/api/keys.rs
-------------------------------------------------
// use rspc::alpha::AlphaRouter;
// use rspc::ErrorCode;
// use sd_crypto::keys::keymanager::{StoredKey, StoredKeyType};
// use sd_crypto::primitives::SECRET_KEY_IDENTIFIER;
// use sd_crypto::types::{Algorithm, HashingAlgorithm, OnboardingConfig, SecretKeyString};
// use sd_crypto::{Error, Protected};
// use serde::Deserialize;
// use specta::Type;
// use std::{path::PathBuf, str::FromStr};
// use tokio::fs::File;
// use tokio::io::{AsyncReadExt, AsyncWriteExt};
// use uuid::Uuid;

// use crate::util::db::write_storedkey_to_db;
// use crate::{invalidate_query, prisma::key};

// use super::utils::library;
// use super::{Ctx, R};

// #[derive(Type, Deserialize)]
// pub struct KeyAddArgs {
// 	algorithm: Algorithm,
// 	hashing_algorithm: HashingAlgorithm,
// 	key: Protected<String>,
// 	library_sync: bool,
// 	automount: bool,
// }

// #[derive(Type, Deserialize)]
// pub struct UnlockKeyManagerArgs {
// 	password: Protected<String>,
// 	secret_key: Protected<String>,
// }

// #[derive(Type, Deserialize)]
// pub struct RestoreBackupArgs {
// 	password: Protected<String>,
// 	secret_key: Protected<String>,
// 	path: PathBuf,
// }

// #[derive(Type, Deserialize)]
// pub struct MasterPasswordChangeArgs {
// 	password: Protected<String>,
// 	algorithm: Algorithm,
// 	hashing_algorithm: HashingAlgorithm,
// }

// #[derive(Type, Deserialize)]
// pub struct AutomountUpdateArgs {
// 	uuid: Uuid,
// 	status: bool,
// }

// pub(crate) fn mount() -> AlphaRouter<Ctx> {
// 	R.router()
// 		.procedure("list", {
// 			R.with2(library())
// 				.query(|(_, library), _: ()| async move { Ok(library.key_manager.dump_keystore()) })
// 		})
// 		// do not unlock the key manager until this route returns true
// 		.procedure("isUnlocked", {
// 			R.with2(library()).query(|(_, library), _: ()| async move {
// 				Ok(library.key_manager.is_unlocked().await)
// 			})
// 		})
// 		.procedure("isSetup", {
// 			R.with2(library()).query(|(_, library), _: ()| async move {
// 				Ok(!library.db.key().find_many(vec![]).exec().await?.is_empty())
// 			})
// 		})
// 		.procedure("setup", {
// 			R.with2(library())
// 				.mutation(|(_, library), config: OnboardingConfig| async move {
// 					let root_key = library.key_manager.onboarding(config, library.id).await?;
// 					write_storedkey_to_db(&library.db, &root_key).await?;
// 					library
// 						.key_manager
// 						.populate_keystore(vec![root_key])
// 						.await?;

// 					invalidate_query!(library, "keys.isSetup");
// 					invalidate_query!(library, "keys.isUnlocked");

// 					Ok(())
// 				})
// 		})
// 		// this is so we can show the key as mounted in the UI
// 		.procedure("listMounted", {
// 			R.with2(library()).query(|(_, library), _: ()| async move {
// 				Ok(library.key_manager.get_mounted_uuids())
// 			})
// 		})
// 		.procedure("getKey", {
// 			R.with2(library())
// 				.query(|(_, library), key_uuid: Uuid| async move {
// 					Ok(library
// 						.key_manager
// 						.get_key(key_uuid)
// 						.await?
// 						.expose()
// 						.clone())
// 				})
// 		})
// 		.procedure("mount", {
// 			R.with2(library())
// 				.mutation(|(_, library), key_uuid: Uuid| async move {
// 					library.key_manager.mount(key_uuid).await?;
// 					// we also need to dispatch jobs that automatically decrypt preview media and metadata here
// 					invalidate_query!(library, "keys.listMounted");
// 					Ok(())
// 				})
// 		})
// 		.procedure("getSecretKey", {
// 			R.with2(library()).query(|(_, library), _: ()| async move {
// 				if library
// 					.key_manager
// 					.keyring_contains_valid_secret_key(library.id)
// 					.await
// 					.is_ok()
// 				{
// 					Ok(Some(
// 						library
// 							.key_manager
// 							.keyring_retrieve(library.id, SECRET_KEY_IDENTIFIER.to_string())
// 							.await?
// 							.expose()
// 							.clone(),
// 					))
// 				} else {
// 					Ok(None)
// 				}
// 			})
// 		})
// 		.procedure("unmount", {
// 			R.with2(library())
// 				.mutation(|(_, library), key_uuid: Uuid| async move {
// 					library.key_manager.unmount(key_uuid)?;
// 					// we also need to delete all in-memory decrypted data associated with this key
// 					invalidate_query!(library, "keys.listMounted");
// 					Ok(())
// 				})
// 		})
// 		.procedure("clearMasterPassword", {
// 			R.with2(library())
// 				.mutation(|(_, library), _: ()| async move {
// 					// This technically clears the root key, but it means the same thing to the frontend
// 					library.key_manager.clear_root_key().await?;

// 					invalidate_query!(library, "keys.isUnlocked");
// 					Ok(())
// 				})
// 		})
// 		.procedure("syncKeyToLibrary", {
// 			R.with2(library())
// 				.mutation(|(_, library), key_uuid: Uuid| async move {
// 					let key = library.key_manager.sync_to_database(key_uuid).await?;

// 					// does not check that the key doesn't exist before writing
// 					write_storedkey_to_db(&library.db, &key).await?;

// 					invalidate_query!(library, "keys.list");
// 					Ok(())
// 				})
// 		})
// 		.procedure("updateAutomountStatus", {
// 			R.with2(library())
// 				.mutation(|(_, library), args: AutomountUpdateArgs| async move {
// 					if !library.key_manager.is_memory_only(args.uuid).await? {
// 						library
// 							.key_manager
// 							.change_automount_status(args.uuid, args.status)
// 							.await?;

// 						library
// 							.db
// 							.key()
// 							.update(
// 								key::uuid::equals(args.uuid.to_string()),
// 								vec![key::automount::set(args.status)],
// 							)
// 							.exec()
// 							.await?;

// 						invalidate_query!(library, "keys.list");
// 					}

// 					Ok(())
// 				})
// 		})
// 		.procedure("deleteFromLibrary", {
// 			R.with2(library())
// 				.mutation(|(_, library), key_uuid: Uuid| async move {
// 					if !library.key_manager.is_memory_only(key_uuid).await? {
// 						library
// 							.db
// 							.key()
// 							.delete(key::uuid::equals(key_uuid.to_string()))
// 							.exec()
// 							.await?;
// 					}

// 					library.key_manager.remove_key(key_uuid).await?;

// 					// we also need to delete all in-memory decrypted data associated with this key
// 					invalidate_query!(library, "keys.list");
// 					invalidate_query!(library, "keys.listMounted");
// 					invalidate_query!(library, "keys.getDefault");
// 					Ok(())
// 				})
// 		})
// 		.procedure("unlockKeyManager", {
// 			R.with2(library())
// 				.mutation(|(_, library), args: UnlockKeyManagerArgs| async move {
// 					let secret_key =
// 						(!args.secret_key.expose().is_empty()).then_some(args.secret_key);

// 					library
// 						.key_manager
// 						.unlock(
// 							args.password,
// 							secret_key.map(SecretKeyString),
// 							library.id,
// 							|| invalidate_query!(library, "keys.isKeyManagerUnlocking"),
// 						)
// 						.await?;

// 					invalidate_query!(library, "keys.isUnlocked");

// 					let automount = library
// 						.db
// 						.key()
// 						.find_many(vec![key::automount::equals(true)])
// 						.exec()
// 						.await?;

// 					for key in automount {
// 						library
// 							.key_manager
// 							.mount(Uuid::from_str(&key.uuid).map_err(|_| Error::Serialization)?)
// 							.await?;

// 						invalidate_query!(library, "keys.listMounted");
// 					}

// 					Ok(())
// 				})
// 		})
// 		.procedure("setDefault", {
// 			R.with2(library())
// 				.mutation(|(_, library), key_uuid: Uuid| async move {
// 					library.key_manager.set_default(key_uuid).await?;

// 					library
// 						.db
// 						.key()
// 						.update_many(
// 							vec![key::default::equals(true)],
// 							vec![key::default::set(false)],
// 						)
// 						.exec()
// 						.await?;

// 					library
// 						.db
// 						.key()
// 						.update(
// 							key::uuid::equals(key_uuid.to_string()),
// 							vec![key::default::set(true)],
// 						)
// 						.exec()
// 						.await?;

// 					invalidate_query!(library, "keys.getDefault");
// 					Ok(())
// 				})
// 		})
// 		.procedure("getDefault", {
// 			R.with2(library()).query(|(_, library), _: ()| async move {
// 				library.key_manager.get_default().await.ok()
// 			})
// 		})
// 		.procedure("isKeyManagerUnlocking", {
// 			R.with2(library()).query(|(_, library), _: ()| async move {
// 				library.key_manager.is_unlocking().await.ok()
// 			})
// 		})
// 		.procedure("unmountAll", {
// 			R.with2(library())
// 				.mutation(|(_, library), _: ()| async move {
// 					library.key_manager.empty_keymount();
// 					invalidate_query!(library, "keys.listMounted");
// 					Ok(())
// 				})
// 		})
// 		.procedure("add", {
// 			// this also mounts the key
// 			R.with2(library())
// 				.mutation(|(_, library), args: KeyAddArgs| async move {
// 					// register the key with the keymanager
// 					let uuid = library
// 						.key_manager
// 						.add_to_keystore(
// 							args.key,
// 							args.algorithm,
// 							args.hashing_algorithm,
// 							!args.library_sync,
// 							args.automount,
// 							None,
// 						)
// 						.await?;

// 					if args.library_sync {
// 						write_storedkey_to_db(
// 							&library.db,
// 							&library.key_manager.access_keystore(uuid).await?,
// 						)
// 						.await?;

// 						if args.automount {
// 							library
// 								.db
// 								.key()
// 								.update(
// 									key::uuid::equals(uuid.to_string()),
// 									vec![key::automount::set(true)],
// 								)
// 								.exec()
// 								.await?;
// 						}
// 					}

// 					library.key_manager.mount(uuid).await?;

// 					invalidate_query!(library, "keys.list");
// 					invalidate_query!(library, "keys.listMounted");
// 					Ok(())
// 				})
// 		})
// 		.procedure("backupKeystore", {
// 			R.with2(library())
// 				.mutation(|(_, library), path: PathBuf| async move {
// 					// dump all stored keys that are in the key manager (maybe these should be taken from prisma as this will include even "non-sync with library" keys)
// 					let mut stored_keys = library.key_manager.dump_keystore();

// 					// include the verification key at the time of backup
// 					stored_keys.push(library.key_manager.get_verification_key().await?);

// 					// exclude all memory-only keys
// 					stored_keys.retain(|k| !k.memory_only);

// 					let mut output_file = File::create(path).await.map_err(Error::Io)?;
// 					output_file
// 						.write_all(
// 							&serde_json::to_vec(&stored_keys).map_err(|_| Error::Serialization)?,
// 						)
// 						.await
// 						.map_err(Error::Io)?;
// 					Ok(())
// 				})
// 		})
// 		.procedure("restoreKeystore", {
// 			R.with2(library())
// 				.mutation(|(_, library), args: RestoreBackupArgs| async move {
// 					let mut input_file = File::open(args.path).await.map_err(Error::Io)?;

// 					let mut backup = Vec::new();

// 					input_file
// 						.read_to_end(&mut backup)
// 						.await
// 						.map_err(Error::Io)?;

// 					let stored_keys: Vec<StoredKey> =
// 						serde_json::from_slice(&backup).map_err(|_| Error::Serialization)?;

// 					let updated_keys = library
// 						.key_manager
// 						.import_keystore_backup(
// 							args.password,
// 							SecretKeyString(args.secret_key),
// 							&stored_keys,
// 						)
// 						.await?;

// 					for key in &updated_keys {
// 						write_storedkey_to_db(&library.db, key).await?;
// 					}

// 					invalidate_query!(library, "keys.list");
// 					invalidate_query!(library, "keys.listMounted");

// 					TryInto::<u32>::try_into(updated_keys.len()).map_err(|_| {
// 						rspc::Error::new(ErrorCode::InternalServerError, "integer overflow".into())
// 					}) // We convert from `usize` (bigint type) to `u32` (number type) because rspc doesn't support bigints.
// 				})
// 		})
// 		.procedure(
// 			"changeMasterPassword",
// 			#[allow(clippy::unwrap_used)] // TODO: Jake is fixing this in a Crypto PR
// 			{
// 				R.with2(library()).mutation(
// 					|(_, library), args: MasterPasswordChangeArgs| async move {
// 						let verification_key = library
// 							.key_manager
// 							.change_master_password(
// 								args.password,
// 								args.algorithm,
// 								args.hashing_algorithm,
// 								library.id,
// 							)
// 							.await?;

// 						invalidate_query!(library, "keys.getSecretKey");

// 						// remove old root key if present
// 						library
// 							.db
// 							.key()
// 							.delete_many(vec![key::key_type::equals(
// 								serde_json::to_string(&StoredKeyType::Root).unwrap(),
// 							)])
// 							.exec()
// 							.await?;

// 						// write the new verification key
// 						write_storedkey_to_db(&library.db, &verification_key).await?;

// 						Ok(())
// 					},
// 				)
// 			},
// 		)
// }



File: ./src/api/preferences.rs
-------------------------------------------------
use rspc::alpha::AlphaRouter;

use super::{utils::library, Ctx, R};
use crate::preferences::LibraryPreferences;

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("update", {
			R.with2(library())
				.mutation(|(_, library), args: LibraryPreferences| async move {
					args.write(&library.db).await?;

					Ok(())
				})
		})
		.procedure("get", {
			R.with2(library()).query(|(_, library), _: ()| async move {
				Ok(LibraryPreferences::read(&library.db).await?)
			})
		})
}



File: ./src/api/volumes.rs
-------------------------------------------------
use rspc::alpha::AlphaRouter;

use crate::volume::get_volumes;

use super::{Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router().procedure("list", {
		R.query(|_, _: ()| async move { Ok(get_volumes().await) })
	})
}



File: ./src/api/libraries.rs
-------------------------------------------------
use crate::{
	library::{Library, LibraryConfig, LibraryName},
	location::{scan_location, LocationCreateArgs},
	util::MaybeUndefined,
	volume::get_volumes,
	Node,
};

use sd_p2p::spacetunnel::RemoteIdentity;
use sd_prisma::prisma::{indexer_rule, statistics};

use std::{convert::identity, sync::Arc};

use chrono::Utc;
use directories::UserDirs;
use futures_concurrency::future::Join;
use rspc::{alpha::AlphaRouter, ErrorCode};
use serde::{Deserialize, Serialize};
use specta::Type;
use tokio::spawn;
use tracing::{debug, error};
use uuid::Uuid;

use super::{
	utils::{get_size, library},
	Ctx, R,
};

// TODO(@Oscar): Replace with `specta::json`
#[derive(Serialize, Type)]
pub struct LibraryConfigWrapped {
	pub uuid: Uuid,
	pub instance_id: Uuid,
	pub instance_public_key: RemoteIdentity,
	pub config: LibraryConfig,
}

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("list", {
			R.query(|node, _: ()| async move {
				node.libraries
					.get_all()
					.await
					.into_iter()
					.map(|lib| async move {
						LibraryConfigWrapped {
							uuid: lib.id,
							instance_id: lib.instance_uuid,
							instance_public_key: lib.identity.to_remote_identity(),
							config: lib.config().await,
						}
					})
					.collect::<Vec<_>>()
					.join()
					.await
			})
		})
		.procedure("statistics", {
			R.with2(library())
				.query(|(node, library), _: ()| async move {
					// TODO: get from database if library is offline
					// let _statistics = library
					// 	.db
					// 	.statistics()
					// 	.find_unique(statistics::id::equals(library.node_local_id))
					// 	.exec()
					// 	.await?;

					let volumes = get_volumes().await;
					// save_volume(&library).await?;

					let mut total_capacity: u64 = 0;
					let mut available_capacity: u64 = 0;
					for volume in volumes {
						total_capacity += volume.total_capacity;
						available_capacity += volume.available_capacity;
					}

					let library_db_size = get_size(
						node.config
							.data_directory()
							.join("libraries")
							.join(&format!("{}.db", library.id)),
					)
					.await
					.unwrap_or(0);

					let thumbnail_folder_size =
						get_size(node.config.data_directory().join("thumbnails"))
							.await
							.unwrap_or(0);

					use statistics::*;
					let params = vec![
						id::set(1), // Each library is a database so only one of these ever exists
						date_captured::set(Utc::now().into()),
						total_object_count::set(0),
						library_db_size::set(library_db_size.to_string()),
						total_bytes_used::set(0.to_string()),
						total_bytes_capacity::set(total_capacity.to_string()),
						total_unique_bytes::set(0.to_string()),
						total_bytes_free::set(available_capacity.to_string()),
						preview_media_bytes::set(thumbnail_folder_size.to_string()),
					];

					Ok(library
						.db
						.statistics()
						.upsert(
							statistics::id::equals(1), // Each library is a database so only one of these ever exists
							statistics::create(params.clone()),
							params,
						)
						.exec()
						.await?)
				})
		})
		.procedure("create", {
			#[derive(Deserialize, Type, Default)]
			pub struct DefaultLocations {
				desktop: bool,
				documents: bool,
				downloads: bool,
				pictures: bool,
				music: bool,
				videos: bool,
			}

			#[derive(Deserialize, Type)]
			pub struct CreateLibraryArgs {
				name: LibraryName,
				default_locations: Option<DefaultLocations>,
			}

			async fn create_default_locations_on_library_creation(
				DefaultLocations {
					desktop,
					documents,
					downloads,
					pictures,
					music,
					videos,
				}: DefaultLocations,
				node: Arc<Node>,
				library: Arc<Library>,
			) -> Result<(), rspc::Error> {
				// If all of them are false, we skip
				if [!desktop, !documents, !downloads, !pictures, !music, !videos]
					.into_iter()
					.all(identity)
				{
					return Ok(());
				}

				let Some(default_locations_paths) = UserDirs::new() else {
					return Err(rspc::Error::new(
						ErrorCode::NotFound,
						"Didn't find any system locations".to_string(),
					));
				};

				let default_rules_ids = library
					.db
					.indexer_rule()
					.find_many(vec![indexer_rule::default::equals(Some(true))])
					.select(indexer_rule::select!({ id }))
					.exec()
					.await
					.map_err(|e| {
						rspc::Error::with_cause(
							ErrorCode::InternalServerError,
							"Failed to get default indexer rules for default locations".to_string(),
							e,
						)
					})?
					.into_iter()
					.map(|rule| rule.id)
					.collect::<Vec<_>>();

				let mut maybe_error = None;

				[
					(desktop, default_locations_paths.desktop_dir()),
					(documents, default_locations_paths.document_dir()),
					(downloads, default_locations_paths.download_dir()),
					(pictures, default_locations_paths.picture_dir()),
					(music, default_locations_paths.audio_dir()),
					(videos, default_locations_paths.video_dir()),
				]
				.into_iter()
				.filter_map(|entry| {
					if let (true, Some(path)) = entry {
						let node = Arc::clone(&node);
						let library = Arc::clone(&library);
						let indexer_rules_ids = default_rules_ids.clone();
						let path = path.to_path_buf();
						Some(spawn(async move {
							let Some(location) = LocationCreateArgs {
								path,
								dry_run: false,
								indexer_rules_ids,
							}
							.create(&node, &library)
							.await
							.map_err(rspc::Error::from)?
							else {
								return Ok(());
							};

							scan_location(&node, &library, location)
								.await
								.map_err(rspc::Error::from)
						}))
					} else {
						None
					}
				})
				.collect::<Vec<_>>()
				.join()
				.await
				.into_iter()
				.map(|spawn_res| {
					spawn_res
						.map_err(|_| {
							rspc::Error::new(
								ErrorCode::InternalServerError,
								"A task to create a default location failed".to_string(),
							)
						})
						.and_then(identity)
				})
				.fold(&mut maybe_error, |maybe_error, res| {
					if let Err(e) = res {
						error!("Failed to create default location: {e:#?}");
						*maybe_error = Some(e);
					}
					maybe_error
				});

				if let Some(e) = maybe_error {
					return Err(e);
				}

				debug!("Created default locations");

				Ok(())
			}

			R.mutation(
				|node,
				 CreateLibraryArgs {
				     name,
				     default_locations,
				 }: CreateLibraryArgs| async move {
					debug!("Creating library");

					let library = node.libraries.create(name, None, &node).await?;

					debug!("Created library {}", library.id);

					if let Some(locations) = default_locations {
						create_default_locations_on_library_creation(
							locations,
							node,
							Arc::clone(&library),
						)
						.await?;
					}

					Ok(LibraryConfigWrapped {
						uuid: library.id,
						instance_id: library.instance_uuid,
						instance_public_key: library.identity.to_remote_identity(),
						config: library.config().await,
					})
				},
			)
		})
		.procedure("edit", {
			#[derive(Type, Deserialize)]
			pub struct EditLibraryArgs {
				pub id: Uuid,
				pub name: Option<LibraryName>,
				pub description: MaybeUndefined<String>,
			}

			R.mutation(
				|node,
				 EditLibraryArgs {
				     id,
				     name,
				     description,
				 }: EditLibraryArgs| async move {
					Ok(node.libraries.edit(id, name, description).await?)
				},
			)
		})
		.procedure(
			"delete",
			R.mutation(|node, id: Uuid| async move {
				node.libraries.delete(&id).await.map_err(Into::into)
			}),
		)
}



File: ./src/api/categories.rs
-------------------------------------------------
use crate::library::Category;

use std::{collections::BTreeMap, str::FromStr};

use rspc::{alpha::AlphaRouter, ErrorCode};
use strum::VariantNames;

use super::{utils::library, Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router().procedure("list", {
		R.with2(library()).query(|(_, library), _: ()| async move {
			let (categories, queries): (Vec<_>, Vec<_>) = Category::VARIANTS
				.iter()
				.map(|category| {
					let category = Category::from_str(category)
						.expect("it's alright this category string exists");
					(
						category,
						library.db.object().count(vec![category.to_where_param()]),
					)
				})
				.unzip();

			Ok(categories
				.into_iter()
				.zip(
					library
						.db
						._batch(queries)
						.await?
						.into_iter()
						// TODO(@Oscar): rspc bigint support
						.map(|count| {
							i32::try_from(count).map_err(|_| {
								rspc::Error::new(
									ErrorCode::InternalServerError,
									"category item count overflowed 'i32'!".into(),
								)
							})
						})
						.collect::<Result<Vec<_>, _>>()?,
				)
				.collect::<BTreeMap<_, _>>())
		})
	})
}



File: ./src/api/ephemeral_files.rs
-------------------------------------------------
use crate::{
	api::utils::library,
	invalidate_query,
	library::Library,
	location::file_path_helper::IsolatedFilePathData,
	object::{
		fs::{error::FileSystemJobsError, find_available_filename_for_duplicate},
		media::media_data_extractor::{
			can_extract_media_data_for_image, extract_media_data, MediaDataError,
		},
	},
	util::error::FileIOError,
};

use sd_file_ext::extensions::ImageExtension;
use sd_media_metadata::MediaMetadata;

use std::{ffi::OsStr, path::PathBuf, str::FromStr};

use async_recursion::async_recursion;
use futures_concurrency::future::TryJoin;
use regex::Regex;
use rspc::{alpha::AlphaRouter, ErrorCode};
use serde::Deserialize;
use specta::Type;
use tokio::{fs, io};
use tokio_stream::{wrappers::ReadDirStream, StreamExt};
use tracing::{error, warn};

use super::{
	files::{create_directory, FromPattern},
	Ctx, R,
};

const UNTITLED_FOLDER_STR: &str = "Untitled Folder";

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("getMediaData", {
			R.query(|_, full_path: PathBuf| async move {
				let Some(extension) = full_path.extension().and_then(|ext| ext.to_str()) else {
					return Ok(None);
				};

				// TODO(fogodev): change this when we have media data for audio and videos
				let image_extension = ImageExtension::from_str(extension).map_err(|e| {
					error!("Failed to parse image extension: {e:#?}");
					rspc::Error::new(ErrorCode::BadRequest, "Invalid image extension".to_string())
				})?;

				if !can_extract_media_data_for_image(&image_extension) {
					return Ok(None);
				}

				match extract_media_data(full_path).await {
					Ok(img_media_data) => Ok(Some(MediaMetadata::Image(Box::new(img_media_data)))),
					Err(MediaDataError::MediaData(sd_media_metadata::Error::NoExifDataOnPath(
						_,
					))) => Ok(None),
					Err(e) => Err(rspc::Error::with_cause(
						ErrorCode::InternalServerError,
						"Failed to extract media data".to_string(),
						e,
					)),
				}
			})
		})
		.procedure("createFolder", {
			#[derive(Type, Deserialize)]
			pub struct CreateEphemeralFolderArgs {
				pub path: PathBuf,
				pub name: Option<String>,
			}
			R.with2(library()).mutation(
				|(_, library),
				 CreateEphemeralFolderArgs { mut path, name }: CreateEphemeralFolderArgs| async move {
					path.push(name.as_deref().unwrap_or(UNTITLED_FOLDER_STR));

					create_directory(path, &library).await
				},
			)
		})
		.procedure("deleteFiles", {
			R.with2(library())
				.mutation(|(_, library), paths: Vec<PathBuf>| async move {
					paths
						.into_iter()
						.map(|path| async move {
							match fs::metadata(&path).await {
								Ok(metadata) => if metadata.is_dir() {
									fs::remove_dir_all(&path).await
								} else {
									fs::remove_file(&path).await
								}
								.map_err(|e| FileIOError::from((path, e, "Failed to delete file"))),
								Err(e) if e.kind() == io::ErrorKind::NotFound => Ok(()),
								Err(e) => Err(FileIOError::from((
									path,
									e,
									"Failed to get file metadata for deletion",
								))),
							}
						})
						.collect::<Vec<_>>()
						.try_join()
						.await?;

					invalidate_query!(library, "search.ephemeralPaths");

					Ok(())
				})
		})
		.procedure("copyFiles", {
			R.with2(library())
				.mutation(|(_, library), args: EphemeralFileSystemOps| async move {
					args.copy(&library).await
				})
		})
		.procedure("cutFiles", {
			R.with2(library())
				.mutation(|(_, library), args: EphemeralFileSystemOps| async move {
					args.cut(&library).await
				})
		})
		.procedure("renameFile", {
			#[derive(Type, Deserialize)]
			pub struct EphemeralRenameOne {
				pub from_path: PathBuf,
				pub to: String,
			}

			#[derive(Type, Deserialize)]
			pub struct EphemeralRenameMany {
				pub from_pattern: FromPattern,
				pub to_pattern: String,
				pub from_paths: Vec<PathBuf>,
			}

			#[derive(Type, Deserialize)]
			pub enum EphemeralRenameKind {
				One(EphemeralRenameOne),
				Many(EphemeralRenameMany),
			}

			#[derive(Type, Deserialize)]
			pub struct EphemeralRenameFileArgs {
				pub kind: EphemeralRenameKind,
			}

			impl EphemeralRenameFileArgs {
				pub async fn rename_one(
					EphemeralRenameOne { from_path, to }: EphemeralRenameOne,
				) -> Result<(), rspc::Error> {
					let Some(old_name) = from_path.file_name() else {
						return Err(rspc::Error::new(
							ErrorCode::BadRequest,
							"Missing file name on file to be renamed".to_string(),
						));
					};

					if old_name == OsStr::new(&to) {
						return Ok(());
					}

					let (new_file_name, new_extension) =
						IsolatedFilePathData::separate_name_and_extension_from_str(&to).map_err(
							|e| rspc::Error::with_cause(ErrorCode::BadRequest, e.to_string(), e),
						)?;

					if !IsolatedFilePathData::accept_file_name(new_file_name) {
						return Err(rspc::Error::new(
							ErrorCode::BadRequest,
							"Invalid file name".to_string(),
						));
					}

					let Some(parent) = from_path.parent() else {
						return Err(rspc::Error::new(
							ErrorCode::BadRequest,
							"Missing parent path on file to be renamed".to_string(),
						));
					};

					let new_file_full_path = parent.join(if !new_extension.is_empty() {
						&to
					} else {
						new_file_name
					});

					match fs::metadata(&new_file_full_path).await {
						Ok(_) => Err(rspc::Error::new(
							ErrorCode::Conflict,
							"Renaming would overwrite a file".to_string(),
						)),

						Err(e) => {
							if e.kind() != std::io::ErrorKind::NotFound {
								return Err(rspc::Error::with_cause(
									ErrorCode::InternalServerError,
									"Failed to check if file exists".to_string(),
									e,
								));
							}

							fs::rename(&from_path, new_file_full_path)
								.await
								.map_err(|e| {
									FileIOError::from((from_path, e, "Failed to rename file"))
										.into()
								})
						}
					}
				}

				pub async fn rename_many(
					EphemeralRenameMany {
						ref from_pattern,
						ref to_pattern,
						from_paths,
					}: EphemeralRenameMany,
				) -> Result<(), rspc::Error> {
					let from_regex = &Regex::new(&from_pattern.pattern).map_err(|e| {
						rspc::Error::with_cause(
							rspc::ErrorCode::BadRequest,
							"Invalid `from` regex pattern".to_string(),
							e,
						)
					})?;

					from_paths
						.into_iter()
						.map(|old_path| async move {
							let Some(old_name) = old_path.file_name() else {
								return Err(rspc::Error::new(
									ErrorCode::BadRequest,
									"Missing file name on file to be renamed".to_string(),
								));
							};

							let Some(old_name_str) = old_name.to_str() else {
								return Err(rspc::Error::new(
									ErrorCode::BadRequest,
									"File with non UTF-8 name".to_string(),
								));
							};

							let replaced_full_name = if from_pattern.replace_all {
								from_regex.replace_all(old_name_str, to_pattern)
							} else {
								from_regex.replace(old_name_str, to_pattern)
							};

							if !IsolatedFilePathData::accept_file_name(replaced_full_name.as_ref())
							{
								return Err(rspc::Error::new(
									ErrorCode::BadRequest,
									"Invalid file name".to_string(),
								));
							}

							let Some(parent) = old_path.parent() else {
								return Err(rspc::Error::new(
									ErrorCode::BadRequest,
									"Missing parent path on file to be renamed".to_string(),
								));
							};

							let new_path = parent.join(replaced_full_name.as_ref());

							fs::rename(&old_path, &new_path).await.map_err(|e| {
								error!(
									"Failed to rename file from: '{}' to: '{}'; Error: {e:#?}",
									old_path.display(),
									new_path.display()
								);
								let e = FileIOError::from((old_path, e, "Failed to rename file"));
								rspc::Error::with_cause(ErrorCode::Conflict, e.to_string(), e)
							})
						})
						.collect::<Vec<_>>()
						.try_join()
						.await?;

					Ok(())
				}
			}

			R.with2(library()).mutation(
				|(_, library), EphemeralRenameFileArgs { kind }: EphemeralRenameFileArgs| async move {
					let res = match kind {
						EphemeralRenameKind::One(one) => {
							EphemeralRenameFileArgs::rename_one(one).await
						}
						EphemeralRenameKind::Many(many) => {
							EphemeralRenameFileArgs::rename_many(many).await
						}
					};

					if res.is_ok() {
						invalidate_query!(library, "search.ephemeralPaths");
					}

					res
				},
			)
		})
}

#[derive(Type, Deserialize)]
struct EphemeralFileSystemOps {
	sources: Vec<PathBuf>,
	target_dir: PathBuf,
}

impl EphemeralFileSystemOps {
	async fn check_target_directory(&self) -> Result<(), rspc::Error> {
		match fs::metadata(&self.target_dir).await {
			Ok(metadata) => {
				if !metadata.is_dir() {
					return Err(rspc::Error::new(
						ErrorCode::BadRequest,
						"Target is not a directory".to_string(),
					));
				}
			}
			Err(e) if e.kind() == io::ErrorKind::NotFound => {
				let e = FileIOError::from((&self.target_dir, e, "Target directory not found"));
				return Err(rspc::Error::with_cause(
					ErrorCode::BadRequest,
					e.to_string(),
					e,
				));
			}
			Err(e) => {
				return Err(FileIOError::from((
					&self.target_dir,
					e,
					"Failed to get target metadata",
				))
				.into());
			}
		}

		Ok(())
	}

	fn check_sources(&self) -> Result<(), rspc::Error> {
		if self.sources.is_empty() {
			return Err(rspc::Error::new(
				ErrorCode::BadRequest,
				"Sources cannot be empty".to_string(),
			));
		}

		Ok(())
	}

	async fn check(&self) -> Result<(), rspc::Error> {
		self.check_sources()?;
		self.check_target_directory().await?;

		Ok(())
	}

	#[async_recursion]
	async fn copy(self, library: &Library) -> Result<(), rspc::Error> {
		self.check().await?;

		let EphemeralFileSystemOps {
			sources,
			target_dir,
		} = self;

		let (directories_to_create, files_to_copy) = sources
			.into_iter()
			.filter_map(|source| {
				if let Some(name) = source.file_name() {
					let target = target_dir.join(name);
					Some((source, target))
				} else {
					warn!("Skipping file with no name: '{}'", source.display());
					None
				}
			})
			.map(|(source, target)| async move {
				match fs::metadata(&source).await {
					Ok(metadata) => Ok((source, target, metadata.is_dir())),
					Err(e) => Err(FileIOError::from((
						source,
						e,
						"Failed to get source file metadata",
					))),
				}
			})
			.collect::<Vec<_>>()
			.try_join()
			.await?
			.into_iter()
			.partition::<Vec<_>, _>(|(_, _, is_dir)| *is_dir);

		files_to_copy
			.into_iter()
			.map(|(source, mut target, _)| async move {
				match fs::metadata(&target).await {
					Ok(_) => target = find_available_filename_for_duplicate(&target).await?,
					Err(e) if e.kind() == io::ErrorKind::NotFound => {
						// Everything is awesome!
					}
					Err(e) => {
						return Err(FileSystemJobsError::FileIO(FileIOError::from((
							target,
							e,
							"Failed to get target file metadata",
						))));
					}
				}

				fs::copy(&source, target).await.map_err(|e| {
					FileSystemJobsError::FileIO(FileIOError::from((
						source,
						e,
						"Failed to copy file",
					)))
				})
			})
			.collect::<Vec<_>>()
			.try_join()
			.await?;

		if !directories_to_create.is_empty() {
			directories_to_create
				.into_iter()
				.map(|(source, mut target, _)| async move {
					match fs::metadata(&target).await {
						Ok(_) => target = find_available_filename_for_duplicate(&target).await?,
						Err(e) if e.kind() == io::ErrorKind::NotFound => {
							// Everything is awesome!
						}
						Err(e) => {
							return Err(rspc::Error::from(FileIOError::from((
								target,
								e,
								"Failed to get target file metadata",
							))));
						}
					}

					fs::create_dir_all(&target).await.map_err(|e| {
						FileIOError::from((&target, e, "Failed to create directory"))
					})?;

					let more_files =
						ReadDirStream::new(fs::read_dir(&source).await.map_err(|e| {
							FileIOError::from((&source, e, "Failed to read directory to be copied"))
						})?)
						.map(|read_dir| match read_dir {
							Ok(dir_entry) => Ok(dir_entry.path()),
							Err(e) => Err(FileIOError::from((
								&source,
								e,
								"Failed to read directory to be copied",
							))),
						})
						.collect::<Result<Vec<_>, _>>()
						.await?;

					if !more_files.is_empty() {
						Self {
							sources: more_files,
							target_dir: target,
						}
						.copy(library)
						.await
					} else {
						Ok(())
					}
				})
				.collect::<Vec<_>>()
				.try_join()
				.await?;
		}

		invalidate_query!(library, "search.ephemeralPaths");

		Ok(())
	}

	async fn cut(self, library: &Library) -> Result<(), rspc::Error> {
		self.check().await?;

		let EphemeralFileSystemOps {
			sources,
			target_dir,
		} = self;

		sources
			.into_iter()
			.filter_map(|source| {
				if let Some(name) = source.file_name() {
					let target = target_dir.join(name);
					Some((source, target))
				} else {
					warn!("Skipping file with no name: '{}'", source.display());
					None
				}
			})
			.map(|(source, target)| async move {
				match fs::metadata(&target).await {
					Ok(_) => {
						return Err(FileSystemJobsError::WouldOverwrite(
							target.into_boxed_path(),
						));
					}
					Err(e) if e.kind() == io::ErrorKind::NotFound => {
						// Everything is awesome!
					}
					Err(e) => {
						return Err(FileSystemJobsError::FileIO(FileIOError::from((
							source,
							e,
							"Failed to get target file metadata",
						))));
					}
				}

				fs::rename(&source, target).await.map_err(|e| {
					FileSystemJobsError::FileIO(FileIOError::from((
						source,
						e,
						"Failed to move file",
					)))
				})
			})
			.collect::<Vec<_>>()
			.try_join()
			.await?;

		invalidate_query!(library, "search.ephemeralPaths");

		Ok(())
	}
}



File: ./src/api/backups.rs
-------------------------------------------------
use std::{
	cmp,
	path::{Path, PathBuf},
	sync::Arc,
	time::{SystemTime, UNIX_EPOCH},
};

use flate2::{bufread::GzDecoder, write::GzEncoder, Compression};
use futures::executor::block_on;
use futures_concurrency::future::TryJoin;
use rspc::{alpha::AlphaRouter, ErrorCode};
use serde::{Serialize, Serializer};
use specta::Type;
use tar::Archive;
use tempfile::tempdir;
use thiserror::Error;
use tokio::{
	fs::{self, File},
	io::{
		self, AsyncBufReadExt, AsyncRead, AsyncReadExt, AsyncWrite, AsyncWriteExt, BufReader,
		BufWriter,
	},
	spawn,
};
use tracing::{error, info};
use uuid::Uuid;

use crate::{
	invalidate_query,
	library::{Library, LibraryManagerError},
	util::error::FileIOError,
	Node,
};

use super::{utils::library, Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("getAll", {
			#[derive(Serialize, Type)]
			pub struct Backup {
				#[serde(flatten)]
				header: Header,
				path: PathBuf,
			}

			#[derive(Serialize, Type)]
			pub struct GetAll {
				backups: Vec<Backup>,
				directory: PathBuf,
			}

			async fn process_backups(path: impl AsRef<Path>) -> Result<Vec<Backup>, BackupError> {
				let path = path.as_ref();

				let mut read_dir = fs::read_dir(path).await.map_err(|e| {
					FileIOError::from((&path, e, "Failed to read backups directory"))
				})?;

				let mut backups = vec![];

				while let Some(entry) = read_dir.next_entry().await.map_err(|e| {
					FileIOError::from((path, e, "Failed to read next entry to backup"))
				})? {
					let entry_path = entry.path();

					let metadata = entry.metadata().await.map_err(|e| {
						FileIOError::from((
							&entry_path,
							e,
							"Failed to read metadata from backup entry",
						))
					})?;

					if metadata.is_file() {
						backups.push(async move {
							let mut file = File::open(&entry_path).await.map_err(|e| {
								FileIOError::from((&entry_path, e, "Failed to open backup entry"))
							})?;

							Header::read(&mut file, &entry_path)
								.await
								.map(|header| Backup {
									header,
									path: entry_path,
								})
						});
					}
				}

				backups.try_join().await
			}

			R.query(|node, _: ()| async move {
				let directory = node.data_dir.join("backups");

				let backups = match fs::metadata(&directory).await {
					Ok(_) => process_backups(directory.clone()).await.map_err(|e| {
						rspc::Error::with_cause(
							ErrorCode::InternalServerError,
							"Failed to fetch backups".to_string(),
							e,
						)
					})?,
					Err(e) if e.kind() == io::ErrorKind::NotFound => vec![],
					Err(e) => {
						return Err(
							FileIOError::from((&directory, e, "Failed to fetch backups")).into(),
						)
					}
				};

				Ok(GetAll { backups, directory })
			})
		})
		.procedure("backup", {
			R.with2(library())
				.mutation(|(node, library), _: ()| start_backup(node, library))
		})
		.procedure("restore", {
			R
				// TODO: Paths as strings is bad but here we want the flexibility of the frontend allowing any path
				.mutation(|node, path: String| start_restore(node, path.into()))
		})
		.procedure("delete", {
			R
				// TODO: Paths as strings is bad but here we want the flexibility of the frontend allowing any path
				.mutation(|node, path: String| async move {
					fs::remove_file(path)
						.await
						.map(|_| {
							invalidate_query!(node; node, "backups.getAll");
						})
						.map_err(|_| {
							rspc::Error::new(
								ErrorCode::InternalServerError,
								"Error deleting backup!".to_string(),
							)
						})
				})
		})
}

async fn start_backup(node: Arc<Node>, library: Arc<Library>) -> Uuid {
	let bkp_id = Uuid::new_v4();

	spawn(async move {
		match do_backup(bkp_id, &node, &library).await {
			Ok(path) => {
				info!(
					"Backup '{bkp_id}' for library '{}' created at '{path:?}'!",
					library.id
				);
				invalidate_query!(library, "backups.getAll");
			}
			Err(e) => {
				error!(
					"Error with backup '{bkp_id}' for library '{}': {e:?}",
					library.id
				);

				// TODO: Alert user something went wrong
			}
		}
	});

	bkp_id
}

#[derive(Error, Debug)]
enum BackupError {
	#[error("library manager error: {0}")]
	LibraryManager(#[from] LibraryManagerError),
	#[error("malformed header")]
	MalformedHeader,
	#[error("Library already exists, please remove it and try again!")]
	LibraryAlreadyExists,

	#[error(transparent)]
	FileIO(#[from] FileIOError),
}

async fn do_backup(id: Uuid, node: &Node, library: &Library) -> Result<PathBuf, BackupError> {
	let backups_dir = node.data_dir.join("backups");
	fs::create_dir_all(&backups_dir)
		.await
		.map_err(|e| FileIOError::from((&backups_dir, e)))?;

	let timestamp = SystemTime::now()
		.duration_since(UNIX_EPOCH)
		.expect("Time went backwards")
		.as_millis();

	let bkp_path = backups_dir.join(format!("{id}.bkp"));
	let mut bkp_file = BufWriter::new(
		File::create(&bkp_path)
			.await
			.map_err(|e| FileIOError::from((&bkp_path, e, "Failed to create backup file")))?,
	);

	// Header. We do this so the file is self-sufficient.
	Header {
		id,
		timestamp,
		library_id: library.id,
		library_name: library.config().await.name.to_string(),
	}
	.write(&mut bkp_file)
	.await
	.map_err(|e| FileIOError::from((&bkp_path, e, "Failed to create backup file")))?;

	// Introducing this adapter here to bridge tokio stuff to std::io stuff
	struct WriterAdapter(BufWriter<File>);

	impl std::io::Write for WriterAdapter {
		fn write(&mut self, buf: &[u8]) -> io::Result<usize> {
			block_on(self.0.write(buf))
		}

		fn flush(&mut self) -> io::Result<()> {
			block_on(self.0.flush())
		}
	}

	// Regular tar.gz encoded data
	let mut tar = tar::Builder::new(GzEncoder::new(
		WriterAdapter(bkp_file),
		Compression::default(),
	));

	let library_config_path = node
		.libraries
		.libraries_dir
		.join(format!("{}.sdlibrary", library.id));

	tar.append_file(
		"library.sdlibrary",
		&mut std::fs::File::open(&library_config_path).map_err(|e| {
			FileIOError::from((
				library_config_path,
				e,
				"Failed to open library config file to do a backup",
			))
		})?,
	)
	.map_err(|e| {
		FileIOError::from((
			&bkp_path,
			e,
			"Failed to append library config file to out backup tar.gz file",
		))
	})?;

	let library_db_path = node
		.libraries
		.libraries_dir
		.join(format!("{}.db", library.id));

	tar.append_file(
		"library.db",
		&mut std::fs::File::open(&library_db_path).map_err(|e| {
			FileIOError::from((
				library_db_path,
				e,
				"Failed to open library database file to do a backup",
			))
		})?,
	)
	.map_err(|e| {
		FileIOError::from((
			&bkp_path,
			e,
			"Failed to append library database file to out backup tar.gz file",
		))
	})?;

	Ok(bkp_path)
}

async fn start_restore(node: Arc<Node>, path: PathBuf) {
	match restore_backup(&node, &path).await {
		Ok(Header { id, library_id, .. }) => {
			info!("Restored to '{id}' for library '{library_id}'!",);
		}
		Err(e) => {
			error!("Error restoring backup '{}': {e:#?}", path.display());

			// TODO: Alert user something went wrong
		}
	}
}

async fn restore_backup(node: &Arc<Node>, path: impl AsRef<Path>) -> Result<Header, BackupError> {
	let path = path.as_ref();

	let mut file = BufReader::new(fs::File::open(path).await.map_err(|e| {
		FileIOError::from((path, e, "Failed trying to open backup file to be restored"))
	})?);

	let header = Header::read(&mut file, path).await?;

	// TODO: Actually handle restoring into a library that exists. For now it's easier to error out.
	let None = node.libraries.get_library(&header.library_id).await else {
		return Err(BackupError::LibraryAlreadyExists);
	};

	let temp_dir = tempdir().map_err(|e| {
		FileIOError::from((
			"/tmp",
			e,
			"Failed to get a temporary directory to restore backup",
		))
	})?;

	// Introducing this adapter here to bridge tokio stuff to std::io stuff
	struct ReaderAdapter(BufReader<File>);

	impl std::io::Read for ReaderAdapter {
		fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {
			block_on(self.0.read(buf))
		}
	}

	impl std::io::BufRead for ReaderAdapter {
		fn fill_buf(&mut self) -> io::Result<&[u8]> {
			block_on(self.0.fill_buf())
		}

		fn consume(&mut self, amt: usize) {
			self.0.consume(amt)
		}
	}

	let temp_dir_path = temp_dir.path();

	let mut archive = Archive::new(GzDecoder::new(ReaderAdapter(file)));
	archive.unpack(&temp_dir).map_err(|e| {
		FileIOError::from((temp_dir_path, e, "Failed to unpack backup compressed data"))
	})?;

	let library_config_path = temp_dir_path.join("library.sdlibrary");

	let library_config_restored_path = node
		.libraries
		.libraries_dir
		.join(format!("{}.sdlibrary", header.library_id));

	fs::copy(library_config_path, &library_config_restored_path)
		.await
		.map_err(|e| {
			FileIOError::from((
				&library_config_restored_path,
				e,
				"Failed to restore library config file from backup",
			))
		})?;

	let db_path = temp_dir_path.join("library.db");
	let db_restored_path = node
		.libraries
		.libraries_dir
		.join(format!("{}.db", header.library_id));

	fs::copy(db_path, &db_restored_path).await.map_err(|e| {
		FileIOError::from((
			&db_restored_path,
			e,
			"Failed to restore library database file from backup",
		))
	})?;

	node.libraries
		.load(
			header.library_id,
			db_restored_path,
			library_config_restored_path,
			None,
			true,
			node,
		)
		.await?;

	Ok(header)
}

#[derive(Debug, PartialEq, Eq, Serialize, Type)]
struct Header {
	// Backup unique id
	id: Uuid,
	// Time since epoch the backup was created at
	#[specta(type = String)]
	#[serde(serialize_with = "as_string")]
	timestamp: u128,
	// Library id
	library_id: Uuid,
	// Library display name
	library_name: String,
}

fn as_string<T: ToString, S>(x: &T, s: S) -> Result<S::Ok, S::Error>
where
	S: Serializer,
{
	s.serialize_str(&x.to_string())
}

impl Header {
	async fn write(&self, file: &mut (impl AsyncWrite + Unpin)) -> Result<(), io::Error> {
		// For future versioning we can bump `1` to `2` and match on it in the decoder.
		file.write_all(b"sdbkp1").await?;
		file.write_all(&self.id.to_bytes_le()).await?;
		file.write_all(&self.timestamp.to_le_bytes()).await?;
		file.write_all(&self.library_id.to_bytes_le()).await?;
		{
			let bytes = &self.library_name.as_bytes()
				[..cmp::min(u32::MAX as usize, self.library_name.len())];
			file.write_all(&(bytes.len() as u32).to_le_bytes()).await?;
			file.write_all(bytes).await?;
		}

		Ok(())
	}

	async fn read(
		file: &mut (impl AsyncRead + Unpin),
		path: impl AsRef<Path>,
	) -> Result<Self, BackupError> {
		let mut buf = vec![0u8; 6 + 16 + 16 + 16 + 4];
		let path = path.as_ref();
		file.read_exact(&mut buf)
			.await
			.map_err(|e| FileIOError::from((path, e)))?;

		if &buf[..6] != b"sdbkp1" {
			return Err(BackupError::MalformedHeader);
		}

		Ok(Self {
			id: Uuid::from_bytes_le(
				buf[6..22]
					.try_into()
					.map_err(|_| BackupError::MalformedHeader)?,
			),
			timestamp: u128::from_le_bytes(
				buf[22..38]
					.try_into()
					.map_err(|_| BackupError::MalformedHeader)?,
			),
			library_id: Uuid::from_bytes_le(
				buf[38..54]
					.try_into()
					.map_err(|_| BackupError::MalformedHeader)?,
			),

			library_name: {
				let len = u32::from_le_bytes(
					buf[54..58]
						.try_into()
						.map_err(|_| BackupError::MalformedHeader)?,
				);

				let mut name = vec![0; len as usize];
				file.read_exact(&mut name)
					.await
					.map_err(|e| FileIOError::from((path, e)))?;

				String::from_utf8(name).map_err(|_| BackupError::MalformedHeader)?
			},
		})
	}
}

#[cfg(test)]
mod tests {
	use super::*;

	#[tokio::test]
	async fn test_backup_header() {
		let original = Header {
			id: Uuid::new_v4(),
			timestamp: 1234567890,
			library_id: Uuid::new_v4(),
			library_name: "Test Library".to_string(),
		};

		let mut buf = Vec::new();
		original.write(&mut buf).await.unwrap();

		let decoded = Header::read(&mut buf.as_slice(), "").await.unwrap();
		assert_eq!(original, decoded);
	}
}



File: ./src/api/p2p.rs
-------------------------------------------------
use rspc::{alpha::AlphaRouter, ErrorCode};
use sd_p2p::spacetunnel::RemoteIdentity;
use serde::Deserialize;
use specta::Type;
use std::path::PathBuf;
use uuid::Uuid;

use crate::p2p::{operations, P2PEvent, PairingDecision};

use super::{Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("events", {
			R.subscription(|node, _: ()| async move {
				let mut rx = node.p2p.subscribe();

				let mut queued = Vec::new();

				// TODO: Don't block subscription start
				for peer in node.p2p.node.get_discovered() {
					queued.push(P2PEvent::DiscoveredPeer {
						identity: peer.identity,
						metadata: peer.metadata,
					});
				}

				// TODO: Don't block subscription start
				for identity in node
					.p2p
					.manager
					.get_connected_peers()
					.await
					.map_err(|_err| {
						rspc::Error::new(
							ErrorCode::InternalServerError,
							"todo: error getting connected peers".into(),
						)
					})? {
					queued.push(P2PEvent::ConnectedPeer { identity });
				}

				Ok(async_stream::stream! {
					for event in queued.drain(..queued.len()) {
						yield event;
					}

					while let Ok(event) = rx.recv().await {
						yield event;
					}
				})
			})
		})
		.procedure("state", {
			R.query(|node, _: ()| async move { node.p2p.state() })
		})
		.procedure("spacedrop", {
			#[derive(Type, Deserialize)]
			pub struct SpacedropArgs {
				identity: RemoteIdentity,
				file_path: Vec<String>,
			}

			R.mutation(|node, args: SpacedropArgs| async move {
				operations::spacedrop(
					node.p2p.clone(),
					args.identity,
					args.file_path
						.into_iter()
						.map(PathBuf::from)
						.collect::<Vec<_>>(),
				)
				.await
				.map_err(|_err| {
					rspc::Error::new(ErrorCode::InternalServerError, "todo: error".into())
				})
			})
		})
		.procedure("acceptSpacedrop", {
			R.mutation(|node, (id, path): (Uuid, Option<String>)| async move {
				match path {
					Some(path) => node.p2p.accept_spacedrop(id, path).await,
					None => node.p2p.reject_spacedrop(id).await,
				}
			})
		})
		.procedure("cancelSpacedrop", {
			R.mutation(|node, id: Uuid| async move { node.p2p.cancel_spacedrop(id).await })
		})
		.procedure("pair", {
			R.mutation(|node, id: RemoteIdentity| async move {
				node.p2p.pairing.clone().originator(id, node).await
			})
		})
		.procedure("pairingResponse", {
			R.mutation(|node, (pairing_id, decision): (u16, PairingDecision)| {
				node.p2p.pairing.decision(pairing_id, decision);
			})
		})
}



File: ./src/api/auth.rs
-------------------------------------------------
use std::time::Duration;

use reqwest::{Response, StatusCode};
use rspc::alpha::AlphaRouter;
use serde::{de::DeserializeOwned, Deserialize, Serialize};
use specta::Type;

use crate::{auth::DEVICE_CODE_URN, util::http::ensure_response};

use super::{Ctx, R};

async fn parse_json_body<T: DeserializeOwned>(response: Response) -> Result<T, rspc::Error> {
	response.json().await.map_err(|_| {
		rspc::Error::new(
			rspc::ErrorCode::InternalServerError,
			"JSON conversion failed".to_string(),
		)
	})
}

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("loginSession", {
			#[derive(Serialize, Type)]
			#[specta(inline)]
			enum Response {
				Start {
					user_code: String,
					verification_url: String,
					verification_url_complete: String,
				},
				Complete,
				Error,
			}

			R.subscription(|node, _: ()| async move {
				async_stream::stream! {
					#[derive(Deserialize, Type)]
					struct DeviceAuthorizationResponse {
						device_code: String,
						user_code: String,
						verification_url: String,
						verification_uri_complete: String,
					}

					let Ok(auth_response) =	(match node.http
						.post(&format!("{}/login/device/code", &node.env.api_url))
						.form(&[("client_id", &node.env.client_id)])
						.send()
						.await {
							Ok(r) => r.json::<DeviceAuthorizationResponse>().await,
							Err(e) => Err(e)
						}) else {
							yield Response::Error;
							return;
						};

					yield Response::Start {
						user_code: auth_response.user_code.clone(),
						verification_url: auth_response.verification_url.clone(),
						verification_url_complete: auth_response.verification_uri_complete.clone(),
					};

					yield loop {
						tokio::time::sleep(Duration::from_secs(5)).await;

						let Ok(token_resp) = node.http
							.post(&format!("{}/login/oauth/access_token", &node.env.api_url))
							.form(&[
								("grant_type", DEVICE_CODE_URN),
								("device_code", &auth_response.device_code),
								("client_id", &node.env.client_id)
							])
							.send()
							.await else {
								break Response::Error;
							};

						match token_resp.status() {
							StatusCode::OK => {
								let Ok(token) = token_resp.json().await else {
									break Response::Error;
								};

								if node.config
									.write(|c| c.auth_token = Some(token))
									.await.is_err() {
										break Response::Error;
									};


								break Response::Complete;
							},
							StatusCode::BAD_REQUEST => {
								#[derive(Debug, Deserialize)]
								struct OAuth400 {
									error: String
								}

								let Ok(resp) = token_resp.json::<OAuth400>().await else {
									break Response::Error;
								};

								match resp.error.as_str() {
									"authorization_pending" => continue,
									_ => {
										break Response::Error;
									}
								}
							},
							_ => {
								break Response::Error;
							}
						}
					}
				}
			})
		})
		.procedure(
			"logout",
			R.mutation(|node, _: ()| async move {
				node.config
					.write(|c| c.auth_token = None)
					.await
					.map(|_| ())
					.map_err(|_| {
						rspc::Error::new(
							rspc::ErrorCode::InternalServerError,
							"Failed to write config".to_string(),
						)
					})
			}),
		)
		.procedure("me", {
			R.query(|node, _: ()| async move {
				#[derive(Serialize, Deserialize, Type)]
				#[specta(inline)]
				struct Response {
					id: String,
					email: String,
				}

				node.authed_api_request(
					node.http
						.get(&format!("{}/api/v1/user/me", &node.env.api_url)),
				)
				.await
				.and_then(ensure_response)
				.map(parse_json_body::<Response>)?
				.await
			})
		})
}



File: ./src/api/tags.rs
-------------------------------------------------
use std::collections::BTreeMap;

use chrono::Utc;
use itertools::{Either, Itertools};
use rspc::{alpha::AlphaRouter, ErrorCode};
use sd_file_ext::kind::ObjectKind;
use sd_prisma::{prisma, prisma_sync};
use sd_sync::OperationFactory;
use sd_utils::uuid_to_bytes;
use serde::Deserialize;
use specta::Type;

use serde_json::json;
use uuid::Uuid;

use crate::{
	invalidate_query,
	library::Library,
	object::tag::TagCreateArgs,
	prisma::{file_path, object, tag, tag_on_object},
};

use super::{utils::library, Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("list", {
			R.with2(library()).query(|(_, library), _: ()| async move {
				Ok(library.db.tag().find_many(vec![]).exec().await?)
			})
		})
		.procedure("getForObject", {
			R.with2(library())
				.query(|(_, library), object_id: i32| async move {
					Ok(library
						.db
						.tag()
						.find_many(vec![tag::tag_objects::some(vec![
							tag_on_object::object_id::equals(object_id),
						])])
						.exec()
						.await?)
				})
		})
		.procedure("getWithObjects", {
			R.with2(library()).query(
				|(_, library), object_ids: Vec<object::id::Type>| async move {
					let Library { db, .. } = library.as_ref();

					let tags_with_objects = db
						.tag()
						.find_many(vec![tag::tag_objects::some(vec![
							tag_on_object::object_id::in_vec(object_ids.clone()),
						])])
						.select(tag::select!({
							id
							tag_objects(vec![tag_on_object::object_id::in_vec(object_ids.clone())]): select {
								date_created
								object: select {
									id
								}
							}
						}))
						.exec()
						.await?;

					Ok(tags_with_objects
						.into_iter()
						.map(|tag| (tag.id, tag.tag_objects))
						.collect::<BTreeMap<_, _>>())
				},
			)
		})
		.procedure("get", {
			R.with2(library())
				.query(|(_, library), tag_id: i32| async move {
					Ok(library
						.db
						.tag()
						.find_unique(tag::id::equals(tag_id))
						.exec()
						.await?)
				})
		})
		.procedure("create", {
			R.with2(library())
				.mutation(|(_, library), args: TagCreateArgs| async move {
					let created_tag = args.exec(&library).await?;

					invalidate_query!(library, "tags.list");

					Ok(created_tag)
				})
		})
		.procedure("assign", {
			#[derive(Debug, Type, Deserialize)]
			#[specta(inline)]
			enum Target {
				Object(prisma::object::id::Type),
				FilePath(prisma::file_path::id::Type),
			}

			#[derive(Debug, Type, Deserialize)]
			#[specta(inline)]
			struct TagAssignArgs {
				targets: Vec<Target>,
				tag_id: i32,
				unassign: bool,
			}

			R.with2(library())
				.mutation(|(_, library), args: TagAssignArgs| async move {
					let Library { db, sync, .. } = library.as_ref();

					let tag = db
						.tag()
						.find_unique(tag::id::equals(args.tag_id))
						.select(tag::select!({ pub_id }))
						.exec()
						.await?
						.ok_or_else(|| {
							rspc::Error::new(ErrorCode::NotFound, "Tag not found".to_string())
						})?;

					let (objects, file_paths) = db
						._batch({
							let (objects, file_paths): (Vec<_>, Vec<_>) = args
								.targets
								.into_iter()
								.partition_map(|target| match target {
									Target::Object(id) => Either::Left(id),
									Target::FilePath(id) => Either::Right(id),
								});

							(
								db.object()
									.find_many(vec![object::id::in_vec(objects)])
									.select(object::select!({
										id
										pub_id
									})),
								db.file_path()
									.find_many(vec![file_path::id::in_vec(file_paths)])
									.select(file_path::select!({
										id
										pub_id
										object: select { id pub_id }
									})),
							)
						})
						.await?;

					macro_rules! sync_id {
						($pub_id:expr) => {
							prisma_sync::tag_on_object::SyncId {
								tag: prisma_sync::tag::SyncId {
									pub_id: tag.pub_id.clone(),
								},
								object: prisma_sync::object::SyncId { pub_id: $pub_id },
							}
						};
					}

					if args.unassign {
						let query = db.tag_on_object().delete_many(vec![
							tag_on_object::tag_id::equals(args.tag_id),
							tag_on_object::object_id::in_vec(
								objects
									.iter()
									.map(|o| o.id)
									.chain(
										file_paths
											.iter()
											.filter_map(|fp| fp.object.as_ref().map(|o| o.id)),
									)
									.collect(),
							),
						]);

						sync.write_ops(
							db,
							(
								objects
									.into_iter()
									.map(|o| o.pub_id)
									.chain(
										file_paths
											.into_iter()
											.filter_map(|fp| fp.object.map(|o| o.pub_id)),
									)
									.map(|pub_id| sync.relation_delete(sync_id!(pub_id)))
									.collect(),
								query,
							),
						)
						.await?;
					} else {
						let (new_objects, _) = db
							._batch({
								let (left, right): (Vec<_>, Vec<_>) = file_paths
									.iter()
									.filter(|fp| fp.object.is_none())
									.map(|fp| {
										let id = uuid_to_bytes(Uuid::new_v4());

										(
											db.object().create(
												id.clone(),
												vec![
													object::date_created::set(None),
													object::kind::set(Some(
														ObjectKind::Folder as i32,
													)),
												],
											),
											db.file_path().update(
												file_path::id::equals(fp.id),
												vec![file_path::object::connect(
													object::pub_id::equals(id),
												)],
											),
										)
									})
									.unzip();

								(left, right)
							})
							.await?;

						let (sync_ops, db_creates) = objects
							.into_iter()
							.map(|o| (o.id, o.pub_id))
							.chain(
								file_paths
									.into_iter()
									.filter_map(|fp| fp.object.map(|o| (o.id, o.pub_id))),
							)
							.chain(new_objects.into_iter().map(|o| (o.id, o.pub_id)))
							.fold(
								(vec![], vec![]),
								|(mut sync_ops, mut db_creates), (id, pub_id)| {
									db_creates.push(tag_on_object::CreateUnchecked {
										tag_id: args.tag_id,
										object_id: id,
										_params: vec![tag_on_object::date_created::set(Some(
											Utc::now().into(),
										))],
									});

									sync_ops.extend(sync.relation_create(sync_id!(pub_id), []));

									(sync_ops, db_creates)
								},
							);

						sync.write_ops(db, (sync_ops, db.tag_on_object().create_many(db_creates)))
							.await?;
					}

					invalidate_query!(library, "tags.getForObject");
					invalidate_query!(library, "tags.getWithObjects");
					invalidate_query!(library, "search.objects");

					Ok(())
				})
		})
		.procedure("update", {
			#[derive(Type, Deserialize)]
			pub struct TagUpdateArgs {
				pub id: i32,
				pub name: Option<String>,
				pub color: Option<String>,
			}

			R.with2(library())
				.mutation(|(_, library), args: TagUpdateArgs| async move {
					let Library { sync, db, .. } = library.as_ref();

					let tag = db
						.tag()
						.find_unique(tag::id::equals(args.id))
						.select(tag::select!({ pub_id }))
						.exec()
						.await?
						.ok_or(rspc::Error::new(
							ErrorCode::NotFound,
							"Error finding tag in db".into(),
						))?;

					db.tag()
						.update(
							tag::id::equals(args.id),
							vec![tag::date_modified::set(Some(Utc::now().into()))],
						)
						.exec()
						.await?;

					sync.write_ops(
						db,
						(
							[
								args.name.as_ref().map(|v| (tag::name::NAME, json!(v))),
								args.color.as_ref().map(|v| (tag::color::NAME, json!(v))),
							]
							.into_iter()
							.flatten()
							.map(|(k, v)| {
								sync.shared_update(
									prisma_sync::tag::SyncId {
										pub_id: tag.pub_id.clone(),
									},
									k,
									v,
								)
							})
							.collect(),
							db.tag().update(
								tag::id::equals(args.id),
								vec![tag::name::set(args.name), tag::color::set(args.color)],
							),
						),
					)
					.await?;

					invalidate_query!(library, "tags.list");

					Ok(())
				})
		})
		.procedure(
			"delete",
			R.with2(library())
				.mutation(|(_, library), tag_id: i32| async move {
					library
						.db
						.tag()
						.delete(tag::id::equals(tag_id))
						.exec()
						.await?;

					invalidate_query!(library, "tags.list");

					Ok(())
				}),
		)
}



File: ./src/api/sync.rs
-------------------------------------------------
use rspc::alpha::AlphaRouter;
use sd_core_sync::GetOpsArgs;

use super::{utils::library, Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("newMessage", {
			R.with2(library())
				.subscription(|(_, library), _: ()| async move {
					async_stream::stream! {
						let mut rx = library.sync.tx.subscribe();
						while let Ok(_msg) = rx.recv().await {
							// let op = match msg {
							// 	SyncMessage::Ingested => (),
							// 	SyncMessage::Created => op
							// };
							yield ();
						}
					}
				})
		})
		.procedure("messages", {
			R.with2(library()).query(|(_, library), _: ()| async move {
				Ok(library
					.sync
					.get_ops(GetOpsArgs {
						clocks: vec![],
						count: 1000,
					})
					.await?)
			})
		})
}



File: ./src/api/utils/invalidate.rs
-------------------------------------------------
use crate::api::{CoreEvent, Ctx, Router, R};

use async_stream::stream;
use rspc::alpha::AlphaRouter;
use serde::Serialize;
use serde_hashkey::to_key;
use serde_json::Value;
use specta::{DataType, Type};
use std::{
	collections::HashMap,
	sync::{
		atomic::{AtomicBool, Ordering},
		Arc,
	},
	time::Duration,
};
use tokio::sync::broadcast;
use tracing::{debug, warn};

#[cfg(debug_assertions)]
use std::sync::Mutex;

/// holds information about all invalidation queries done with the [`invalidate_query!`] macro so we can check they are valid when building the router.
#[cfg(debug_assertions)]
pub(crate) static INVALIDATION_REQUESTS: Mutex<InvalidRequests> =
	Mutex::new(InvalidRequests::new());

// fwi: This exists to keep the enum fields private.
#[derive(Debug, Clone, Serialize, Type)]
pub struct SingleInvalidateOperationEvent {
	/// This fields are intentionally private.
	pub key: &'static str,
	arg: Value,
	result: Option<Value>,
}

#[derive(Debug, Clone, Serialize, Type)]
#[serde(tag = "type", content = "data", rename_all = "camelCase")]
pub enum InvalidateOperationEvent {
	Single(SingleInvalidateOperationEvent),
	// TODO: A temporary hack used with Brendan's sync system until the v2 invalidation system is implemented.
	All,
}

impl InvalidateOperationEvent {
	/// If you are using this function, your doing it wrong.
	pub fn dangerously_create(key: &'static str, arg: Value, result: Option<Value>) -> Self {
		Self::Single(SingleInvalidateOperationEvent { key, arg, result })
	}

	pub fn all() -> Self {
		Self::All
	}
}

/// a request to invalidate a specific resource
#[derive(Debug)]
#[allow(dead_code)]
pub(crate) struct InvalidationRequest {
	pub key: &'static str,
	pub arg_ty: Option<DataType>,
	pub result_ty: Option<DataType>,
	pub macro_src: &'static str,
}

/// invalidation request for a specific resource
#[derive(Debug, Default)]
#[allow(dead_code)]
pub(crate) struct InvalidRequests {
	pub queries: Vec<InvalidationRequest>,
}

impl InvalidRequests {
	#[allow(unused)]
	const fn new() -> Self {
		Self {
			queries: Vec::new(),
		}
	}

	#[allow(unused_variables, clippy::panic)]
	pub(crate) fn validate(r: Arc<Router>) {
		#[cfg(debug_assertions)]
		{
			let invalidate_requests = INVALIDATION_REQUESTS
				.lock()
				.expect("Failed to lock the mutex for invalidation requests");

			let queries = r.queries();
			for req in &invalidate_requests.queries {
				if let Some(query_ty) = queries.get(req.key) {
					if let Some(arg) = &req.arg_ty {
						if &query_ty.ty.input != arg {
							panic!(
								"Error at '{}': Attempted to invalid query '{}' but the argument type does not match the type defined on the router.",
								req.macro_src, req.key
                        	);
						}
					}

					if let Some(result) = &req.result_ty {
						if &query_ty.ty.result != result {
							panic!(
								"Error at '{}': Attempted to invalid query '{}' but the data type does not match the type defined on the router.",
								req.macro_src, req.key
                        	);
						}
					}
				} else {
					panic!(
						"Error at '{}': Attempted to invalid query '{}' which was not found in the router",
						req.macro_src, req.key
					);
				}
			}
		}
	}
}

/// `invalidate_query` is a macro which stores a list of all of it's invocations so it can ensure all of the queries match the queries attached to the router.
/// This allows invalidate to be type-safe even when the router keys are stringly typed.
/// ```ignore
/// invalidate_query!(
/// library, // crate::library::LibraryContext
/// "version": (), // Name of the query and the type of it
/// () // The arguments
/// );
/// ```
#[macro_export]
// #[allow(clippy::crate_in_macro_def)]
macro_rules! invalidate_query {
	($ctx:expr, $key:literal) => {{
		let ctx: &$crate::library::Library = &$ctx; // Assert the context is the correct type

		#[cfg(debug_assertions)]
		{
			#[ctor::ctor]
			fn invalidate() {
				$crate::api::utils::INVALIDATION_REQUESTS
					.lock()
					.unwrap()
					.queries
					.push($crate::api::utils::InvalidationRequest {
						key: $key,
						arg_ty: None,
						result_ty: None,
            			macro_src: concat!(file!(), ":", line!()),
					})
			}
		}

		::tracing::trace!(target: "sd_core::invalidate-query", "invalidate_query!(\"{}\") at {}", $key, concat!(file!(), ":", line!()));

		// The error are ignored here because they aren't mission critical. If they fail the UI might be outdated for a bit.
		ctx.emit($crate::api::CoreEvent::InvalidateOperation(
			$crate::api::utils::InvalidateOperationEvent::dangerously_create($key, serde_json::Value::Null, None)
		))
	}};
	(node; $ctx:expr, $key:literal) => {{
		let ctx: &$crate::Node = &$ctx; // Assert the context is the correct type

		#[cfg(debug_assertions)]
		{
			#[ctor::ctor]
			fn invalidate() {
				$crate::api::utils::INVALIDATION_REQUESTS
					.lock()
					.unwrap()
					.queries
					.push($crate::api::utils::InvalidationRequest {
						key: $key,
						arg_ty: None,
						result_ty: None,
            			macro_src: concat!(file!(), ":", line!()),
					})
			}
		}

		::tracing::trace!(target: "sd_core::invalidate-query", "invalidate_query!(\"{}\") at {}", $key, concat!(file!(), ":", line!()));

		// The error are ignored here because they aren't mission critical. If they fail the UI might be outdated for a bit.
		ctx.event_bus.0.send($crate::api::CoreEvent::InvalidateOperation(
			$crate::api::utils::InvalidateOperationEvent::dangerously_create($key, serde_json::Value::Null, None)
		)).ok();
	}};
	($ctx:expr, $key:literal: $arg_ty:ty, $arg:expr $(,)?) => {{
		let _: $arg_ty = $arg; // Assert the type the user provided is correct
		let ctx: &$crate::library::Library = &$ctx; // Assert the context is the correct type

		#[cfg(debug_assertions)]
		{
			#[ctor::ctor]
			fn invalidate() {
				$crate::api::utils::INVALIDATION_REQUESTS
					.lock()
					.unwrap()
					.queries
					.push($crate::api::utils::InvalidationRequest {
						key: $key,
						arg_ty: <$arg_ty as specta::Type>::reference(specta::DefOpts {
                            parent_inline: false,
                            type_map: &mut specta::TypeDefs::new(),
                        }, &[]).map_err(|e| {
								::tracing::error!(
									"Failed to get type reference for invalidate query '{}': {:?}",
									$key,
									e
								)
							}).ok(),
						result_ty: None,
                        macro_src: concat!(file!(), ":", line!()),
					})
			}
		}

		::tracing::trace!(target: "sd_core::invalidate-query", "invalidate_query!(\"{}\") at {}", $key, concat!(file!(), ":", line!()));

		// The error are ignored here because they aren't mission critical. If they fail the UI might be outdated for a bit.
		let _ = serde_json::to_value($arg)
			.map(|v|
				ctx.emit($crate::api::CoreEvent::InvalidateOperation(
					$crate::api::utils::InvalidateOperationEvent::dangerously_create($key, v, None),
				))
			)
			.map_err(|_| {
				tracing::warn!("Failed to serialize invalidate query event!");
			});
	}};
	($ctx:expr, $key:literal: $arg_ty:ty, $arg:expr, $result_ty:ty: $result:expr $(,)?) => {{
		let _: $arg_ty = $arg; // Assert the type the user provided is correct
		let ctx: &$crate::library::Library = &$ctx; // Assert the context is the correct type

		#[cfg(debug_assertions)]
		{
			#[ctor::ctor]
			fn invalidate() {
				$crate::api::utils::INVALIDATION_REQUESTS
					.lock()
					.unwrap()
					.queries
					.push($crate::api::utils::InvalidationRequest {
						key: $key,
						arg_ty: Some(<$arg_ty as rspc::internal::specta::Type>::reference(rspc::internal::specta::DefOpts {
                            parent_inline: false,
                            type_map: &mut rspc::internal::specta::TypeDefs::new(),
                        }, &[])),
						result_ty: Some(<$result_ty as rspc::internal::specta::Type>::reference(rspc::internal::specta::DefOpts {
                            parent_inline: false,
                            type_map: &mut rspc::internal::specta::TypeDefs::new(),
                        }, &[])),
                        macro_src: concat!(file!(), ":", line!()),
					})
			}
		}

		::tracing::trace!(target: "sd_core::invalidate-query", "invalidate_query!(\"{}\") at {}", $key, concat!(file!(), ":", line!()));

		// The error are ignored here because they aren't mission critical. If they fail the UI might be outdated for a bit.
		let _ = serde_json::to_value($arg)
			.and_then(|arg|
				serde_json::to_value($result)
				.map(|result|
					ctx.emit($crate::api::CoreEvent::InvalidateOperation(
						$crate::api::utils::InvalidateOperationEvent::dangerously_create($key, arg, Some(result)),
					))
				)
			)
			.map_err(|_| {
				tracing::warn!("Failed to serialize invalidate query event!");
			});
	}};
}

pub(crate) fn mount_invalidate() -> AlphaRouter<Ctx> {
	let (tx, _) = broadcast::channel(100);
	let manager_thread_active = Arc::new(AtomicBool::new(false));

	// TODO: Scope the invalidate queries to a specific library (filtered server side)
	let r = if cfg!(debug_assertions) {
		let count = Arc::new(std::sync::atomic::AtomicU16::new(0));

		R.router()
			.procedure(
				"test-invalidate",
				R.query(move |_, _: ()| count.fetch_add(1, Ordering::SeqCst)),
			)
			.procedure(
				"test-invalidate-mutation",
				R.with2(super::library()).mutation(|(_, library), _: ()| {
					invalidate_query!(library, "invalidation.test-invalidate");
					Ok(())
				}),
			)
	} else {
		R.router()
	};

	r.procedure("listen", {
		R.subscription(move |ctx, _: ()| {
			// This thread is used to deal with batching and deduplication.
			// Their is only ever one of these management threads per Node but we spawn it like this so we can steal the event bus from the rspc context.
			// Batching is important because when refetching data on the frontend rspc can fetch all invalidated queries in a single round trip.
			if !manager_thread_active.swap(true, Ordering::Relaxed) {
				let mut event_bus_rx = ctx.event_bus.0.subscribe();
				let tx = tx.clone();
				let manager_thread_active = manager_thread_active.clone();
				tokio::spawn(async move {
					let mut buf = HashMap::with_capacity(100);
					let mut invalidate_all = false;

					loop {
						tokio::select! {
							event = event_bus_rx.recv() => {
								if let Ok(event) = event {
									if let CoreEvent::InvalidateOperation(op) = event {
										if invalidate_all {
											continue;
										}

										match &op {
											InvalidateOperationEvent::Single(SingleInvalidateOperationEvent { key, arg, .. }) => {
												// Newer data replaces older data in the buffer
												match to_key(&(key, &arg)) {
													Ok(key) => {
														buf.insert(key, op);
													},
													Err(err) => {
														warn!("Error deriving key for invalidate operation '{:?}': {:?}", op, err);
													},
												}
											},
											InvalidateOperationEvent::All => {
												invalidate_all = true;
												buf.clear();
											}
										}
									}
								} else {
									warn!("Shutting down invalidation manager thread due to the core event bus being dropped!");
									break;
								}
							},
							_ = tokio::time::sleep(Duration::from_millis(10)) => {
								let events = match invalidate_all {
									true => vec![InvalidateOperationEvent::all()],
									false => buf.drain().map(|(_k, v)| v).collect::<Vec<_>>(),
								};

								if !events.is_empty() {
									match tx.send(events) {
										Ok(_) => {},
										// All receivers are shutdown means that all clients are disconnected.
										Err(_) => {
											debug!("Shutting down invalidation manager! This is normal if all clients disconnects.");
											manager_thread_active.swap(false, Ordering::Relaxed);
											break;
										}
									}
								}
							}
						}
					}
				});
			}

			let mut rx = tx.subscribe();
			stream! {
				while let Ok(msg) = rx.recv().await {
					yield msg;
				}
			}
		})
	})
}



File: ./src/api/utils/library.rs
-------------------------------------------------
use std::sync::Arc;

use rspc::{
	alpha::{
		unstable::{MwArgMapper, MwArgMapperMiddleware},
		MwV3,
	},
	ErrorCode,
};
use serde::{de::DeserializeOwned, Deserialize, Serialize};
use specta::Type;
use uuid::Uuid;

use crate::{api::Ctx, library::Library};

/// Can wrap a query argument to require it to contain a `library_id` and provide helpers for working with libraries.
#[derive(Clone, Serialize, Deserialize, Type)]
pub(crate) struct LibraryArgs<T> {
	library_id: Uuid,
	arg: T,
}

pub(crate) struct LibraryArgsLike;
impl MwArgMapper for LibraryArgsLike {
	type Input<T> = LibraryArgs<T> where T: Type + DeserializeOwned + 'static;
	type State = Uuid;

	fn map<T: Serialize + DeserializeOwned + Type + 'static>(
		arg: Self::Input<T>,
	) -> (T, Self::State) {
		(arg.arg, arg.library_id)
	}
}

pub(crate) fn library() -> impl MwV3<Ctx, NewCtx = (Ctx, Arc<Library>)> {
	MwArgMapperMiddleware::<LibraryArgsLike>::new().mount(|mw, ctx: Ctx, library_id| async move {
		let library = ctx
			.libraries
			.get_library(&library_id)
			.await
			.ok_or_else(|| {
				rspc::Error::new(
					ErrorCode::BadRequest,
					"You must specify a valid library to use this operation.".to_string(),
				)
			})?;

		Ok(mw.next((ctx, library)))
	})
}



File: ./src/api/utils/mod.rs
-------------------------------------------------
use std::path::Path;

use tokio::{fs, io};

mod invalidate;
mod library;

pub use invalidate::*;
pub(crate) use library::*;

/// Returns the size of the file or directory
pub async fn get_size(path: impl AsRef<Path>) -> Result<u64, io::Error> {
	let path = path.as_ref();
	let metadata = fs::metadata(path).await?;

	if metadata.is_dir() {
		let mut result = 0;
		let mut to_walk = vec![path.to_path_buf()];

		while let Some(path) = to_walk.pop() {
			let mut read_dir = fs::read_dir(&path).await?;

			while let Some(entry) = read_dir.next_entry().await? {
				let metadata = entry.metadata().await?;
				if metadata.is_dir() {
					to_walk.push(entry.path())
				} else {
					result += metadata.len()
				}
			}
		}

		Ok(result)
	} else {
		Ok(metadata.len())
	}
}



File: ./src/api/web_api.rs
-------------------------------------------------
use rspc::alpha::AlphaRouter;
use serde::{Deserialize, Serialize};
use specta::Type;

use crate::util::http::ensure_response;

use super::{Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router().procedure(
		"sendFeedback",
		R.mutation({
			#[derive(Debug, Type, Serialize, Deserialize)]
			struct Feedback {
				message: String,
				emoji: u8,
			}

			|node, args: Feedback| async move {
				node.add_auth_header(
					node.http
						.post(&format!("{}/api/v1/feedback", &node.env.api_url)),
				)
				.await
				.json(&args)
				.send()
				.await
				.map_err(|_| {
					rspc::Error::new(
						rspc::ErrorCode::InternalServerError,
						"Request failed".to_string(),
					)
				})
				.and_then(ensure_response)?;

				Ok(())
			}
		}),
	)
}



File: ./src/api/files.rs
-------------------------------------------------
use crate::{
	api::utils::library,
	invalidate_query,
	job::Job,
	library::Library,
	location::{
		file_path_helper::{
			file_path_to_isolate, file_path_to_isolate_with_id, FilePathError, IsolatedFilePathData,
		},
		get_location_path_from_location_id, LocationError,
	},
	object::{
		fs::{
			copy::FileCopierJobInit, cut::FileCutterJobInit, delete::FileDeleterJobInit,
			erase::FileEraserJobInit, error::FileSystemJobsError,
			find_available_filename_for_duplicate,
		},
		media::media_data_image_from_prisma_data,
	},
	prisma::{file_path, location, object},
	util::{db::maybe_missing, error::FileIOError},
};

use sd_file_ext::kind::ObjectKind;
use sd_images::ConvertableExtension;
use sd_media_metadata::MediaMetadata;

use std::{
	ffi::OsString,
	path::{Path, PathBuf},
	sync::Arc,
};

use chrono::Utc;
use futures::future::join_all;
use regex::Regex;
use rspc::{alpha::AlphaRouter, ErrorCode};
use serde::Deserialize;
use specta::Type;
use tokio::{fs, io, task::spawn_blocking};
use tracing::{error, warn};

use super::{Ctx, R};

const UNTITLED_FOLDER_STR: &str = "Untitled Folder";

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("get", {
			#[derive(Type, Deserialize)]
			pub struct GetArgs {
				pub id: i32,
			}
			R.with2(library())
				.query(|(_, library), args: GetArgs| async move {
					Ok(library
						.db
						.object()
						.find_unique(object::id::equals(args.id))
						.include(object::include!({ file_paths }))
						.exec()
						.await?)
				})
		})
		.procedure("getMediaData", {
			R.with2(library())
				.query(|(_, library), args: object::id::Type| async move {
					library
						.db
						.object()
						.find_unique(object::id::equals(args))
						.select(object::select!({ id kind media_data }))
						.exec()
						.await?
						.and_then(|obj| {
							Some(match obj.kind {
								Some(v) if v == ObjectKind::Image as i32 => {
									MediaMetadata::Image(Box::new(
										media_data_image_from_prisma_data(obj.media_data?).ok()?,
									))
								}
								_ => return None, // TODO(brxken128): audio and video
							})
						})
						.ok_or_else(|| {
							rspc::Error::new(ErrorCode::NotFound, "Object not found".to_string())
						})
				})
		})
		.procedure("getPath", {
			R.with2(library())
				.query(|(_, library), id: i32| async move {
					let isolated_path = IsolatedFilePathData::try_from(
						library
							.db
							.file_path()
							.find_unique(file_path::id::equals(id))
							.select(file_path_to_isolate::select())
							.exec()
							.await?
							.ok_or(LocationError::FilePath(FilePathError::IdNotFound(id)))?,
					)
					.map_err(LocationError::MissingField)?;

					let location_id = isolated_path.location_id();
					let location_path =
						get_location_path_from_location_id(&library.db, location_id).await?;

					Ok(Path::new(&location_path)
						.join(&isolated_path)
						.to_str()
						.map(|str| str.to_string()))
				})
		})
		.procedure("setNote", {
			#[derive(Type, Deserialize)]
			pub struct SetNoteArgs {
				pub id: i32,
				pub note: Option<String>,
			}

			R.with2(library())
				.mutation(|(_, library), args: SetNoteArgs| async move {
					library
						.db
						.object()
						.update(
							object::id::equals(args.id),
							vec![object::note::set(args.note)],
						)
						.exec()
						.await?;

					invalidate_query!(library, "search.paths");
					invalidate_query!(library, "search.objects");

					Ok(())
				})
		})
		.procedure("setFavorite", {
			#[derive(Type, Deserialize)]
			pub struct SetFavoriteArgs {
				pub id: i32,
				pub favorite: bool,
			}

			R.with2(library())
				.mutation(|(_, library), args: SetFavoriteArgs| async move {
					library
						.db
						.object()
						.update(
							object::id::equals(args.id),
							vec![object::favorite::set(Some(args.favorite))],
						)
						.exec()
						.await?;

					invalidate_query!(library, "search.paths");
					invalidate_query!(library, "search.objects");

					Ok(())
				})
		})
		.procedure("createFolder", {
			#[derive(Type, Deserialize)]
			pub struct CreateFolderArgs {
				pub location_id: location::id::Type,
				pub sub_path: Option<PathBuf>,
				pub name: Option<String>,
			}
			R.with2(library()).mutation(
				|(_, library),
				 CreateFolderArgs {
				     location_id,
				     sub_path,
				     name,
				 }: CreateFolderArgs| async move {
					let mut path =
						get_location_path_from_location_id(&library.db, location_id).await?;

					if let Some(sub_path) = sub_path
						.as_ref()
						.and_then(|sub_path| sub_path.strip_prefix("/").ok())
					{
						path.push(sub_path);
					}

					path.push(name.as_deref().unwrap_or(UNTITLED_FOLDER_STR));

					create_directory(path, &library).await
				},
			)
		})
		.procedure("updateAccessTime", {
			R.with2(library())
				.mutation(|(_, library), ids: Vec<i32>| async move {
					library
						.db
						.object()
						.update_many(
							vec![object::id::in_vec(ids)],
							vec![object::date_accessed::set(Some(Utc::now().into()))],
						)
						.exec()
						.await?;

					invalidate_query!(library, "search.paths");
					invalidate_query!(library, "search.objects");
					Ok(())
				})
		})
		.procedure("removeAccessTime", {
			R.with2(library())
				.mutation(|(_, library), object_ids: Vec<i32>| async move {
					library
						.db
						.object()
						.update_many(
							vec![object::id::in_vec(object_ids)],
							vec![object::date_accessed::set(None)],
						)
						.exec()
						.await?;

					invalidate_query!(library, "search.objects");
					invalidate_query!(library, "search.paths");
					Ok(())
				})
		})
		// .procedure("encryptFiles", {
		// 	R.with2(library())
		// 		.mutation(|(node, library), args: FileEncryptorJobInit| async move {
		// 			Job::new(args).spawn(&node, &library).await.map_err(Into::into)
		// 		})
		// })
		// .procedure("decryptFiles", {
		// 	R.with2(library())
		// 		.mutation(|(node, library), args: FileDecryptorJobInit| async move {
		// 			Job::new(args).spawn(&node, &library).await.map_err(Into::into)
		// 		})
		// })
		.procedure("deleteFiles", {
			R.with2(library())
				.mutation(|(node, library), args: FileDeleterJobInit| async move {
					match args.file_path_ids.len() {
						0 => Ok(()),
						1 => {
							let (maybe_location, maybe_file_path) = library
								.db
								._batch((
									library
										.db
										.location()
										.find_unique(location::id::equals(args.location_id))
										.select(location::select!({ path })),
									library
										.db
										.file_path()
										.find_unique(file_path::id::equals(args.file_path_ids[0]))
										.select(file_path_to_isolate::select()),
								))
								.await?;

							let location_path = maybe_location
								.ok_or(LocationError::IdNotFound(args.location_id))?
								.path
								.ok_or(LocationError::MissingPath(args.location_id))?;

							let file_path = maybe_file_path.ok_or(LocationError::FilePath(
								FilePathError::IdNotFound(args.file_path_ids[0]),
							))?;

							let full_path = Path::new(&location_path).join(
								IsolatedFilePathData::try_from(&file_path)
									.map_err(LocationError::MissingField)?,
							);

							match if maybe_missing(file_path.is_dir, "file_path.is_dir")
								.map_err(LocationError::MissingField)?
							{
								fs::remove_dir_all(&full_path).await
							} else {
								fs::remove_file(&full_path).await
							} {
								Ok(()) => Ok(()),
								Err(e) if e.kind() == io::ErrorKind::NotFound => {
									warn!(
										"File not found in the file system, will remove from database: {}",
										full_path.display()
									);
									library
										.db
										.file_path()
										.delete(file_path::id::equals(args.file_path_ids[0]))
										.exec()
										.await
										.map_err(LocationError::from)?;

									Ok(())
								}
								Err(e) => {
									Err(LocationError::from(FileIOError::from((full_path, e)))
										.into())
								}
							}
						}
						_ => Job::new(args)
							.spawn(&node, &library)
							.await
							.map_err(Into::into),
					}
				})
		})
		.procedure("convertImage", {
			#[derive(Type, Deserialize)]
			struct ConvertImageArgs {
				location_id: location::id::Type,
				file_path_id: file_path::id::Type,
				delete_src: bool, // if set, we delete the src image after
				desired_extension: ConvertableExtension,
				quality_percentage: Option<i32>, // 1% - 125%
			}
			R.with2(library())
				.mutation(|(_, library), args: ConvertImageArgs| async move {
					// TODO:(fogodev) I think this will have to be a Job due to possibly being too much CPU Bound for rspc

					let location_path =
						get_location_path_from_location_id(&library.db, args.location_id).await?;

					let isolated_path = IsolatedFilePathData::try_from(
						library
							.db
							.file_path()
							.find_unique(file_path::id::equals(args.file_path_id))
							.select(file_path_to_isolate::select())
							.exec()
							.await?
							.ok_or(LocationError::FilePath(FilePathError::IdNotFound(
								args.file_path_id,
							)))?,
					)?;

					let path = Path::new(&location_path).join(&isolated_path);

					if let Err(e) = fs::metadata(&path).await {
						if e.kind() == io::ErrorKind::NotFound {
							return Err(LocationError::FilePath(FilePathError::NotFound(
								path.into_boxed_path(),
							))
							.into());
						} else {
							return Err(FileIOError::from((
								path,
								e,
								"Got an error trying to read metadata from image to convert",
							))
							.into());
						}
					}

					args.quality_percentage.map(|x| x.clamp(1, 125));

					let path = Arc::new(path);

					let output_extension =
						Arc::new(OsString::from(args.desired_extension.to_string()));

					// TODO(fogodev): Refactor this if Rust get async scoped spawns someday
					let inner_path = Arc::clone(&path);
					let inner_output_extension = Arc::clone(&output_extension);
					let image = spawn_blocking(move || {
						sd_images::convert_image(inner_path.as_ref(), &inner_output_extension).map(
							|mut image| {
								if let Some(quality_percentage) = args.quality_percentage {
									image = image.resize(
										image.width()
											* (quality_percentage as f32 / 100_f32) as u32,
										image.height()
											* (quality_percentage as f32 / 100_f32) as u32,
										image::imageops::FilterType::Triangle,
									);
								}
								image
							},
						)
					})
					.await
					.map_err(|e| {
						error!("{e:#?}");
						rspc::Error::new(
							ErrorCode::InternalServerError,
							"Had an internal problem converting image".to_string(),
						)
					})??;

					let output_path = path.with_extension(output_extension.as_ref());

					if fs::metadata(&output_path)
						.await
						.map(|_| true)
						.map_err(|e| {
							FileIOError::from(
							(
								&output_path,
								e,
								"Got an error trying to check if the desired converted file already exists"
							)
						)
						})? {
						return Err(rspc::Error::new(
							ErrorCode::Conflict,
							"There is already a file with same name and extension in this directory"
								.to_string(),
						));
					} else {
						fs::write(&output_path, image.as_bytes())
							.await
							.map_err(|e| {
								FileIOError::from((
									output_path,
									e,
									"There was an error while writing the image to the output path",
								))
							})?;
					}

					if args.delete_src {
						fs::remove_file(path.as_ref()).await.map_err(|e| {
							// Let's also invalidate the query here, because we succeeded in converting the file
							invalidate_query!(library, "search.paths");
							invalidate_query!(library, "search.objects");

							FileIOError::from((
								path.as_ref(),
								e,
								"There was an error while deleting the source image",
							))
						})?;
					}

					invalidate_query!(library, "search.paths");
					invalidate_query!(library, "search.objects");

					Ok(())
				})
		})
		.procedure("getConvertableImageExtensions", {
			R.query(|_, _: ()| async move { Ok(sd_images::all_compatible_extensions()) })
		})
		.procedure("eraseFiles", {
			R.with2(library())
				.mutation(|(node, library), args: FileEraserJobInit| async move {
					Job::new(args)
						.spawn(&node, &library)
						.await
						.map_err(Into::into)
				})
		})
		.procedure("copyFiles", {
			R.with2(library())
				.mutation(|(node, library), args: FileCopierJobInit| async move {
					Job::new(args)
						.spawn(&node, &library)
						.await
						.map_err(Into::into)
				})
		})
		.procedure("cutFiles", {
			R.with2(library())
				.mutation(|(node, library), args: FileCutterJobInit| async move {
					Job::new(args)
						.spawn(&node, &library)
						.await
						.map_err(Into::into)
				})
		})
		.procedure("renameFile", {
			#[derive(Type, Deserialize)]
			pub struct RenameOne {
				pub from_file_path_id: file_path::id::Type,
				pub to: String,
			}

			#[derive(Type, Deserialize)]
			pub struct RenameMany {
				pub from_pattern: FromPattern,
				pub to_pattern: String,
				pub from_file_path_ids: Vec<file_path::id::Type>,
			}

			#[derive(Type, Deserialize)]
			pub enum RenameKind {
				One(RenameOne),
				Many(RenameMany),
			}

			#[derive(Type, Deserialize)]
			pub struct RenameFileArgs {
				pub location_id: location::id::Type,
				pub kind: RenameKind,
			}

			impl RenameFileArgs {
				pub async fn rename_one(
					RenameOne {
						from_file_path_id,
						to,
					}: RenameOne,
					location_path: impl AsRef<Path>,
					library: &Library,
				) -> Result<(), rspc::Error> {
					let location_path = location_path.as_ref();
					let iso_file_path = IsolatedFilePathData::try_from(
						library
							.db
							.file_path()
							.find_unique(file_path::id::equals(from_file_path_id))
							.select(file_path_to_isolate::select())
							.exec()
							.await?
							.ok_or(LocationError::FilePath(FilePathError::IdNotFound(
								from_file_path_id,
							)))?,
					)
					.map_err(LocationError::MissingField)?;

					if iso_file_path.full_name() == to {
						return Ok(());
					}

					let (new_file_name, new_extension) =
						IsolatedFilePathData::separate_name_and_extension_from_str(&to)
							.map_err(LocationError::FilePath)?;

					let mut new_file_full_path = location_path.join(iso_file_path.parent());
					if !new_extension.is_empty() {
						new_file_full_path.push(format!("{}.{}", new_file_name, new_extension));
					} else {
						new_file_full_path.push(new_file_name);
					}

					match fs::metadata(&new_file_full_path).await {
						Ok(_) => {
							return Err(rspc::Error::new(
								ErrorCode::Conflict,
								"Renaming would overwrite a file".to_string(),
							));
						}

						Err(e) => {
							if e.kind() != std::io::ErrorKind::NotFound {
								return Err(rspc::Error::with_cause(
									ErrorCode::InternalServerError,
									"Failed to check if file exists".to_string(),
									e,
								));
							}

							fs::rename(location_path.join(&iso_file_path), new_file_full_path)
								.await
								.map_err(|e| {
									rspc::Error::with_cause(
										ErrorCode::InternalServerError,
										"Failed to rename file".to_string(),
										e,
									)
								})?;
						}
					}

					Ok(())
				}

				pub async fn rename_many(
					RenameMany {
						from_pattern,
						to_pattern,
						from_file_path_ids,
					}: RenameMany,
					location_path: impl AsRef<Path>,
					library: &Library,
				) -> Result<(), rspc::Error> {
					let location_path = location_path.as_ref();

					let Ok(from_regex) = Regex::new(&from_pattern.pattern) else {
						return Err(rspc::Error::new(
							rspc::ErrorCode::BadRequest,
							"Invalid `from` regex pattern".into(),
						));
					};

					let errors = join_all(
						library
							.db
							.file_path()
							.find_many(vec![file_path::id::in_vec(from_file_path_ids)])
							.select(file_path_to_isolate_with_id::select())
							.exec()
							.await?
							.into_iter()
							.flat_map(IsolatedFilePathData::try_from)
							.map(|iso_file_path| {
								let from = location_path.join(&iso_file_path);
								let mut to = location_path.join(iso_file_path.parent());
								let full_name = iso_file_path.full_name();
								let replaced_full_name = if from_pattern.replace_all {
									from_regex.replace_all(&full_name, &to_pattern)
								} else {
									from_regex.replace(&full_name, &to_pattern)
								}
								.to_string();

								to.push(&replaced_full_name);

								async move {
									if !IsolatedFilePathData::accept_file_name(&replaced_full_name)
									{
										Err(rspc::Error::new(
											ErrorCode::BadRequest,
											"Invalid file name".to_string(),
										))
									} else {
										fs::rename(&from, &to).await.map_err(|e| {
											error!(
													"Failed to rename file from: '{}' to: '{}'; Error: {e:#?}",
													from.display(),
													to.display()
												);
											rspc::Error::with_cause(
												ErrorCode::Conflict,
												"Failed to rename file".to_string(),
												e,
											)
										})
									}
								}
							}),
					)
					.await
					.into_iter()
					.filter_map(Result::err)
					.collect::<Vec<_>>();

					if !errors.is_empty() {
						return Err(rspc::Error::new(
							rspc::ErrorCode::Conflict,
							errors
								.into_iter()
								.map(|e| e.to_string())
								.collect::<Vec<_>>()
								.join("\n"),
						));
					}

					Ok(())
				}
			}

			R.with2(library()).mutation(
				|(_, library), RenameFileArgs { location_id, kind }: RenameFileArgs| async move {
					let location_path =
						get_location_path_from_location_id(&library.db, location_id).await?;

					let res = match kind {
						RenameKind::One(one) => {
							RenameFileArgs::rename_one(one, location_path, &library).await
						}
						RenameKind::Many(many) => {
							RenameFileArgs::rename_many(many, location_path, &library).await
						}
					};

					invalidate_query!(library, "search.paths");
					invalidate_query!(library, "search.objects");

					res
				},
			)
		})
}

pub(super) async fn create_directory(
	mut target_path: PathBuf,
	library: &Library,
) -> Result<String, rspc::Error> {
	match fs::metadata(&target_path).await {
		Ok(metadata) if metadata.is_dir() => {
			target_path = find_available_filename_for_duplicate(&target_path).await?;
		}
		Ok(_) => {
			return Err(FileSystemJobsError::WouldOverwrite(target_path.into_boxed_path()).into())
		}
		Err(e) if e.kind() == io::ErrorKind::NotFound => {
			// Everything is awesome!
		}
		Err(e) => {
			return Err(FileIOError::from((
				target_path,
				e,
				"Failed to access file system and get metadata on directory to be created",
			))
			.into())
		}
	};

	fs::create_dir(&target_path)
		.await
		.map_err(|e| FileIOError::from((&target_path, e, "Failed to create directory")))?;

	invalidate_query!(library, "search.objects");
	invalidate_query!(library, "search.paths");
	invalidate_query!(library, "search.ephemeralPaths");

	Ok(target_path
		.file_name()
		.expect("Failed to get file name")
		.to_string_lossy()
		.to_string())
}

#[derive(Type, Deserialize)]
pub struct FromPattern {
	pub pattern: String,
	pub replace_all: bool,
}



File: ./src/api/locations.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::StatefulJob,
	location::{
		delete_location, find_location,
		indexer::{rules::IndexerRuleCreateArgs, IndexerJobInit},
		light_scan_location, location_with_indexer_rules,
		non_indexed::NonIndexedPathItem,
		relink_location, scan_location, scan_location_sub_path, LocationCreateArgs, LocationError,
		LocationUpdateArgs,
	},
	object::file_identifier::file_identifier_job::FileIdentifierJobInit,
	p2p::PeerMetadata,
	prisma::{file_path, indexer_rule, indexer_rules_in_location, location, object, SortOrder},
	util::AbortOnDrop,
};

use std::path::{Path, PathBuf};

use chrono::{DateTime, Utc};
use directories::UserDirs;
use rspc::{self, alpha::AlphaRouter, ErrorCode};
use serde::{Deserialize, Serialize};
use specta::Type;
use tracing::error;

use super::{utils::library, Ctx, R};

#[derive(Serialize, Type, Debug)]
#[serde(tag = "type")]
pub enum ExplorerItem {
	Path {
		// has_local_thumbnail is true only if there is local existence of a thumbnail
		has_local_thumbnail: bool,
		// thumbnail_key is present if there is a cas_id
		// it includes the shard hex formatted as (["f0", "cab34a76fbf3469f"])
		thumbnail_key: Option<Vec<String>>,
		item: file_path_with_object::Data,
	},
	Object {
		has_local_thumbnail: bool,
		thumbnail_key: Option<Vec<String>>,
		item: object_with_file_paths::Data,
	},
	Location {
		has_local_thumbnail: bool,
		thumbnail_key: Option<Vec<String>>,
		item: location::Data,
	},
	NonIndexedPath {
		has_local_thumbnail: bool,
		thumbnail_key: Option<Vec<String>>,
		item: NonIndexedPathItem,
	},
	SpacedropPeer {
		has_local_thumbnail: bool,
		thumbnail_key: Option<Vec<String>>,
		item: PeerMetadata,
	},
}
#[derive(Serialize, Type, Debug)]
pub struct SystemLocations {
	desktop: Option<PathBuf>,
	documents: Option<PathBuf>,
	downloads: Option<PathBuf>,
	pictures: Option<PathBuf>,
	music: Option<PathBuf>,
	videos: Option<PathBuf>,
}

impl From<UserDirs> for SystemLocations {
	fn from(value: UserDirs) -> Self {
		Self {
			desktop: value.desktop_dir().map(Path::to_path_buf),
			documents: value.document_dir().map(Path::to_path_buf),
			downloads: value.download_dir().map(Path::to_path_buf),
			pictures: value.picture_dir().map(Path::to_path_buf),
			music: value.audio_dir().map(Path::to_path_buf),
			videos: value.video_dir().map(Path::to_path_buf),
		}
	}
}

impl ExplorerItem {
	pub fn name(&self) -> &str {
		match self {
			ExplorerItem::Path {
				item: file_path_with_object::Data { name, .. },
				..
			}
			| ExplorerItem::Location {
				item: location::Data { name, .. },
				..
			} => name.as_deref().unwrap_or(""),
			ExplorerItem::NonIndexedPath { item, .. } => item.name.as_str(),
			_ => "",
		}
	}

	pub fn size_in_bytes(&self) -> u64 {
		match self {
			ExplorerItem::Path {
				item: file_path_with_object::Data {
					size_in_bytes_bytes,
					..
				},
				..
			} => size_in_bytes_bytes
				.as_ref()
				.map(|size| {
					u64::from_be_bytes([
						size[0], size[1], size[2], size[3], size[4], size[5], size[6], size[7],
					])
				})
				.unwrap_or(0),

			ExplorerItem::NonIndexedPath {
				item: NonIndexedPathItem {
					size_in_bytes_bytes,
					..
				},
				..
			} => u64::from_be_bytes([
				size_in_bytes_bytes[0],
				size_in_bytes_bytes[1],
				size_in_bytes_bytes[2],
				size_in_bytes_bytes[3],
				size_in_bytes_bytes[4],
				size_in_bytes_bytes[5],
				size_in_bytes_bytes[6],
				size_in_bytes_bytes[7],
			]),
			_ => 0,
		}
	}

	pub fn date_created(&self) -> DateTime<Utc> {
		match self {
			ExplorerItem::Path {
				item: file_path_with_object::Data { date_created, .. },
				..
			}
			| ExplorerItem::Object {
				item: object_with_file_paths::Data { date_created, .. },
				..
			}
			| ExplorerItem::Location {
				item: location::Data { date_created, .. },
				..
			} => date_created.map(Into::into).unwrap_or_default(),

			ExplorerItem::NonIndexedPath { item, .. } => item.date_created,
			_ => Default::default(),
		}
	}

	pub fn date_modified(&self) -> DateTime<Utc> {
		match self {
			ExplorerItem::Path { item, .. } => {
				item.date_modified.map(Into::into).unwrap_or_default()
			}
			ExplorerItem::NonIndexedPath { item, .. } => item.date_modified,
			_ => Default::default(),
		}
	}
}

file_path::include!(file_path_with_object { object });
object::include!(object_with_file_paths { file_paths });

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("list", {
			R.with2(library()).query(|(_, library), _: ()| async move {
				Ok(library
					.db
					.location()
					.find_many(vec![])
					.order_by(location::date_created::order(SortOrder::Desc))
					.exec()
					.await?)
			})
		})
		.procedure("get", {
			R.with2(library())
				.query(|(_, library), location_id: location::id::Type| async move {
					Ok(library
						.db
						.location()
						.find_unique(location::id::equals(location_id))
						.exec()
						.await?)
				})
		})
		.procedure("getWithRules", {
			R.with2(library())
				.query(|(_, library), location_id: location::id::Type| async move {
					Ok(library
						.db
						.location()
						.find_unique(location::id::equals(location_id))
						.include(location_with_indexer_rules::include())
						.exec()
						.await?)
				})
		})
		.procedure("create", {
			R.with2(library())
				.mutation(|(node, library), args: LocationCreateArgs| async move {
					if let Some(location) = args.create(&node, &library).await? {
						let id = Some(location.id);
						scan_location(&node, &library, location).await?;
						invalidate_query!(library, "locations.list");
						Ok(id)
					} else {
						Ok(None)
					}
				})
		})
		.procedure("update", {
			R.with2(library())
				.mutation(|(node, library), args: LocationUpdateArgs| async move {
					let ret = args.update(&node, &library).await.map_err(Into::into);
					invalidate_query!(library, "locations.list");
					ret
				})
		})
		.procedure("delete", {
			R.with2(library()).mutation(
				|(node, library), location_id: location::id::Type| async move {
					delete_location(&node, &library, location_id).await?;
					invalidate_query!(library, "locations.list");
					Ok(())
				},
			)
		})
		.procedure("relink", {
			R.with2(library())
				.mutation(|(_, library), location_path: PathBuf| async move {
					relink_location(&library, location_path)
						.await
						.map_err(Into::into)
				})
		})
		.procedure("addLibrary", {
			R.with2(library())
				.mutation(|(node, library), args: LocationCreateArgs| async move {
					if let Some(location) = args.add_library(&node, &library).await? {
						let id = location.id;
						scan_location(&node, &library, location).await?;
						invalidate_query!(library, "locations.list");
						Ok(Some(id))
					} else {
						Ok(None)
					}
				})
		})
		.procedure("fullRescan", {
			#[derive(Type, Deserialize)]
			pub struct FullRescanArgs {
				pub location_id: location::id::Type,
				pub reidentify_objects: bool,
			}

			R.with2(library()).mutation(
				|(node, library),
				 FullRescanArgs {
				     location_id,
				     reidentify_objects,
				 }| async move {
					if reidentify_objects {
						library
							.db
							.file_path()
							.update_many(
								vec![
									file_path::location_id::equals(Some(location_id)),
									file_path::object_id::not(None),
									file_path::cas_id::not(None),
								],
								vec![
									file_path::object::disconnect(),
									file_path::cas_id::set(None),
								],
							)
							.exec()
							.await?;

						library.orphan_remover.invoke().await;
					}

					// rescan location
					scan_location(
						&node,
						&library,
						find_location(&library, location_id)
							.include(location_with_indexer_rules::include())
							.exec()
							.await?
							.ok_or(LocationError::IdNotFound(location_id))?,
					)
					.await
					.map_err(Into::into)
				},
			)
		})
		.procedure("subPathRescan", {
			#[derive(Clone, Serialize, Deserialize, Type, Debug)]
			pub struct RescanArgs {
				pub location_id: location::id::Type,
				pub sub_path: String,
			}

			R.with2(library()).mutation(
				|(node, library),
				 RescanArgs {
				     location_id,
				     sub_path,
				 }: RescanArgs| async move {
					scan_location_sub_path(
						&node,
						&library,
						find_location(&library, location_id)
							.include(location_with_indexer_rules::include())
							.exec()
							.await?
							.ok_or(LocationError::IdNotFound(location_id))?,
						sub_path,
					)
					.await
					.map_err(Into::into)
				},
			)
		})
		.procedure("quickRescan", {
			#[derive(Clone, Serialize, Deserialize, Type, Debug)]
			pub struct LightScanArgs {
				pub location_id: location::id::Type,
				pub sub_path: String,
			}

			R.with2(library()).subscription(
				|(node, library),
				 LightScanArgs {
				     location_id,
				     sub_path,
				 }: LightScanArgs| async move {
					if node
						.jobs
						.has_job_running(|job_identity| {
							job_identity.target_location == location_id
								&& (job_identity.name == <IndexerJobInit as StatefulJob>::NAME
									|| job_identity.name
										== <FileIdentifierJobInit as StatefulJob>::NAME)
						})
						.await
					{
						return Err(rspc::Error::new(
							ErrorCode::Conflict,
							"We're still indexing this location, pleases wait a bit...".to_string(),
						));
					}

					let location = find_location(&library, location_id)
						.include(location_with_indexer_rules::include())
						.exec()
						.await?
						.ok_or(LocationError::IdNotFound(location_id))?;

					let handle = tokio::spawn(async move {
						if let Err(e) = light_scan_location(node, library, location, sub_path).await
						{
							error!("light scan error: {e:#?}");
						}
					});

					Ok(AbortOnDrop(handle))
				},
			)
		})
		.procedure(
			"online",
			R.subscription(|node, _: ()| async move {
				let mut rx = node.locations.online_rx();

				async_stream::stream! {
					let online = node.locations.get_online().await;

					yield online;

					while let Ok(locations) = rx.recv().await {
						yield locations;
					}
				}
			}),
		)
		.procedure("systemLocations", {
			R.query(|_, _: ()| async move {
				UserDirs::new().map(SystemLocations::from).ok_or_else(|| {
					rspc::Error::new(
						ErrorCode::NotFound,
						"Didn't find any system locations".to_string(),
					)
				})
			})
		})
		.merge("indexer_rules.", mount_indexer_rule_routes())
}

fn mount_indexer_rule_routes() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("create", {
			R.with2(library())
				.mutation(|(_, library), args: IndexerRuleCreateArgs| async move {
					if args.create(&library).await?.is_some() {
						invalidate_query!(library, "locations.indexer_rules.list");
					}

					Ok(())
				})
		})
		.procedure("delete", {
			R.with2(library())
				.mutation(|(_, library), indexer_rule_id: i32| async move {
					let indexer_rule_db = library.db.indexer_rule();

					if let Some(indexer_rule) = indexer_rule_db
						.to_owned()
						.find_unique(indexer_rule::id::equals(indexer_rule_id))
						.exec()
						.await?
					{
						if indexer_rule.default.unwrap_or_default() {
							return Err(rspc::Error::new(
								ErrorCode::Forbidden,
								format!("Indexer rule <id={indexer_rule_id}> can't be deleted"),
							));
						}
					} else {
						return Err(rspc::Error::new(
							ErrorCode::NotFound,
							format!("Indexer rule <id={indexer_rule_id}> not found"),
						));
					}

					library
						.db
						.indexer_rules_in_location()
						.delete_many(vec![indexer_rules_in_location::indexer_rule_id::equals(
							indexer_rule_id,
						)])
						.exec()
						.await?;

					indexer_rule_db
						.delete(indexer_rule::id::equals(indexer_rule_id))
						.exec()
						.await?;

					invalidate_query!(library, "locations.indexer_rules.list");

					Ok(())
				})
		})
		.procedure("get", {
			R.with2(library())
				.query(|(_, library), indexer_rule_id: i32| async move {
					library
						.db
						.indexer_rule()
						.find_unique(indexer_rule::id::equals(indexer_rule_id))
						.exec()
						.await?
						.ok_or_else(|| {
							rspc::Error::new(
								ErrorCode::NotFound,
								format!("Indexer rule <id={indexer_rule_id}> not found"),
							)
						})
				})
		})
		.procedure("list", {
			R.with2(library()).query(|(_, library), _: ()| async move {
				library
					.db
					.indexer_rule()
					.find_many(vec![])
					.exec()
					.await
					.map_err(Into::into)
			})
		})
		// list indexer rules for location, returning the indexer rule
		.procedure("listForLocation", {
			R.with2(library())
				.query(|(_, library), location_id: location::id::Type| async move {
					library
						.db
						.indexer_rule()
						.find_many(vec![indexer_rule::locations::some(vec![
							indexer_rules_in_location::location_id::equals(location_id),
						])])
						.exec()
						.await
						.map_err(Into::into)
				})
		})
}



File: ./src/api/search/media_data.rs
-------------------------------------------------
use sd_prisma::prisma::{self, media_data};
use serde::{Deserialize, Serialize};
use specta::Type;

use super::utils::*;

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase", tag = "field", content = "value")]
pub enum MediaDataOrder {
	EpochTime(SortOrder),
}

impl MediaDataOrder {
	pub fn get_sort_order(&self) -> prisma::SortOrder {
		(*match self {
			Self::EpochTime(v) => v,
		})
		.into()
	}

	pub fn into_param(self) -> media_data::OrderByWithRelationParam {
		let dir = self.get_sort_order();
		use media_data::*;
		match self {
			Self::EpochTime(_) => epoch_time::order(dir),
		}
	}
}



File: ./src/api/search/object.rs
-------------------------------------------------
use chrono::{DateTime, FixedOffset};
use prisma_client_rust::not;
use prisma_client_rust::{or, OrderByQuery, PaginatedQuery, WhereQuery};
use sd_prisma::prisma::{self, object, tag_on_object};
use serde::{Deserialize, Serialize};
use specta::Type;

// use crate::library::Category;

use super::media_data::*;
use super::utils::{self, *};

#[derive(Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub enum ObjectCursor {
	None,
	DateAccessed(CursorOrderItem<DateTime<FixedOffset>>),
	Kind(CursorOrderItem<i32>),
}

impl ObjectCursor {
	fn apply(self, query: &mut object::FindManyQuery, id: i32) {
		macro_rules! arm {
			($field:ident, $item:ident) => {{
				let item = $item;

				let data = item.data.clone();

				query.add_where(or![
					match item.order {
						SortOrder::Asc => prisma::object::$field::gt(data),
						SortOrder::Desc => prisma::object::$field::lt(data),
					},
					prisma_client_rust::and![
						prisma::object::$field::equals(Some(item.data)),
						match item.order {
							SortOrder::Asc => prisma::object::id::gt(id),
							SortOrder::Desc => prisma::object::id::lt(id),
						}
					]
				]);

				query.add_order_by(prisma::object::$field::order(item.order.into()));
			}};
		}

		match self {
			Self::None => {
				query.add_where(prisma::object::id::gt(id));
			}
			Self::Kind(item) => arm!(kind, item),
			Self::DateAccessed(item) => arm!(date_accessed, item),
		}
	}
}

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase", tag = "field", content = "value")]
pub enum ObjectOrder {
	DateAccessed(SortOrder),
	Kind(SortOrder),
	MediaData(Box<MediaDataOrder>),
}

impl ObjectOrder {
	pub fn get_sort_order(&self) -> prisma::SortOrder {
		(*match self {
			Self::DateAccessed(v) => v,
			Self::Kind(v) => v,
			Self::MediaData(v) => return v.get_sort_order(),
		})
		.into()
	}

	pub fn into_param(self) -> object::OrderByWithRelationParam {
		let dir = self.get_sort_order();
		use object::*;

		match self {
			Self::DateAccessed(_) => date_accessed::order(dir),
			Self::Kind(_) => kind::order(dir),
			Self::MediaData(v) => media_data::order(vec![v.into_param()]),
		}
	}
}

#[derive(Serialize, Deserialize, Type, Debug, Default, Clone, Copy)]
#[serde(rename_all = "camelCase")]
pub enum ObjectHiddenFilter {
	#[default]
	Exclude,
	Include,
}

impl ObjectHiddenFilter {
	pub fn to_param(self) -> Option<object::WhereParam> {
		match self {
			ObjectHiddenFilter::Exclude => Some(or![
				object::hidden::equals(None),
				object::hidden::not(Some(true))
			]),
			ObjectHiddenFilter::Include => None,
		}
	}
}

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase")]
pub enum ObjectFilterArgs {
	Favorite(bool),
	Hidden(ObjectHiddenFilter),
	Kind(InOrNotIn<i32>),
	Tags(InOrNotIn<i32>),
	DateAccessed(Range<chrono::DateTime<FixedOffset>>),
}

impl ObjectFilterArgs {
	pub fn into_params(self) -> Vec<object::WhereParam> {
		use object::*;

		match self {
			Self::Favorite(v) => vec![favorite::equals(Some(v))],
			Self::Hidden(v) => v.to_param().map(|v| vec![v]).unwrap_or_default(),
			Self::Tags(v) => v
				.to_param(
					|v| tags::some(vec![tag_on_object::tag_id::in_vec(v)]),
					|v| tags::none(vec![tag_on_object::tag_id::in_vec(v)]),
				)
				.map(|v| vec![v])
				.unwrap_or_default(),
			Self::Kind(v) => v
				.to_param(kind::in_vec, kind::not_in_vec)
				.map(|v| vec![v])
				.unwrap_or_default(),
			Self::DateAccessed(v) => {
				vec![
					not![date_accessed::equals(None)],
					match v {
						Range::From(v) => date_accessed::gte(v),
						Range::To(v) => date_accessed::lte(v),
					},
				]
			}
		}
	}
}

pub type OrderAndPagination =
	utils::OrderAndPagination<prisma::object::id::Type, ObjectOrder, ObjectCursor>;

impl OrderAndPagination {
	pub fn apply(self, query: &mut object::FindManyQuery) {
		match self {
			Self::OrderOnly(order) => {
				query.add_order_by(order.into_param());
			}
			Self::Offset { offset, order } => {
				query.set_skip(offset as i64);

				if let Some(order) = order {
					query.add_order_by(order.into_param())
				}
			}
			Self::Cursor { id, cursor } => {
				cursor.apply(query, id);

				query.add_order_by(prisma::object::pub_id::order(prisma::SortOrder::Asc))
			}
		}
	}
}



File: ./src/api/search/mod.rs
-------------------------------------------------
pub mod file_path;
pub mod media_data;
pub mod object;
pub mod saved;
mod utils;

pub use self::{file_path::*, object::*, utils::*};

use crate::{
	api::{
		locations::{file_path_with_object, object_with_file_paths, ExplorerItem},
		utils::library,
	},
	library::Library,
	location::{non_indexed, LocationError},
	object::media::thumbnail::get_indexed_thumb_key,
};

use std::path::PathBuf;

use rspc::{alpha::AlphaRouter, ErrorCode};
use sd_prisma::prisma::{self, PrismaClient};
use serde::{Deserialize, Serialize};
use specta::Type;

use super::{Ctx, R};

const MAX_TAKE: u8 = 100;

#[derive(Serialize, Type, Debug)]
struct SearchData<T> {
	cursor: Option<Vec<u8>>,
	items: Vec<T>,
}

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase")]
pub enum SearchFilterArgs {
	FilePath(FilePathFilterArgs),
	Object(ObjectFilterArgs),
}

impl SearchFilterArgs {
	async fn into_params<T>(
		self,
		db: &PrismaClient,
		file_path: fn(Vec<prisma::file_path::WhereParam>) -> Vec<T>,
		object: fn(Vec<prisma::object::WhereParam>) -> Vec<T>,
	) -> Result<Vec<T>, rspc::Error> {
		Ok(match self {
			Self::FilePath(v) => file_path(v.into_params(db).await?),
			Self::Object(v) => object(v.into_params()),
		})
	}

	async fn into_file_path_params(
		self,
		db: &PrismaClient,
	) -> Result<Vec<prisma::file_path::WhereParam>, rspc::Error> {
		self.into_params(db, |v| v, |v| vec![prisma::file_path::object::is(v)])
			.await
	}

	async fn into_object_params(
		self,
		db: &PrismaClient,
	) -> Result<Vec<prisma::object::WhereParam>, rspc::Error> {
		self.into_params(db, |v| vec![prisma::object::file_paths::some(v)], |v| v)
			.await
	}
}

pub fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("ephemeralPaths", {
			#[derive(Serialize, Deserialize, Type, Debug, Clone)]
			#[serde(rename_all = "camelCase", tag = "field", content = "value")]
			enum EphemeralPathOrder {
				Name(SortOrder),
				SizeInBytes(SortOrder),
				DateCreated(SortOrder),
				DateModified(SortOrder),
			}

			#[derive(Deserialize, Type, Debug)]
			#[serde(rename_all = "camelCase")]
			struct EphemeralPathSearchArgs {
				path: PathBuf,
				with_hidden_files: bool,
				#[specta(optional)]
				order: Option<EphemeralPathOrder>,
			}

			R.with2(library()).query(
				|(node, library),
				 EphemeralPathSearchArgs {
				     path,
				     with_hidden_files,
				     order,
				 }| async move {
					let mut paths =
						non_indexed::walk(path, with_hidden_files, node, library).await?;

					macro_rules! order_match {
						($order:ident, [$(($variant:ident, |$i:ident| $func:expr)),+]) => {{
							match $order {
								$(EphemeralPathOrder::$variant(order) => {
									paths.entries.sort_unstable_by(|path1, path2| {
										let func = |$i: &ExplorerItem| $func;

										let one = func(path1);
										let two = func(path2);

										match order {
											SortOrder::Desc => two.cmp(&one),
											SortOrder::Asc => one.cmp(&two),
										}
									});
								})+
							}
						}};
					}

					if let Some(order) = order {
						order_match!(
							order,
							[
								(Name, |p| p.name().to_lowercase()),
								(SizeInBytes, |p| p.size_in_bytes()),
								(DateCreated, |p| p.date_created()),
								(DateModified, |p| p.date_modified())
							]
						)
					}

					Ok(paths)
				},
			)
		})
		.procedure("paths", {
			#[derive(Deserialize, Type, Debug)]
			#[serde(rename_all = "camelCase")]
			struct FilePathSearchArgs {
				#[specta(optional)]
				take: Option<u8>,
				#[specta(optional)]
				order_and_pagination: Option<file_path::OrderAndPagination>,
				#[serde(default)]
				filters: Vec<SearchFilterArgs>,
				#[serde(default = "default_group_directories")]
				group_directories: bool,
			}

			fn default_group_directories() -> bool {
				true
			}

			R.with2(library()).query(
				|(node, library),
				 FilePathSearchArgs {
				     take,
				     order_and_pagination,
				     filters,
				     group_directories,
				 }| async move {
					let Library { db, .. } = library.as_ref();

					let mut query = db.file_path().find_many({
						let mut params = Vec::new();

						for filter in filters {
							params.extend(filter.into_file_path_params(db).await?);
						}

						params
					});

					if let Some(take) = take {
						query = query.take(take as i64);
					}

					// WARN: this order_by for grouping directories MUST always come before the other order_by
					if group_directories {
						query = query
							.order_by(prisma::file_path::is_dir::order(prisma::SortOrder::Desc));
					}

					// WARN: this order_by for sorting data MUST always come after the other order_by
					if let Some(order_and_pagination) = order_and_pagination {
						order_and_pagination.apply(&mut query, group_directories)
					}

					let file_paths = query
						.include(file_path_with_object::include())
						.exec()
						.await?;

					let mut items = Vec::with_capacity(file_paths.len());

					for file_path in file_paths {
						let thumbnail_exists_locally = if let Some(cas_id) = &file_path.cas_id {
							library
								.thumbnail_exists(&node, cas_id)
								.await
								.map_err(LocationError::from)?
						} else {
							false
						};

						items.push(ExplorerItem::Path {
							has_local_thumbnail: thumbnail_exists_locally,
							thumbnail_key: file_path
								.cas_id
								.as_ref()
								.map(|i| get_indexed_thumb_key(i, library.id)),
							item: file_path,
						})
					}

					Ok(SearchData {
						items,
						cursor: None,
					})
				},
			)
		})
		.procedure("pathsCount", {
			#[derive(Deserialize, Type, Debug)]
			#[serde(rename_all = "camelCase")]
			#[specta(inline)]
			struct Args {
				#[specta(default)]
				filters: Vec<SearchFilterArgs>,
			}

			R.with2(library())
				.query(|(_, library), Args { filters }| async move {
					let Library { db, .. } = library.as_ref();

					Ok(db
						.file_path()
						.count({
							let mut params = Vec::new();

							for filter in filters {
								params.extend(filter.into_file_path_params(db).await?);
							}

							params
						})
						.exec()
						.await? as u32)
				})
		})
		.procedure("objects", {
			#[derive(Deserialize, Type, Debug)]
			#[serde(rename_all = "camelCase")]
			struct ObjectSearchArgs {
				take: u8,
				#[specta(optional)]
				order_and_pagination: Option<object::OrderAndPagination>,
				#[serde(default)]
				filters: Vec<SearchFilterArgs>,
			}

			R.with2(library()).query(
				|(node, library),
				 ObjectSearchArgs {
				     take,
				     order_and_pagination,
				     filters,
				 }| async move {
					let Library { db, .. } = library.as_ref();

					let take = take.max(MAX_TAKE);

					let mut query = db
						.object()
						.find_many({
							let mut params = Vec::new();

							for filter in filters {
								params.extend(filter.into_object_params(db).await?);
							}

							params
						})
						.take(take as i64);

					if let Some(order_and_pagination) = order_and_pagination {
						order_and_pagination.apply(&mut query);
					}

					let (objects, cursor) = {
						let mut objects = query
							.include(object_with_file_paths::include())
							.exec()
							.await?;

						let cursor = (objects.len() as u8 > take)
							.then(|| objects.pop())
							.flatten()
							.map(|r| r.pub_id);

						(objects, cursor)
					};

					let mut items = Vec::with_capacity(objects.len());

					for object in objects {
						let cas_id = object
							.file_paths
							.iter()
							.map(|fp| fp.cas_id.as_ref())
							.find_map(|c| c);

						let thumbnail_exists_locally = if let Some(cas_id) = cas_id {
							library.thumbnail_exists(&node, cas_id).await.map_err(|e| {
								rspc::Error::with_cause(
									ErrorCode::InternalServerError,
									"Failed to check that thumbnail exists".to_string(),
									e,
								)
							})?
						} else {
							false
						};

						items.push(ExplorerItem::Object {
							has_local_thumbnail: thumbnail_exists_locally,
							thumbnail_key: cas_id.map(|i| get_indexed_thumb_key(i, library.id)),
							item: object,
						});
					}

					Ok(SearchData { items, cursor })
				},
			)
		})
		.procedure("objectsCount", {
			#[derive(Deserialize, Type, Debug)]
			#[serde(rename_all = "camelCase")]
			#[specta(inline)]
			struct Args {
				#[serde(default)]
				filters: Vec<SearchFilterArgs>,
			}

			R.with2(library())
				.query(|(_, library), Args { filters }| async move {
					let Library { db, .. } = library.as_ref();

					Ok(db
						.object()
						.count({
							let mut params = Vec::new();

							for filter in filters {
								params.extend(filter.into_object_params(db).await?);
							}

							params
						})
						.exec()
						.await? as u32)
				})
		})
		.merge("saved.", saved::mount())
}



File: ./src/api/search/file_path.rs
-------------------------------------------------
use chrono::{DateTime, FixedOffset, Utc};
use prisma_client_rust::{OrderByQuery, PaginatedQuery, WhereQuery};
use rspc::ErrorCode;
use sd_prisma::prisma::{self, file_path};
use serde::{Deserialize, Serialize};
use specta::Type;

use crate::location::{
	file_path_helper::{check_file_path_exists, IsolatedFilePathData},
	LocationError,
};

use super::object::*;
use super::utils::{self, *};

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase", tag = "field", content = "value")]
pub enum FilePathOrder {
	Name(SortOrder),
	SizeInBytes(SortOrder),
	DateCreated(SortOrder),
	DateModified(SortOrder),
	DateIndexed(SortOrder),
	Object(Box<ObjectOrder>),
}

impl FilePathOrder {
	pub fn get_sort_order(&self) -> prisma::SortOrder {
		(*match self {
			Self::Name(v) => v,
			Self::SizeInBytes(v) => v,
			Self::DateCreated(v) => v,
			Self::DateModified(v) => v,
			Self::DateIndexed(v) => v,
			Self::Object(v) => return v.get_sort_order(),
		})
		.into()
	}

	pub fn into_param(self) -> file_path::OrderByWithRelationParam {
		let dir = self.get_sort_order();
		use file_path::*;
		match self {
			Self::Name(_) => name::order(dir),
			Self::SizeInBytes(_) => size_in_bytes_bytes::order(dir),
			Self::DateCreated(_) => date_created::order(dir),
			Self::DateModified(_) => date_modified::order(dir),
			Self::DateIndexed(_) => date_indexed::order(dir),
			Self::Object(v) => object::order(vec![v.into_param()]),
		}
	}
}

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase")]
pub enum FilePathFilterArgs {
	Locations(InOrNotIn<file_path::id::Type>),
	Path {
		location_id: prisma::location::id::Type,
		path: String,
		include_descendants: bool,
	},
	// #[deprecated]
	// Search(String),
	Name(TextMatch),
	Extension(InOrNotIn<String>),
	CreatedAt(Range<DateTime<Utc>>),
	ModifiedAt(Range<DateTime<Utc>>),
	IndexedAt(Range<DateTime<Utc>>),
	Hidden(bool),
}

impl FilePathFilterArgs {
	pub async fn into_params(
		self,
		db: &prisma::PrismaClient,
	) -> Result<Vec<file_path::WhereParam>, rspc::Error> {
		use file_path::*;

		Ok(match self {
			Self::Locations(v) => v
				.to_param(
					file_path::location_id::in_vec,
					file_path::location_id::not_in_vec,
				)
				.map(|v| vec![v])
				.unwrap_or_default(),
			Self::Path {
				location_id,
				path,
				include_descendants,
			} => {
				let directory_materialized_path_str = if !path.is_empty() && path != "/" {
					let parent_iso_file_path =
						IsolatedFilePathData::from_relative_str(location_id, &path);

					if !check_file_path_exists::<LocationError>(&parent_iso_file_path, db).await? {
						return Err(rspc::Error::new(
							ErrorCode::NotFound,
							"Directory not found".into(),
						));
					}

					parent_iso_file_path.materialized_path_for_children()
				} else {
					Some("/".into())
				};

				directory_materialized_path_str
					.map(Some)
					.map(|materialized_path| {
						vec![if include_descendants {
							materialized_path::starts_with(
								materialized_path.unwrap_or_else(|| "/".into()),
							)
						} else {
							materialized_path::equals(materialized_path)
						}]
					})
					.unwrap_or_default()
			}
			Self::Name(v) => v
				.to_param(name::contains, name::starts_with, name::ends_with, |s| {
					name::equals(Some(s))
				})
				.map(|v| vec![v])
				.unwrap_or_default(),
			Self::Extension(v) => v
				.to_param(extension::in_vec, extension::not_in_vec)
				.map(|v| vec![v])
				.unwrap_or_default(),
			Self::CreatedAt(v) => {
				vec![match v {
					Range::From(v) => date_created::gte(v.into()),
					Range::To(v) => date_created::lte(v.into()),
				}]
			}
			Self::ModifiedAt(v) => {
				vec![match v {
					Range::From(v) => date_modified::gte(v.into()),
					Range::To(v) => date_modified::lte(v.into()),
				}]
			}
			Self::IndexedAt(v) => {
				vec![match v {
					Range::From(v) => date_indexed::gte(v.into()),
					Range::To(v) => date_indexed::lte(v.into()),
				}]
			}
			Self::Hidden(v) => {
				vec![hidden::equals(Some(v))]
			}
		})
	}
}

#[derive(Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub enum FilePathObjectCursor {
	DateAccessed(CursorOrderItem<DateTime<FixedOffset>>),
	Kind(CursorOrderItem<i32>),
}

impl FilePathObjectCursor {
	fn apply(self, query: &mut file_path::FindManyQuery) {
		macro_rules! arm {
			($field:ident, $item:ident) => {{
				let item = $item;

				query.add_where(match item.order {
					SortOrder::Asc => {
						prisma::file_path::object::is(vec![prisma::object::$field::gt(item.data)])
					}
					SortOrder::Desc => {
						prisma::file_path::object::is(vec![prisma::object::$field::lt(item.data)])
					}
				});

				query.add_order_by(prisma::file_path::object::order(vec![
					prisma::object::$field::order(item.order.into()),
				]));
			}};
		}

		match self {
			FilePathObjectCursor::Kind(item) => arm!(kind, item),
			FilePathObjectCursor::DateAccessed(item) => {
				arm!(date_accessed, item)
			}
		};
	}
}

#[derive(Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub enum FilePathCursorVariant {
	None,
	Name(CursorOrderItem<String>),
	SizeInBytes(SortOrder),
	DateCreated(CursorOrderItem<DateTime<FixedOffset>>),
	DateModified(CursorOrderItem<DateTime<FixedOffset>>),
	DateIndexed(CursorOrderItem<DateTime<FixedOffset>>),
	Object(FilePathObjectCursor),
}

impl FilePathCursorVariant {
	pub fn apply(self, query: &mut file_path::FindManyQuery, id: i32) {
		macro_rules! arm {
			($field:ident, $item:ident) => {{
				let item = $item;

				let data = item.data.clone();

				query.add_where(prisma_client_rust::or![
					match item.order {
						SortOrder::Asc => prisma::file_path::$field::gt(data),
						SortOrder::Desc => prisma::file_path::$field::lt(data),
					},
					prisma_client_rust::and![
						prisma::file_path::$field::equals(Some(item.data)),
						match item.order {
							SortOrder::Asc => prisma::file_path::id::gt(id),
							SortOrder::Desc => prisma::file_path::id::lt(id),
						}
					]
				]);

				query.add_order_by(prisma::file_path::$field::order(item.order.into()));
			}};
		}

		match self {
			Self::None => {
				query.add_where(prisma::file_path::id::gt(id));
			}
			Self::SizeInBytes(order) => {
				query.add_order_by(prisma::file_path::size_in_bytes_bytes::order(order.into()));
			}
			Self::Name(item) => arm!(name, item),
			Self::DateCreated(item) => {
				arm!(date_created, item)
			}
			Self::DateModified(item) => {
				arm!(date_modified, item)
			}
			Self::DateIndexed(item) => {
				arm!(date_indexed, item)
			}
			Self::Object(obj) => obj.apply(query),
		};
	}
}

#[derive(Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub struct FilePathCursor {
	pub is_dir: bool,
	pub variant: FilePathCursorVariant,
}

pub type OrderAndPagination =
	utils::OrderAndPagination<prisma::file_path::id::Type, FilePathOrder, FilePathCursor>;

impl OrderAndPagination {
	pub fn apply(self, query: &mut file_path::FindManyQuery, group_directories: bool) {
		match self {
			Self::OrderOnly(order) => {
				query.add_order_by(order.into_param());
			}
			Self::Offset { offset, order } => {
				query.set_skip(offset as i64);

				if let Some(order) = order {
					query.add_order_by(order.into_param())
				}
			}
			Self::Cursor { id, cursor } => {
				// This may seem dumb but it's vital!
				// If we're grouping by directories + all directories have been fetched,
				// we don't want to include them in the results.
				// It's important to keep in mind that since the `order_by` for
				// `group_directories` comes before all other orderings,
				// all other orderings will be applied independently to directories and paths.
				if group_directories && !cursor.is_dir {
					query.add_where(prisma::file_path::is_dir::not(Some(true)))
				}

				cursor.variant.apply(query, id);

				query.add_order_by(prisma::file_path::id::order(prisma::SortOrder::Asc));
			}
		}
	}
}



File: ./src/api/search/saved.rs
-------------------------------------------------
use chrono::{DateTime, FixedOffset, Utc};
use rspc::alpha::AlphaRouter;
use sd_utils::chain_optional_iter;
use serde::{Deserialize, Serialize};
use specta::Type;
use uuid::Uuid;

use crate::{api::utils::library, invalidate_query, prisma::saved_search};

use super::{Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("create", {
			R.with2(library()).mutation({
				#[derive(Serialize, Type, Deserialize, Clone, Debug)]
				#[specta(inline)]
				pub struct Args {
					pub name: String,
					#[specta(optional)]
					pub search: Option<String>,
					#[specta(optional)]
					pub filters: Option<String>,
					#[specta(optional)]
					pub description: Option<String>,
					#[specta(optional)]
					pub icon: Option<String>,
				}

				|(_, library), args: Args| async move {
					let pub_id = Uuid::new_v4().as_bytes().to_vec();
					let date_created: DateTime<FixedOffset> = Utc::now().into();

					library
						.db
						.saved_search()
						.create(
							pub_id,
							chain_optional_iter(
								[
									saved_search::date_created::set(Some(date_created)),
									saved_search::name::set(Some(args.name)),
								],
								[
									args.filters
										.map(|s| {
											serde_json::to_string(
												&serde_json::from_str::<serde_json::Value>(&s)
													.unwrap(),
											)
											.unwrap()
										})
										.map(Some)
										.map(saved_search::filters::set),
									args.search.map(Some).map(saved_search::search::set),
									args.description
										.map(Some)
										.map(saved_search::description::set),
									args.icon.map(Some).map(saved_search::icon::set),
								],
							),
						)
						.exec()
						.await?;

					invalidate_query!(library, "search.saved.list");

					Ok(())
				}
			})
		})
		.procedure("get", {
			R.with2(library())
				.query(|(_, library), search_id: i32| async move {
					Ok(library
						.db
						.saved_search()
						.find_unique(saved_search::id::equals(search_id))
						.exec()
						.await?)
				})
		})
		.procedure("list", {
			R.with2(library()).query(|(_, library), _: ()| async move {
				Ok(library
					.db
					.saved_search()
					.find_many(vec![])
					// .order_by(saved_search::order::order(prisma::SortOrder::Desc))
					.exec()
					.await?)
			})
		})
		.procedure("update", {
			R.with2(library()).mutation({
				saved_search::partial_unchecked!(Args {
					name
					description
					icon
					search
					filters
				});

				|(_, library), (id, args): (saved_search::id::Type, Args)| async move {
					let mut params = args.to_params();
					params.push(saved_search::date_modified::set(Some(Utc::now().into())));

					library
						.db
						.saved_search()
						.update_unchecked(saved_search::id::equals(id), params)
						.exec()
						.await?;

					invalidate_query!(library, "search.saved.list");
					invalidate_query!(library, "search.saved.get");

					Ok(())
				}
			})
		})
		.procedure("delete", {
			R.with2(library())
				.mutation(|(_, library), search_id: i32| async move {
					library
						.db
						.saved_search()
						.delete(saved_search::id::equals(search_id))
						.exec()
						.await?;

					invalidate_query!(library, "search.saved.list");
					// disabled as it's messing with pre-delete navigation
					// invalidate_query!(library, "search.saved.get");

					Ok(())
				})
		})
}



File: ./src/api/search/utils.rs
-------------------------------------------------
use sd_prisma::prisma;
use serde::{Deserialize, Serialize};
use specta::Type;

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase")]
pub enum Range<T> {
	From(T),
	To(T),
}

#[derive(Serialize, Deserialize, Type, Debug, Clone, Copy)]
#[serde(rename_all = "PascalCase")]
pub enum SortOrder {
	Asc,
	Desc,
}

impl From<SortOrder> for prisma::SortOrder {
	fn from(value: SortOrder) -> prisma::SortOrder {
		match value {
			SortOrder::Asc => prisma::SortOrder::Asc,
			SortOrder::Desc => prisma::SortOrder::Desc,
		}
	}
}

#[derive(Deserialize, Type, Debug, Clone)]
#[serde(untagged)]
pub enum MaybeNot<T> {
	None(T),
	Not { not: T },
}

impl<T> MaybeNot<T> {
	pub fn into_prisma<R: From<prisma_client_rust::Operator<R>>>(self, param: fn(T) -> R) -> R {
		match self {
			Self::None(v) => param(v),
			Self::Not { not } => prisma_client_rust::not![param(not)],
		}
	}
}

#[derive(Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub struct CursorOrderItem<T> {
	pub order: SortOrder,
	pub data: T,
}

#[derive(Deserialize, Type, Debug)]
#[serde(rename_all = "camelCase")]
pub enum OrderAndPagination<TId, TOrder, TCursor> {
	OrderOnly(TOrder),
	Offset { offset: i32, order: Option<TOrder> },
	Cursor { id: TId, cursor: TCursor },
}

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase")]
pub enum InOrNotIn<T> {
	In(Vec<T>),
	NotIn(Vec<T>),
}

impl<T> InOrNotIn<T> {
	pub fn is_empty(&self) -> bool {
		match self {
			Self::In(v) => v.is_empty(),
			Self::NotIn(v) => v.is_empty(),
		}
	}

	pub fn to_param<TParam>(
		self,
		in_fn: fn(Vec<T>) -> TParam,
		not_in_fn: fn(Vec<T>) -> TParam,
	) -> Option<TParam> {
		self.is_empty()
			.then_some(None)
			.unwrap_or_else(|| match self {
				Self::In(v) => Some(in_fn(v)),
				Self::NotIn(v) => Some(not_in_fn(v)),
			})
	}
}

#[derive(Serialize, Deserialize, Type, Debug, Clone)]
#[serde(rename_all = "camelCase")]
pub enum TextMatch {
	Contains(String),
	StartsWith(String),
	EndsWith(String),
	Equals(String),
}

impl TextMatch {
	pub fn is_empty(&self) -> bool {
		match self {
			Self::Contains(v) => v.is_empty(),
			Self::StartsWith(v) => v.is_empty(),
			Self::EndsWith(v) => v.is_empty(),
			Self::Equals(v) => v.is_empty(),
		}
	}

	// 3. Update the to_param method of TextMatch
	pub fn to_param<TParam>(
		self,
		contains_fn: fn(String) -> TParam,
		starts_with_fn: fn(String) -> TParam,
		ends_with_fn: fn(String) -> TParam,
		equals_fn: fn(String) -> TParam,
	) -> Option<TParam> {
		self.is_empty()
			.then_some(None)
			.unwrap_or_else(|| match self {
				Self::Contains(v) => Some(contains_fn(v)),
				Self::StartsWith(v) => Some(starts_with_fn(v)),
				Self::EndsWith(v) => Some(ends_with_fn(v)),
				Self::Equals(v) => Some(equals_fn(v)),
			})
	}
}



File: ./src/api/mod.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::JobProgressEvent,
	node::config::{NodeConfig, NodePreferences},
	Node,
};

use sd_p2p::P2PStatus;

use std::sync::{atomic::Ordering, Arc};

use itertools::Itertools;
use rspc::{alpha::Rspc, Config, ErrorCode};
use serde::{Deserialize, Serialize};
use specta::Type;
use uuid::Uuid;

mod auth;
mod backups;
// mod categories;
mod ephemeral_files;
mod files;
mod jobs;
mod keys;
mod libraries;
pub mod locations;
mod nodes;
pub mod notifications;
mod p2p;
mod preferences;
pub(crate) mod search;
mod sync;
mod tags;
pub mod utils;
pub mod volumes;
mod web_api;

use utils::{InvalidRequests, InvalidateOperationEvent};

#[allow(non_upper_case_globals)]
pub(crate) const R: Rspc<Ctx> = Rspc::new();

pub type Ctx = Arc<Node>;
pub type Router = rspc::Router<Ctx>;

/// Represents an internal core event, these are exposed to client via a rspc subscription.
#[derive(Debug, Clone, Serialize, Type)]
pub enum CoreEvent {
	NewThumbnail { thumb_key: Vec<String> },
	JobProgress(JobProgressEvent),
	InvalidateOperation(InvalidateOperationEvent),
}

/// All of the feature flags provided by the core itself. The frontend has it's own set of feature flags!
///
/// If you want a variant of this to show up on the frontend it must be added to `backendFeatures` in `useFeatureFlag.tsx`
#[derive(Debug, PartialEq, Eq, Clone, Serialize, Deserialize, Type)]
#[serde(rename_all = "camelCase")]
pub enum BackendFeature {
	SyncEmitMessages,
	FilesOverP2P,
}

impl BackendFeature {
	pub fn restore(&self, node: &Node) {
		match self {
			BackendFeature::SyncEmitMessages => {
				node.libraries
					.emit_messages_flag
					.store(true, Ordering::Relaxed);
			}
			BackendFeature::FilesOverP2P => {
				node.files_over_p2p_flag.store(true, Ordering::Relaxed);
			}
		}
	}
}

// A version of [NodeConfig] that is safe to share with the frontend
#[derive(Debug, Serialize, Deserialize, Clone, Type)]
pub struct SanitisedNodeConfig {
	/// id is a unique identifier for the current node. Each node has a public identifier (this one) and is given a local id for each library (done within the library code).
	pub id: Uuid,
	/// name is the display name of the current node. This is set by the user and is shown in the UI. // TODO: Length validation so it can fit in DNS record
	pub name: String,
	pub p2p_enabled: bool,
	pub p2p_port: Option<u16>,
	pub features: Vec<BackendFeature>,
	pub preferences: NodePreferences,
}

impl From<NodeConfig> for SanitisedNodeConfig {
	fn from(value: NodeConfig) -> Self {
		Self {
			id: value.id,
			name: value.name,
			p2p_enabled: value.p2p.enabled,
			p2p_port: value.p2p.port,
			features: value.features,
			preferences: value.preferences,
		}
	}
}

#[derive(Serialize, Debug, Type)]
struct NodeState {
	#[serde(flatten)]
	config: SanitisedNodeConfig,
	data_path: String,
	p2p: P2PStatus,
}

pub(crate) fn mount() -> Arc<Router> {
	let r = R
		.router()
		.procedure("buildInfo", {
			#[derive(Serialize, Type)]
			pub struct BuildInfo {
				version: &'static str,
				commit: &'static str,
			}

			R.query(|_, _: ()| BuildInfo {
				version: env!("CARGO_PKG_VERSION"),
				commit: env!("GIT_HASH"),
			})
		})
		.procedure("nodeState", {
			R.query(|node, _: ()| async move {
				Ok(NodeState {
					config: node.config.get().await.into(),
					// We are taking the assumption here that this value is only used on the frontend for display purposes
					data_path: node
						.config
						.data_directory()
						.to_str()
						.expect("Found non-UTF-8 path")
						.to_string(),
					p2p: node.p2p.manager.status(),
				})
			})
		})
		.procedure("toggleFeatureFlag", {
			R.mutation(|node, feature: BackendFeature| async move {
				let config = node.config.get().await;

				let enabled = if config.features.iter().contains(&feature) {
					node.config
						.write(|cfg| {
							cfg.features.retain(|f| *f != feature);
						})
						.await
						.map(|_| false)
				} else {
					node.config
						.write(|cfg| {
							cfg.features.push(feature.clone());
						})
						.await
						.map(|_| true)
				}
				.map_err(|err| rspc::Error::new(ErrorCode::InternalServerError, err.to_string()))?;

				match feature {
					BackendFeature::SyncEmitMessages => {
						node.libraries
							.emit_messages_flag
							.store(enabled, Ordering::Relaxed);
					}
					BackendFeature::FilesOverP2P => {
						node.files_over_p2p_flag.store(enabled, Ordering::Relaxed);
					}
				}

				invalidate_query!(node; node, "nodeState");

				Ok(())
			})
		})
		.merge("api.", web_api::mount())
		.merge("auth.", auth::mount())
		.merge("search.", search::mount())
		.merge("library.", libraries::mount())
		.merge("volumes.", volumes::mount())
		.merge("tags.", tags::mount())
		// .merge("categories.", categories::mount())
		// .merge("keys.", keys::mount())
		.merge("locations.", locations::mount())
		.merge("ephemeralFiles.", ephemeral_files::mount())
		.merge("files.", files::mount())
		.merge("jobs.", jobs::mount())
		.merge("p2p.", p2p::mount())
		.merge("nodes.", nodes::mount())
		.merge("sync.", sync::mount())
		.merge("preferences.", preferences::mount())
		.merge("notifications.", notifications::mount())
		.merge("backups.", backups::mount())
		.merge("invalidation.", utils::mount_invalidate())
		.build(
			#[allow(clippy::let_and_return)]
			{
				let config = Config::new().set_ts_bindings_header("/* eslint-disable */");

				#[cfg(all(debug_assertions, not(feature = "mobile")))]
				let config = config.export_ts_bindings(
					std::path::PathBuf::from(env!("CARGO_MANIFEST_DIR"))
						.join("../packages/client/src/core.ts"),
				);

				config
			},
		)
		.arced();

	InvalidRequests::validate(r.clone()); // This validates all invalidation calls.

	r
}

#[cfg(test)]
mod tests {
	/// This test will ensure the rspc router and all calls to `invalidate_query` are valid and also export an updated version of the Typescript bindings.
	#[test]
	fn test_and_export_rspc_bindings() {
		super::mount();
	}
}



File: ./src/api/nodes.rs
-------------------------------------------------
use crate::{invalidate_query, prisma::location, util::MaybeUndefined};

use sd_prisma::prisma::instance;

use rspc::{alpha::AlphaRouter, ErrorCode};
use serde::Deserialize;
use specta::Type;
use tracing::error;
use uuid::Uuid;

use super::{locations::ExplorerItem, utils::library, Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("edit", {
			#[derive(Deserialize, Type)]
			pub struct ChangeNodeNameArgs {
				pub name: Option<String>,
				pub p2p_enabled: Option<bool>,
				pub p2p_port: MaybeUndefined<u16>,
			}
			R.mutation(|node, args: ChangeNodeNameArgs| async move {
				if let Some(name) = &args.name {
					if name.is_empty() || name.len() > 250 {
						return Err(rspc::Error::new(
							ErrorCode::BadRequest,
							"invalid node name".into(),
						));
					}
				}

				let does_p2p_need_refresh =
					args.p2p_enabled.is_some() || args.p2p_port.is_defined();

				node.config
					.write(|config| {
						if let Some(name) = args.name {
							config.name = name;
						}

						config.p2p.enabled = args.p2p_enabled.unwrap_or(config.p2p.enabled);

						if let Some(v) = args.p2p_port.into() {
							config.p2p.port = v;
						}
					})
					.await
					.map_err(|err| {
						error!("Failed to write config: {}", err);
						rspc::Error::new(
							ErrorCode::InternalServerError,
							"error updating config".into(),
						)
					})?;

				// If a P2P config was modified reload it
				if does_p2p_need_refresh {
					node.p2p
						.manager
						.update_config(node.config.get().await.p2p.clone())
						.await;
				}

				invalidate_query!(node; node, "nodeState");

				Ok(())
			})
		})
		// TODO: add pagination!! and maybe ordering etc
		.procedure("listLocations", {
			R.with2(library())
				// TODO: I don't like this. `node_id` should probs be a machine hash or something cause `node_id` is dynamic in the context of P2P and what does it mean for removable media to be owned by a node?
				.query(|(_, library), node_id: Option<Uuid>| async move {
					// Be aware multiple instances can exist on a single node. This is generally an edge case but it's possible.
					let instances = library
						.db
						.instance()
						.find_many(vec![node_id
							.map(|id| instance::node_id::equals(id.as_bytes().to_vec()))
							.unwrap_or(instance::id::equals(
								library.config().await.instance_id,
							))])
						.exec()
						.await?;

					Ok(library
						.db
						.location()
						.find_many(
							instances
								.into_iter()
								.map(|i| location::instance_id::equals(Some(i.id)))
								.collect(),
						)
						.exec()
						.await?
						.into_iter()
						.map(|location| ExplorerItem::Location {
							has_local_thumbnail: false,
							thumbnail_key: None,
							item: location,
						})
						.collect::<Vec<_>>())
				})
		})
		.procedure("updateThumbnailerPreferences", {
			#[derive(Deserialize, Type)]
			pub struct UpdateThumbnailerPreferences {
				pub background_processing_percentage: u8, // 0-100
			}
			R.mutation(
				|node,
				 UpdateThumbnailerPreferences {
				     background_processing_percentage,
				 }: UpdateThumbnailerPreferences| async move {
					node.config
						.update_preferences(|preferences| {
							preferences
								.thumbnailer
								.set_background_processing_percentage(
									background_processing_percentage,
								);
						})
						.await
						.map_err(|e| {
							error!("failed to update thumbnailer preferences: {e:#?}");
							rspc::Error::with_cause(
								ErrorCode::InternalServerError,
								"Failed to update thumbnailer preferences".to_string(),
								e,
							)
						})
				},
			)
		})
}



File: ./src/api/notifications.rs
-------------------------------------------------
use async_stream::stream;
use chrono::{DateTime, Utc};
use futures::future::join_all;
use rspc::{alpha::AlphaRouter, ErrorCode};
use sd_prisma::prisma::notification;
use serde::{Deserialize, Serialize};
use specta::Type;
use uuid::Uuid;

use crate::api::{Ctx, R};

use super::utils::library;

/// Represents a single notification.
#[derive(Debug, Clone, Serialize, Deserialize, Type)]
pub struct Notification {
	#[serde(flatten)]
	pub id: NotificationId,
	pub data: NotificationData,
	pub read: bool,
	pub expires: Option<DateTime<Utc>>,
}

#[derive(Debug, Clone, Serialize, Deserialize, Type, PartialEq, Eq)]
#[serde(tag = "type", content = "id", rename_all = "camelCase")]
pub enum NotificationId {
	Library(Uuid, u32),
	Node(u32),
}

/// Represents the data of a single notification.
/// This data is used by the frontend to properly display the notification.
#[derive(Debug, Clone, Serialize, Deserialize, Type)]
pub enum NotificationData {
	PairingRequest { id: Uuid, pairing_id: u16 },
	Test,
}

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("get", {
			R.query(|node, _: ()| async move {
				let mut notifications = node.config.get().await.notifications;
				for lib_notifications in join_all(node.libraries.get_all().await.into_iter().map(
					|library| async move {
						library
							.db
							.notification()
							.find_many(vec![])
							.exec()
							.await
							.map_err(|err| {
								rspc::Error::new(
									ErrorCode::InternalServerError,
									format!(
										"Failed to get notifications for library '{}': {}",
										library.id, err
									),
								)
							})?
							.into_iter()
							.map(|n| {
								Ok(Notification {
									id: NotificationId::Library(library.id, n.id as u32),
									data: rmp_serde::from_slice(&n.data).map_err(|err| {
										rspc::Error::new(
											ErrorCode::InternalServerError,
											format!(
												"Failed to get notifications for library '{}': {}",
												library.id, err
											),
										)
									})?,
									read: false,
									expires: n.expires_at.map(Into::into),
								})
							})
							.collect::<Result<Vec<Notification>, rspc::Error>>()
					},
				))
				.await
				{
					notifications.extend(lib_notifications?);
				}

				Ok(notifications)
			})
		})
		.procedure("dismiss", {
			R.query(|node, id: NotificationId| async move {
				match id {
					NotificationId::Library(library_id, id) => {
						node.libraries
							.get_library(&library_id)
							.await
							.ok_or_else(|| {
								rspc::Error::new(ErrorCode::NotFound, "Library not found".into())
							})?
							.db
							.notification()
							.delete_many(vec![notification::id::equals(id as i32)])
							.exec()
							.await
							.map_err(|err| {
								rspc::Error::new(ErrorCode::InternalServerError, err.to_string())
							})?;
					}
					NotificationId::Node(id) => {
						node.config
							.write(|cfg| {
								cfg.notifications
									.retain(|n| n.id != NotificationId::Node(id));
							})
							.await
							.map_err(|err| {
								rspc::Error::new(ErrorCode::InternalServerError, err.to_string())
							})?;
					}
				}

				Ok(())
			})
		})
		.procedure("dismissAll", {
			R.query(|node, _: ()| async move {
				node.config
					.write(|cfg| {
						cfg.notifications = vec![];
					})
					.await
					.map_err(|err| {
						rspc::Error::new(ErrorCode::InternalServerError, err.to_string())
					})?;

				join_all(
					node.libraries
						.get_all()
						.await
						.into_iter()
						.map(|library| async move {
							library.db.notification().delete_many(vec![]).exec().await
						}),
				)
				.await
				.into_iter()
				.collect::<Result<Vec<_>, _>>()?;

				Ok(())
			})
		})
		.procedure("listen", {
			R.subscription(|node, _: ()| async move {
				let mut sub = node.notifications.subscribe();

				stream! {
					while let Ok(notification) = sub.recv().await {
						yield notification;
					}
				}
			})
		})
		.procedure("test", {
			R.mutation(|node, _: ()| async move {
				node.emit_notification(NotificationData::Test, None).await;
			})
		})
		.procedure("testLibrary", {
			R.with2(library())
				.mutation(|(_, library), _: ()| async move {
					library
						.emit_notification(NotificationData::Test, None)
						.await;
				})
		})
}



File: ./src/api/jobs.rs
-------------------------------------------------
use crate::{
	invalidate_query,
	job::{job_without_data, Job, JobReport, JobStatus, Jobs},
	location::{find_location, LocationError},
	object::{
		file_identifier::file_identifier_job::FileIdentifierJobInit, media::MediaProcessorJobInit,
		validation::validator_job::ObjectValidatorJobInit,
	},
	prisma::{job, location, SortOrder},
};

use std::{
	collections::{hash_map::Entry, BTreeMap, HashMap, VecDeque},
	path::PathBuf,
	time::Instant,
};

use chrono::{DateTime, Utc};
use prisma_client_rust::or;
use rspc::alpha::AlphaRouter;
use serde::{Deserialize, Serialize};
use specta::Type;
use tokio::time::Duration;
use tracing::{info, trace};
use uuid::Uuid;

use super::{utils::library, CoreEvent, Ctx, R};

pub(crate) fn mount() -> AlphaRouter<Ctx> {
	R.router()
		.procedure("progress", {
			// Listen for updates from the job manager
			// - the client listens for events containing an updated JobReport
			// - the client replaces its local copy of the JobReport using the index provided by the reports procedure
			// - this should be used with the ephemeral sync engine
			R.with2(library())
				.subscription(|(node, _), _: ()| async move {
					let mut event_bus_rx = node.event_bus.0.subscribe();
					// debounce per-job
					let mut intervals = BTreeMap::<Uuid, Instant>::new();

					async_stream::stream! {
						loop {
							let progress_event = loop {
								if let Ok(CoreEvent::JobProgress(progress_event)) = event_bus_rx.recv().await {
									break progress_event;
								}
							};

							let instant = intervals.entry(progress_event.id).or_insert_with(
								Instant::now
							);

							if instant.elapsed() <= Duration::from_secs_f64(1.0 / 30.0) {
								continue;
							}

							yield progress_event;

							*instant = Instant::now();
						}
					}
				})
		})
		.procedure("reports", {
			// Reports provides the client with a list of JobReports
			// - we query with a custom select! to avoid returning paused job cache `job.data`
			// - results must include running jobs, and be combined with the in-memory state
			//	  this is to ensure the client will always get the correct initial state
			// - jobs are sorted in to groups by their action
			// - TODO: refactor grouping system to a many-to-many table
			#[derive(Debug, Clone, Serialize, Deserialize, Type)]
			pub struct JobGroup {
				id: Uuid,
				action: Option<String>,
				status: JobStatus,
				created_at: DateTime<Utc>,
				jobs: VecDeque<JobReport>,
			}

			R.with2(library())
				.query(|(node, library), _: ()| async move {
					let mut groups: HashMap<String, JobGroup> = HashMap::new();

					let job_reports: Vec<JobReport> = library
						.db
						.job()
						.find_many(vec![])
						.order_by(job::date_created::order(SortOrder::Desc))
						.take(100)
						.select(job_without_data::select())
						.exec()
						.await?
						.into_iter()
						.flat_map(JobReport::try_from)
						.collect();

					let active_reports_by_id = node.jobs.get_active_reports_with_id().await;

					for job in job_reports {
						// action name and group key are computed from the job data
						let (action_name, group_key) = job.get_meta();

						trace!(
							"job {:#?}, action_name {}, group_key {:?}",
							job,
							action_name,
							group_key
						);

						// if the job is running, use the in-memory report
						let report = active_reports_by_id.get(&job.id).unwrap_or(&job);

						// if we have a group key, handle grouping
						if let Some(group_key) = group_key {
							match groups.entry(group_key) {
								// Create new job group with metadata
								Entry::Vacant(entry) => {
									entry.insert(JobGroup {
										id: job.parent_id.unwrap_or(job.id),
										action: Some(action_name.clone()),
										status: job.status,
										jobs: [report.clone()].into_iter().collect(),
										created_at: job.created_at.unwrap_or(Utc::now()),
									});
								}
								// Add to existing job group
								Entry::Occupied(mut entry) => {
									let group = entry.get_mut();

									// protect paused status from being overwritten
									if report.status != JobStatus::Paused {
										group.status = report.status;
									}

									group.jobs.push_front(report.clone());
								}
							}
						} else {
							// insert individual job as group
							groups.insert(
								job.id.to_string(),
								JobGroup {
									id: job.id,
									action: None,
									status: job.status,
									jobs: [report.clone()].into_iter().collect(),
									created_at: job.created_at.unwrap_or(Utc::now()),
								},
							);
						}
					}

					let mut groups_vec = groups.into_values().collect::<Vec<_>>();
					groups_vec.sort_by(|a, b| b.created_at.cmp(&a.created_at));

					Ok(groups_vec)
				})
		})
		.procedure("isActive", {
			R.with2(library())
				.query(|(node, library), _: ()| async move {
					Ok(node.jobs.has_active_workers(library.id).await)
				})
		})
		.procedure("clear", {
			R.with2(library())
				.mutation(|(_, library), id: Uuid| async move {
					library
						.db
						.job()
						.delete(job::id::equals(id.as_bytes().to_vec()))
						.exec()
						.await?;

					invalidate_query!(library, "jobs.reports");
					Ok(())
				})
		})
		.procedure("clearAll", {
			R.with2(library())
				.mutation(|(_, library), _: ()| async move {
					info!("Clearing all jobs");
					library
						.db
						.job()
						.delete_many(vec![or![
							job::status::equals(Some(JobStatus::Canceled as i32)),
							job::status::equals(Some(JobStatus::Failed as i32)),
							job::status::equals(Some(JobStatus::Completed as i32)),
							job::status::equals(Some(JobStatus::CompletedWithErrors as i32)),
						]])
						.exec()
						.await?;

					invalidate_query!(library, "jobs.reports");
					Ok(())
				})
		})
		// pause job
		.procedure("pause", {
			R.with2(library())
				.mutation(|(node, library), id: Uuid| async move {
					let ret = Jobs::pause(&node.jobs, id).await.map_err(Into::into);
					invalidate_query!(library, "jobs.reports");
					ret
				})
		})
		.procedure("resume", {
			R.with2(library())
				.mutation(|(node, library), id: Uuid| async move {
					let ret = Jobs::resume(&node.jobs, id).await.map_err(Into::into);
					invalidate_query!(library, "jobs.reports");
					ret
				})
		})
		.procedure("cancel", {
			R.with2(library())
				.mutation(|(node, library), id: Uuid| async move {
					let ret = Jobs::cancel(&node.jobs, id).await.map_err(Into::into);
					invalidate_query!(library, "jobs.reports");
					ret
				})
		})
		.procedure("generateThumbsForLocation", {
			#[derive(Type, Deserialize)]
			pub struct GenerateThumbsForLocationArgs {
				pub id: location::id::Type,
				pub path: PathBuf,
				#[serde(default)]
				pub regenerate: bool,
			}

			R.with2(library()).mutation(
				|(node, library),
				 GenerateThumbsForLocationArgs {
				     id,
				     path,
				     regenerate,
				 }: GenerateThumbsForLocationArgs| async move {
					let Some(location) = find_location(&library, id).exec().await? else {
						return Err(LocationError::IdNotFound(id).into());
					};

					Job::new(MediaProcessorJobInit {
						location,
						sub_path: Some(path),
						regenerate_thumbnails: regenerate,
					})
					.spawn(&node, &library)
					.await
					.map_err(Into::into)
				},
			)
		})
		.procedure("objectValidator", {
			#[derive(Type, Deserialize)]
			pub struct ObjectValidatorArgs {
				pub id: location::id::Type,
				pub path: PathBuf,
			}

			R.with2(library())
				.mutation(|(node, library), args: ObjectValidatorArgs| async move {
					let Some(location) = find_location(&library, args.id).exec().await? else {
						return Err(LocationError::IdNotFound(args.id).into());
					};

					Job::new(ObjectValidatorJobInit {
						location,
						sub_path: Some(args.path),
					})
					.spawn(&node, &library)
					.await
					.map_err(Into::into)
				})
		})
		.procedure("identifyUniqueFiles", {
			#[derive(Type, Deserialize)]
			pub struct IdentifyUniqueFilesArgs {
				pub id: location::id::Type,
				pub path: PathBuf,
			}

			R.with2(library()).mutation(
				|(node, library), args: IdentifyUniqueFilesArgs| async move {
					let Some(location) = find_location(&library, args.id).exec().await? else {
						return Err(LocationError::IdNotFound(args.id).into());
					};

					Job::new(FileIdentifierJobInit {
						location,
						sub_path: Some(args.path),
					})
					.spawn(&node, &library)
					.await
					.map_err(Into::into)
				},
			)
		})
		.procedure("newThumbnail", {
			R.with2(library())
				.subscription(|(node, _), _: ()| async move {
					// TODO: Only return event for the library that was subscribed to

					let mut event_bus_rx = node.event_bus.0.subscribe();
					async_stream::stream! {
						while let Ok(event) = event_bus_rx.recv().await {
							match event {
								CoreEvent::NewThumbnail { thumb_key } => yield thumb_key,
								_ => {}
							}
						}
					}
				})
		})
}



File: ./src/node/config.rs
-------------------------------------------------
use crate::{
	api::{notifications::Notification, BackendFeature},
	auth::OAuthToken,
	object::media::thumbnail::preferences::ThumbnailerPreferences,
	util::{
		error::FileIOError,
		version_manager::{Kind, ManagedVersion, VersionManager, VersionManagerError},
	},
};

use sd_p2p::{Keypair, ManagerConfig};

use std::{
	path::{Path, PathBuf},
	sync::Arc,
};

use int_enum::IntEnum;
use serde::{Deserialize, Serialize};
use serde_json::{json, Map, Value};
use serde_repr::{Deserialize_repr, Serialize_repr};
use specta::Type;
use thiserror::Error;
use tokio::{
	fs,
	sync::{watch, RwLock},
};
use tracing::error;
use uuid::Uuid;

/// NODE_STATE_CONFIG_NAME is the name of the file which stores the NodeState
pub const NODE_STATE_CONFIG_NAME: &str = "node_state.sdconfig";

/// NodeConfig is the configuration for a node. This is shared between all libraries and is stored in a JSON file on disk.
#[derive(Debug, Clone, Serialize, Deserialize)] // If you are adding `specta::Type` on this your probably about to leak the P2P private key
pub struct NodeConfig {
	/// id is a unique identifier for the current node. Each node has a public identifier (this one) and is given a local id for each library (done within the library code).
	pub id: Uuid,
	/// name is the display name of the current node. This is set by the user and is shown in the UI. // TODO: Length validation so it can fit in DNS record
	pub name: String,
	/// core level notifications
	#[serde(default)]
	pub notifications: Vec<Notification>,
	/// The p2p identity keypair for this node. This is used to identify the node on the network.
	/// This keypair does effectively nothing except for provide libp2p with a stable peer_id.
	pub keypair: Keypair,
	/// P2P config
	#[serde(default)]
	pub p2p: ManagerConfig,
	/// Feature flags enabled on the node
	#[serde(default)]
	pub features: Vec<BackendFeature>,
	/// Authentication for Spacedrive Accounts
	pub auth_token: Option<OAuthToken>,

	/// The aggreagation of many different preferences for the node
	pub preferences: NodePreferences,

	version: NodeConfigVersion,
}

#[derive(Debug, Clone, Serialize, Deserialize, Default, PartialEq, Eq, Type)]
pub struct NodePreferences {
	pub thumbnailer: ThumbnailerPreferences,
}

#[derive(
	IntEnum, Debug, Clone, Copy, Eq, PartialEq, strum::Display, Serialize_repr, Deserialize_repr,
)]
#[repr(u64)]
pub enum NodeConfigVersion {
	V0 = 0,
	V1 = 1,
	V2 = 2,
}

impl ManagedVersion<NodeConfigVersion> for NodeConfig {
	const LATEST_VERSION: NodeConfigVersion = NodeConfigVersion::V2;
	const KIND: Kind = Kind::Json("version");
	type MigrationError = NodeConfigError;

	fn from_latest_version() -> Option<Self> {
		let mut name = match hostname::get() {
			// SAFETY: This is just for display purposes so it doesn't matter if it's lossy
			Ok(hostname) => hostname.to_string_lossy().into_owned(),
			Err(e) => {
				error!("Falling back to default node name as an error occurred getting your systems hostname: '{e:#?}'");
				"my-spacedrive".into()
			}
		};
		name.truncate(250);

		Some(Self {
			id: Uuid::new_v4(),
			name,
			keypair: Keypair::generate(),
			version: Self::LATEST_VERSION,
			p2p: ManagerConfig::default(),
			features: vec![],
			notifications: vec![],
			auth_token: None,
			preferences: NodePreferences::default(),
		})
	}
}

impl NodeConfig {
	pub async fn load(path: impl AsRef<Path>) -> Result<Self, NodeConfigError> {
		let path = path.as_ref();
		VersionManager::<Self, NodeConfigVersion>::migrate_and_load(
			path,
			|current, next| async move {
				match (current, next) {
					(NodeConfigVersion::V0, NodeConfigVersion::V1) => {
						let mut config: Map<String, Value> =
							serde_json::from_slice(&fs::read(path).await.map_err(|e| {
								FileIOError::from((
									path,
									e,
									"Failed to read node config file for migration",
								))
							})?)
							.map_err(VersionManagerError::SerdeJson)?;

						// All were never hooked up to the UI
						config.remove("p2p_email");
						config.remove("p2p_img_url");
						config.remove("p2p_port");

						// In a recent PR I screwed up Serde `default` so P2P was disabled by default, prior it was always enabled.
						// Given the config for it is behind a feature flag (so no one would have changed it) this fixes the default.
						if let Some(Value::Object(obj)) = config.get_mut("p2p") {
							obj.insert("enabled".into(), Value::Bool(true));
						}

						fs::write(
							path,
							serde_json::to_vec(&config).map_err(VersionManagerError::SerdeJson)?,
						)
						.await
						.map_err(|e| FileIOError::from((path, e)))?;
					}

					(NodeConfigVersion::V1, NodeConfigVersion::V2) => {
						let mut config: Map<String, Value> =
							serde_json::from_slice(&fs::read(path).await.map_err(|e| {
								FileIOError::from((
									path,
									e,
									"Failed to read node config file for migration",
								))
							})?)
							.map_err(VersionManagerError::SerdeJson)?;

						config.insert(
							String::from("preferences"),
							json!(NodePreferences::default()),
						);

						let a =
							serde_json::to_vec(&config).map_err(VersionManagerError::SerdeJson)?;

						fs::write(path, a)
							.await
							.map_err(|e| FileIOError::from((path, e)))?;
					}

					_ => {
						error!("Node config version is not handled: {:?}", current);
						return Err(VersionManagerError::UnexpectedMigration {
							current_version: current.int_value(),
							next_version: next.int_value(),
						}
						.into());
					}
				}

				Ok(())
			},
		)
		.await
	}

	async fn save(&self, path: impl AsRef<Path>) -> Result<(), NodeConfigError> {
		let path = path.as_ref();
		fs::write(path, serde_json::to_vec(self)?)
			.await
			.map_err(|e| FileIOError::from((path, e)))?;

		Ok(())
	}
}

pub struct Manager {
	config: RwLock<NodeConfig>,
	data_directory_path: PathBuf,
	config_file_path: PathBuf,
	preferences_watcher_tx: watch::Sender<NodePreferences>,
}

impl Manager {
	/// new will create a new NodeConfigManager with the given path to the config file.
	pub(crate) async fn new(
		data_directory_path: impl AsRef<Path>,
	) -> Result<Arc<Self>, NodeConfigError> {
		let data_directory_path = data_directory_path.as_ref().to_path_buf();
		let config_file_path = data_directory_path.join(NODE_STATE_CONFIG_NAME);

		let config = NodeConfig::load(&config_file_path).await?;

		let (preferences_watcher_tx, _preferences_watcher_rx) =
			watch::channel(config.preferences.clone());

		Ok(Arc::new(Self {
			config: RwLock::new(config),
			data_directory_path,
			config_file_path,
			preferences_watcher_tx,
		}))
	}

	/// get will return the current NodeConfig in a read only state.
	pub(crate) async fn get(&self) -> NodeConfig {
		self.config.read().await.clone()
	}

	/// get a node config preferences watcher receiver
	pub(crate) fn preferences_watcher(&self) -> watch::Receiver<NodePreferences> {
		self.preferences_watcher_tx.subscribe()
	}

	/// data_directory returns the path to the directory storing the configuration data.
	pub(crate) fn data_directory(&self) -> PathBuf {
		self.data_directory_path.clone()
	}

	/// write allows the user to update the configuration. This is done in a closure while a Mutex lock is held so that the user can't cause a race condition if the config were to be updated in multiple parts of the app at the same time.
	pub(crate) async fn write<F: FnOnce(&mut NodeConfig)>(
		&self,
		mutation_fn: F,
	) -> Result<NodeConfig, NodeConfigError> {
		let mut config = self.config.write().await;

		mutation_fn(&mut config);

		self.preferences_watcher_tx.send_if_modified(|current| {
			let modified = current != &config.preferences;
			if modified {
				*current = config.preferences.clone();
			}
			modified
		});

		config
			.save(&self.config_file_path)
			.await
			.map(|()| config.clone())
	}

	/// update_preferences allows the user to update the preferences of the node
	pub(crate) async fn update_preferences(
		&self,
		update_fn: impl FnOnce(&mut NodePreferences),
	) -> Result<(), NodeConfigError> {
		let mut config = self.config.write().await;

		update_fn(&mut config.preferences);

		self.preferences_watcher_tx
			.send_replace(config.preferences.clone());

		config.save(&self.config_file_path).await
	}
}

#[derive(Error, Debug)]
pub enum NodeConfigError {
	#[error(transparent)]
	SerdeJson(#[from] serde_json::Error),
	#[error(transparent)]
	VersionManager(#[from] VersionManagerError<NodeConfigVersion>),
	#[error(transparent)]
	FileIO(#[from] FileIOError),
}



File: ./src/node/mod.rs
-------------------------------------------------
pub mod config;
mod platform;

pub use platform::*;



File: ./src/node/platform.rs
-------------------------------------------------
use crate::NodeError;
use serde::{Deserialize, Serialize};
use specta::Type;

#[allow(clippy::upper_case_acronyms)]
#[repr(u8)]
#[derive(Debug, Clone, Copy, Serialize, Deserialize, Type, Eq, PartialEq)]
pub enum Platform {
	Unknown = 0,
	Windows = 1,
	MacOS = 2,
	Linux = 3,
	IOS = 4,
	Android = 5,
}

impl Platform {
	#[allow(unreachable_code)]
	pub fn current() -> Self {
		#[cfg(target_os = "windows")]
		return Self::Windows;

		#[cfg(target_os = "macos")]
		return Self::MacOS;

		#[cfg(target_os = "linux")]
		return Self::Linux;

		#[cfg(target_os = "ios")]
		return Self::IOS;

		#[cfg(target_os = "android")]
		return Self::Android;

		Self::Unknown
	}
}

impl TryFrom<u8> for Platform {
	type Error = NodeError;

	fn try_from(value: u8) -> Result<Self, Self::Error> {
		let s = match value {
			0 => Self::Unknown,
			1 => Self::Windows,
			2 => Self::MacOS,
			3 => Self::Linux,
			4 => Self::IOS,
			5 => Self::Android,
			_ => return Err(NodeError::InvalidPlatformInt(value)),
		};

		Ok(s)
	}
}



File: ./src/notifications.rs
-------------------------------------------------
use std::sync::{atomic::AtomicU32, Arc};

use tokio::sync::broadcast;

use crate::api::notifications::Notification;

#[derive(Clone)]
pub struct Notifications(
	// Keep this private and use `Node::emit_notification` or `Library::emit_notification` instead.
	broadcast::Sender<Notification>,
	// Counter for `NotificationId::Node(_)`. NotificationId::Library(_, _)` is autogenerated by the DB.
	Arc<AtomicU32>,
);

impl Notifications {
	#[allow(clippy::new_without_default)]
	pub fn new() -> Self {
		let (tx, _) = broadcast::channel(30);
		Self(tx, Arc::new(AtomicU32::new(0)))
	}

	pub fn subscribe(&self) -> broadcast::Receiver<Notification> {
		self.0.subscribe()
	}

	/// DO NOT USE THIS. Use `Node::emit_notification` or `Library::emit_notification` instead.
	pub fn _internal_send(&self, notif: Notification) {
		self.0.send(notif).ok();
	}

	pub fn _internal_next_id(&self) -> u32 {
		self.1.fetch_add(1, std::sync::atomic::Ordering::SeqCst)
	}
}



File: ./src/volume/mod.rs
-------------------------------------------------
// Adapted from: https://github.com/kimlimjustin/xplorer/blob/f4f3590d06783d64949766cc2975205a3b689a56/src-tauri/src/drives.rs

use std::{
	fmt::Display,
	hash::{Hash, Hasher},
	path::PathBuf,
	sync::OnceLock,
};

use serde::{Deserialize, Serialize};
use serde_with::{serde_as, DisplayFromStr};
use specta::Type;
use sysinfo::{DiskExt, System, SystemExt};
use thiserror::Error;
use tokio::sync::Mutex;
use tracing::error;

pub mod watcher;

fn sys_guard() -> &'static Mutex<System> {
	static SYS: OnceLock<Mutex<System>> = OnceLock::new();
	SYS.get_or_init(|| Mutex::new(System::new_all()))
}

#[derive(Serialize, Deserialize, Debug, Clone, Type, Hash, PartialEq, Eq)]
#[allow(clippy::upper_case_acronyms)]
pub enum DiskType {
	SSD,
	HDD,
	Removable,
}

impl Display for DiskType {
	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
		f.write_str(match self {
			Self::SSD => "SSD",
			Self::HDD => "HDD",
			Self::Removable => "Removable",
		})
	}
}

#[serde_as]
#[derive(Serialize, Deserialize, Debug, Clone, Type)]
pub struct Volume {
	pub name: String,
	pub mount_points: Vec<PathBuf>,
	#[specta(type = String)]
	#[serde_as(as = "DisplayFromStr")]
	pub total_capacity: u64,
	#[specta(type = String)]
	#[serde_as(as = "DisplayFromStr")]
	pub available_capacity: u64,
	pub disk_type: DiskType,
	pub file_system: Option<String>,
	pub is_root_filesystem: bool,
}

impl Hash for Volume {
	fn hash<H: Hasher>(&self, state: &mut H) {
		self.name.hash(state);
		self.mount_points.iter().for_each(|mount_point| {
			// Hashing like this to ignore ordering between mount points
			mount_point.hash(state);
		});
		self.disk_type.hash(state);
		self.file_system.hash(state);
	}
}

impl PartialEq for Volume {
	fn eq(&self, other: &Self) -> bool {
		self.name == other.name
			&& self.disk_type == other.disk_type
			&& self.file_system == other.file_system
			// Leaving mount points for last because O(n * m)
			&& self
				.mount_points
				.iter()
				.all(|mount_point| other.mount_points.contains(mount_point))
	}
}

impl Eq for Volume {}

#[derive(Error, Debug)]
pub enum VolumeError {
	#[error("Database error: {0}")]
	DatabaseErr(#[from] prisma_client_rust::QueryError),
	#[error("FromUtf8Error: {0}")]
	FromUtf8Error(#[from] std::string::FromUtf8Error),
}

impl From<VolumeError> for rspc::Error {
	fn from(e: VolumeError) -> Self {
		rspc::Error::with_cause(rspc::ErrorCode::InternalServerError, e.to_string(), e)
	}
}

#[cfg(target_os = "linux")]
pub async fn get_volumes() -> Vec<Volume> {
	use std::{collections::HashMap, path::Path};

	let mut sys = sys_guard().lock().await;
	sys.refresh_disks_list();

	let mut volumes: Vec<Volume> = Vec::new();
	let mut path_to_volume_index = HashMap::new();
	for disk in sys.disks() {
		let disk_name = disk.name();
		let mount_point = disk.mount_point().to_path_buf();
		let file_system = String::from_utf8(disk.file_system().to_vec())
			.map(|s| s.to_uppercase())
			.ok();
		let total_capacity = disk.total_space();
		let available_capacity = disk.available_space();
		let is_root_filesystem = mount_point.is_absolute() && mount_point.parent().is_none();

		let mut disk_path: PathBuf = PathBuf::from(disk_name);
		if file_system.as_ref().map(|fs| fs == "ZFS").unwrap_or(false) {
			// Use a custom path for ZFS disks to avoid conflicts with normal disks paths
			disk_path = Path::new("zfs://").join(disk_path);
		} else {
			// Ignore non-devices disks (overlay, fuse, tmpfs, etc.)
			if !disk_path.starts_with("/dev") {
				continue;
			}

			// Ensure disk has a valid device path
			let real_path = match tokio::fs::canonicalize(disk_name).await {
				Err(real_path) => {
					error!(
						"Failed to canonicalize disk path {}: {:#?}",
						disk_name.to_string_lossy(),
						real_path
					);
					continue;
				}
				Ok(real_path) => real_path,
			};

			// Check if disk is a symlink to another disk
			if real_path != disk_path {
				// Disk is a symlink to another disk, assign it to the same volume
				path_to_volume_index.insert(
					real_path.into_os_string(),
					path_to_volume_index
						.get(disk_name)
						.cloned()
						.unwrap_or(path_to_volume_index.len()),
				);
			}
		}

		if let Some(volume_index) = path_to_volume_index.get(disk_name) {
			// Disk already has a volume assigned, update it
			let volume: &mut Volume = volumes
				.get_mut(*volume_index)
				.expect("Volume index is present so the Volume must be present too");

			// Update mount point if not already present
			let mount_points = &mut volume.mount_points;
			if mount_point.iter().all(|p| *p != mount_point) {
				mount_points.push(mount_point);
				let mount_points_to_check = mount_points.clone();
				mount_points.retain(|candidate| {
					!mount_points_to_check
						.iter()
						.any(|path| candidate.starts_with(path) && candidate != path)
				});
				if !volume.is_root_filesystem {
					volume.is_root_filesystem = is_root_filesystem;
				}
			}

			// Update mount capacity, it can change between mounts due to quotas (ZFS, BTRFS?)
			if volume.total_capacity < total_capacity {
				volume.total_capacity = total_capacity;
			}

			// This shouldn't change between mounts, but just in case
			if volume.available_capacity > available_capacity {
				volume.available_capacity = available_capacity;
			}

			continue;
		}

		// Assign volume to disk path
		path_to_volume_index.insert(disk_path.into_os_string(), volumes.len());

		let mut name = disk_name.to_string_lossy().to_string();
		if name.replace(char::REPLACEMENT_CHARACTER, "") == "" {
			name = "Unknown".to_string()
		}

		volumes.push(Volume {
			name,
			disk_type: if disk.is_removable() {
				DiskType::Removable
			} else {
				match disk.kind() {
					sysinfo::DiskKind::SSD => DiskType::SSD,
					sysinfo::DiskKind::HDD => DiskType::HDD,
					_ => DiskType::Removable,
				}
			},
			file_system,
			mount_points: vec![mount_point],
			total_capacity,
			available_capacity,
			is_root_filesystem,
		});
	}

	volumes
}

#[cfg(target_os = "macos")]
#[derive(Deserialize)]
#[serde(rename_all = "kebab-case")]
struct ImageSystemEntity {
	mount_point: Option<String>,
}

#[cfg(target_os = "macos")]
#[derive(Deserialize)]
#[serde(rename_all = "kebab-case")]
struct ImageInfo {
	system_entities: Vec<ImageSystemEntity>,
}

#[cfg(target_os = "macos")]
#[derive(Deserialize)]
#[serde(rename_all = "kebab-case")]
struct HDIUtilInfo {
	images: Vec<ImageInfo>,
}

#[cfg(not(target_os = "linux"))]
pub async fn get_volumes() -> Vec<Volume> {
	use futures::future;
	use tokio::process::Command;

	let mut sys = sys_guard().lock().await;
	sys.refresh_disks_list();

	// Ignore mounted DMGs
	#[cfg(target_os = "macos")]
	let dmgs = &Command::new("hdiutil")
		.args(["info", "-plist"])
		.output()
		.await
		.map_err(|err| error!("Failed to execute hdiutil: {err:#?}"))
		.ok()
		.and_then(|wmic_process| {
			use std::str::FromStr;

			if wmic_process.status.success() {
				let info: Result<HDIUtilInfo, _> = plist::from_bytes(&wmic_process.stdout);
				match info {
					Err(err) => {
						error!("Failed to parse hdiutil output: {err:#?}");
						None
					}
					Ok(info) => Some(
						info.images
							.into_iter()
							.flat_map(|image| image.system_entities)
							.flat_map(|entity: ImageSystemEntity| entity.mount_point)
							.flat_map(|mount_point| PathBuf::from_str(mount_point.as_str()))
							.collect::<std::collections::HashSet<_>>(),
					),
				}
			} else {
				error!("Command hdiutil return error");
				None
			}
		});

	future::join_all(sys.disks().iter().map(|disk| async {
		#[cfg(not(windows))]
		let disk_name = disk.name();
		let mount_point = disk.mount_point().to_path_buf();

		#[cfg(windows)]
		let Ok((disk_name, mount_point)) = ({
			use normpath::PathExt;
			mount_point
				.normalize_virtually()
				.map(|p| (p.localize_name().to_os_string(), p.into_path_buf()))
		}) else {
			return None;
		};

		#[cfg(target_os = "macos")]
		{
			// Ignore mounted DMGs
			if dmgs
				.as_ref()
				.map(|dmgs| dmgs.contains(&mount_point))
				.unwrap_or(false)
			{
				return None;
			}

			if !(mount_point.starts_with("/Volumes") || mount_point.starts_with("/System/Volumes"))
			{
				return None;
			}
		}

		#[allow(unused_mut)] // mut is used in windows
		let mut total_capacity = disk.total_space();
		let available_capacity = disk.available_space();
		let is_root_filesystem = mount_point.is_absolute() && mount_point.parent().is_none();

		// Fix broken google drive partition size in Windows
		#[cfg(windows)]
		if total_capacity < available_capacity && is_root_filesystem {
			// Use available capacity as total capacity in the case we can't get the correct value
			total_capacity = available_capacity;

			let caption = mount_point.to_str();
			if let Some(caption) = caption {
				let mut caption = caption.to_string();

				// Remove path separator from Disk letter
				caption.pop();

				let wmic_output = Command::new("cmd")
					.args([
						"/C",
						&format!("wmic logical disk where Caption='{caption}' get Size"),
					])
					.output()
					.await
					.map_err(|err| error!("Failed to execute hdiutil: {err:#?}"))
					.ok()
					.and_then(|wmic_process| {
						if wmic_process.status.success() {
							String::from_utf8(wmic_process.stdout).ok()
						} else {
							error!("Command wmic return error");
							None
						}
					});

				if let Some(wmic_output) = wmic_output {
					match wmic_output.split("\r\r\n").collect::<Vec<&str>>()[1]
						.to_string()
						.trim()
						.parse::<u64>()
					{
						Err(err) => error!("Failed to parse wmic output: {err:#?}"),
						Ok(n) => total_capacity = n,
					}
				}
			}
		}

		let mut name = disk_name.to_string_lossy().to_string();
		if name.replace(char::REPLACEMENT_CHARACTER, "") == "" {
			name = "Unknown".to_string()
		}

		Some(Volume {
			name,
			disk_type: if disk.is_removable() {
				DiskType::Removable
			} else {
				match disk.kind() {
					sysinfo::DiskKind::SSD => DiskType::SSD,
					sysinfo::DiskKind::HDD => DiskType::HDD,
					_ => DiskType::Removable,
				}
			},
			mount_points: vec![mount_point],
			file_system: String::from_utf8(disk.file_system().to_vec()).ok(),
			total_capacity,
			available_capacity,
			is_root_filesystem,
		})
	}))
	.await
	.into_iter()
	.flatten()
	.collect::<Vec<Volume>>()
}

// pub async fn save_volume(library: &Library) -> Result<(), VolumeError> {
// 	// enter all volumes associate with this client add to db
// 	for volume in get_volumes() {
// 		let params = vec![
// 			disk_type::set(volume.disk_type.map(|t| t.to_string())),
// 			filesystem::set(volume.file_system.clone()),
// 			total_bytes_capacity::set(volume.total_capacity.to_string()),
// 			total_bytes_available::set(volume.available_capacity.to_string()),
// 		];

// 		library
// 			.db
// 			.volume()
// 			.upsert(
// 				node_id_mount_point_name(
// 					library.node_local_id,
// 					volume.mount_point,
// 					volume.name,
// 				),
// 				volume::create(
// 					library.node_local_id,
// 					volume.name,
// 					volume.mount_point,
// 					params.clone(),
// 				),
// 				params,
// 			)
// 			.exec()
// 			.await?;
// 	}
// 	// cleanup: remove all unmodified volumes associate with this client

// 	Ok(())
// }

// #[test]
// fn test_get_volumes() {
//   let volumes = get_volumes()?;
//   dbg!(&volumes);
//   assert!(volumes.len() > 0);
// }



File: ./src/volume/watcher.rs
-------------------------------------------------
use crate::{invalidate_query, library::Library};

use std::{collections::HashSet, sync::Arc};

use tokio::{
	spawn,
	time::{interval, Duration},
};

use super::get_volumes;

pub fn spawn_volume_watcher(library: Arc<Library>) {
	spawn(async move {
		let mut interval = interval(Duration::from_secs(1));
		let mut existing_volumes = get_volumes().await.into_iter().collect::<HashSet<_>>();

		loop {
			interval.tick().await;

			let current_volumes = get_volumes().await.into_iter().collect::<HashSet<_>>();

			if existing_volumes != current_volumes {
				existing_volumes = current_volumes;
				invalidate_query!(&library, "volumes.list");
			}
		}
	});
}



